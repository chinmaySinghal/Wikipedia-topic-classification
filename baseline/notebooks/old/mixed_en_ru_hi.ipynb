{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import spacy\n",
    "import string\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import mwparserfromhell\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  8 11:55:46 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    On   | 00000000:05:00.0 Off |                  N/A |\r\n",
      "| 27%   30C    P8     7W / 180W |      0MiB /  8119MiB |      0%   E. Process |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_EMBEDDINGS_FOLDER = \"/scratch/mz2476/wiki/embeddings/\"\n",
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/aligned_datasets/\"\n",
    "PATH_TO_MODELS_FOLDER = \"/scratch/mz2476/wiki/models/\"\n",
    "\n",
    "PATH_TO_SAVED_EMBED_FOLDER = \"/scratch/mz2476/wiki/data/aligned_datasets/mix_en_hi_ru/\"\n",
    "PATH_TO_DATA_FOR_MODEL_FOLDER = \"/scratch/mz2476/wiki/data/aligned_datasets/data_for_model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import preprocess\n",
    "import importlib\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from preprocess import tokenize_dataset, TensoredDataset, pad_collate_fn\n",
    "from preprocess import create_vocab_from_tokens, create_lookups_for_vocab\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES_LIST = [\"english\", \"russian\", \"hindi\"] # \n",
    "LANGUAGES_DICT = defaultdict(dict)\n",
    "\n",
    "monolingual_train_size = 30000\n",
    "multilingual_train_size = 10000\n",
    "val_size = 1000\n",
    "\n",
    "# assuming the data is in PATH_TO_DATA_FOLDER\n",
    "for language in LANGUAGES_LIST:\n",
    "    language_code = language[:2]\n",
    "    LANGUAGES_DICT[language][\"language_code\"] = language_code\n",
    "    FILE_NAMES_DICT = {\n",
    "        \"vocab\": f\"{PATH_TO_DATA_FOR_MODEL_FOLDER}vocab_all_{language_code}.pt\",\n",
    "        \"monolingual_train\": f\"{PATH_TO_DATA_FOR_MODEL_FOLDER}df_wiki_monolingual_train_{monolingual_train_size}_{language_code}.pt\",\n",
    "        \"multilingual_train\": f\"{PATH_TO_DATA_FOR_MODEL_FOLDER}df_wiki_multilingual_train_{multilingual_train_size}_{language_code}.pt\",\n",
    "        \"val\": f\"{PATH_TO_DATA_FOR_MODEL_FOLDER}df_wiki_valid_{val_size}_{language_code}.pt\",\n",
    "        \"test\": f\"{PATH_TO_DATA_FOR_MODEL_FOLDER}df_wiki_test_{language_code}.pt\",\n",
    "        \"fasttext_embeddings\": f\"{PATH_TO_EMBEDDINGS_FOLDER}wiki.{language_code}.align.vec\",\n",
    "        \"embed_matrix\": f'{PATH_TO_SAVED_EMBED_FOLDER}embeddings_matrix_with_idx_to_word_{language_code}.pt',\n",
    "    }\n",
    "    # ADD check that these files exist\n",
    "    LANGUAGES_DICT[language][\"FILE_NAMES_DICT\"] = FILE_NAMES_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english vocab size is: 741334\n",
      "russian vocab size is: 858845\n",
      "hindi vocab size is: 441314\n"
     ]
    }
   ],
   "source": [
    "# LOAD vocab, tensor dataset, classes\n",
    "classes = torch.load(PATH_TO_DATA_FOLDER + \"45_classes_list.pt\")\n",
    "mlb = MultiLabelBinarizer(classes)\n",
    "\n",
    "for language, lang_dict in LANGUAGES_DICT.items():\n",
    "    vocab = torch.load(lang_dict[\"FILE_NAMES_DICT\"][\"vocab\"])\n",
    "    print(f\"{language} vocab size is:\", len(vocab))\n",
    "#     LANGUAGES_DICT[language][\"vocab\"] = vocab\n",
    "    LANGUAGES_DICT[language][\"index_to_word\"], LANGUAGES_DICT[language][\"word_to_index\"] =\\\n",
    "        create_lookups_for_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order: dict_keys(['english', 'russian', 'hindi'])\n"
     ]
    }
   ],
   "source": [
    "# Create combined vocab, index_to_word, word_to_index\n",
    "# 0 - <pad>, 1 - <unk> \n",
    "vocab = [\"<pad>\", \"<unk>\"]\n",
    "print(\"Order:\", LANGUAGES_DICT.keys())\n",
    "for language, lang_dict in LANGUAGES_DICT.items(): # .keys() keep same order in Python version >= 3.7\n",
    "    assert lang_dict[\"index_to_word\"][0] != \"<pad>\"\n",
    "    vocab += lang_dict[\"index_to_word\"]\n",
    "    \n",
    "index_to_word, word_to_index = create_lookups_for_vocab(vocab)\n",
    "assert len(set(word_to_index)) == len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2041495"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train size: 30000 \n",
      "Combined val size: 3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sections_tokens</th>\n",
       "      <th>raw_outlinks</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q617433</td>\n",
       "      <td>[History_And_Society.Education, STEM.Informati...</td>\n",
       "      <td>[building, completed, one, eight, eight, nine,...</td>\n",
       "      <td>[history, founding, expansion, modern, one, ni...</td>\n",
       "      <td>[[[Latin]], [[Private university|Private]], [[...</td>\n",
       "      <td>[Latin, Private university, research universit...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q1649653</td>\n",
       "      <td>[STEM.Technology]</td>\n",
       "      <td>[набор, средств, инженерного, анализа, выпуска...</td>\n",
       "      <td>[история, создания, описание, примечания, лите...</td>\n",
       "      <td>[[[Siemens PLM Software]], [[Computer-aided en...</td>\n",
       "      <td>[Siemens PLM Software, Computer-aided engineer...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q2047143</td>\n",
       "      <td>[Geography.Asia]</td>\n",
       "      <td>[चित्र, नौ, शून्य, सात, नौ, पूर्वी, बंगाल, असम...</td>\n",
       "      <td>[उत्पत्ति, पृष्ठभूमि, विभाजन, प्रभाव, बंगभंग, ...</td>\n",
       "      <td>[[[चित्र:Bengal gazetteer 1907-9.jpg|right|thu...</td>\n",
       "      <td>[चित्र:Bengal gazetteer 1907-9.jpg, कर्जन, भार...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q162448</td>\n",
       "      <td>[Geography.Landforms, Geography.Asia]</td>\n",
       "      <td>[मिनिकॉय, मलिक्, मह्ल, भारतीय, द्वीपसमूह, लक्ष...</td>\n",
       "      <td>[शब्द, व्युत्पत्ति, भूगोल, गाँव, जलवायु, जनसां...</td>\n",
       "      <td>[[[मह्ल]], [[मलयालम भाषा|मलयाली]], [[भारतीय मा...</td>\n",
       "      <td>[मह्ल, मलयालम भाषा, भारतीय मानक समय, डाक सूचक ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q19969359</td>\n",
       "      <td>[Culture.Sports, Geography.Africa]</td>\n",
       "      <td>[замбия, летние, олимпийские, игры, ноль, шест...</td>\n",
       "      <td>[состав, сборной, результаты, соревнований, фа...</td>\n",
       "      <td>[[[Пунза, Мэтьюс|Мэтьюс Пунза]], [[Замбия]], [...</td>\n",
       "      <td>[Пунза, Мэтьюс, Замбия, Летние Олимпийские игр...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         QID                               mid_level_categories  \\\n",
       "0    Q617433  [History_And_Society.Education, STEM.Informati...   \n",
       "1   Q1649653                                  [STEM.Technology]   \n",
       "2   Q2047143                                   [Geography.Asia]   \n",
       "3    Q162448              [Geography.Landforms, Geography.Asia]   \n",
       "4  Q19969359                 [Culture.Sports, Geography.Africa]   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [building, completed, one, eight, eight, nine,...   \n",
       "1  [набор, средств, инженерного, анализа, выпуска...   \n",
       "2  [चित्र, नौ, शून्य, सात, नौ, पूर्वी, बंगाल, असम...   \n",
       "3  [मिनिकॉय, मलिक्, मह्ल, भारतीय, द्वीपसमूह, लक्ष...   \n",
       "4  [замбия, летние, олимпийские, игры, ноль, шест...   \n",
       "\n",
       "                                     sections_tokens  \\\n",
       "0  [history, founding, expansion, modern, one, ni...   \n",
       "1  [история, создания, описание, примечания, лите...   \n",
       "2  [उत्पत्ति, पृष्ठभूमि, विभाजन, प्रभाव, बंगभंग, ...   \n",
       "3  [शब्द, व्युत्पत्ति, भूगोल, गाँव, जलवायु, जनसां...   \n",
       "4  [состав, сборной, результаты, соревнований, фа...   \n",
       "\n",
       "                                        raw_outlinks  \\\n",
       "0  [[[Latin]], [[Private university|Private]], [[...   \n",
       "1  [[[Siemens PLM Software]], [[Computer-aided en...   \n",
       "2  [[[चित्र:Bengal gazetteer 1907-9.jpg|right|thu...   \n",
       "3  [[[मह्ल]], [[मलयालम भाषा|मलयाली]], [[भारतीय मा...   \n",
       "4  [[[Пунза, Мэтьюс|Мэтьюс Пунза]], [[Замбия]], [...   \n",
       "\n",
       "                                            outlinks  \\\n",
       "0  [Latin, Private university, research universit...   \n",
       "1  [Siemens PLM Software, Computer-aided engineer...   \n",
       "2  [चित्र:Bengal gazetteer 1907-9.jpg, कर्जन, भार...   \n",
       "3  [मह्ल, मलयालम भाषा, भारतीय मानक समय, डाक सूचक ...   \n",
       "4  [Пунза, Мэтьюс, Замбия, Летние Олимпийские игр...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 57\n",
    "\n",
    "wiki_train, wiki_valid = [], []\n",
    "\n",
    "dict_of_dfs = defaultdict()\n",
    "\n",
    "for language, lang_dict in LANGUAGES_DICT.items():\n",
    "    language_code = lang_dict[\"language_code\"]\n",
    "    dict_of_dfs[f\"monolingual_train_{language_code}\"], dict_of_dfs[f\"multilingual_train_{language_code}\"] =\\\n",
    "            (torch.load(lang_dict[\"FILE_NAMES_DICT\"][\"monolingual_train\"]),\n",
    "             torch.load(lang_dict[\"FILE_NAMES_DICT\"][\"multilingual_train\"]))\n",
    "    dict_of_dfs[f\"val_{language_code}\"] = torch.load(lang_dict[\"FILE_NAMES_DICT\"][\"val\"])\n",
    "    wiki_train.append(dict_of_dfs[f\"multilingual_train_{language_code}\"])\n",
    "    wiki_valid.append(dict_of_dfs[f\"val_{language_code}\"])\n",
    "\n",
    "wiki_train = pd.concat(wiki_train).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "wiki_valid = pd.concat(wiki_valid).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "dict_of_dfs[\"train\"] = wiki_train\n",
    "dict_of_dfs[\"val\"] = wiki_valid\n",
    "\n",
    "print(f\"Combined train size: {wiki_train.shape[0]} \\nCombined val size: {wiki_valid.shape[0]}\")\n",
    "wiki_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['monolingual_train_en', 'multilingual_train_en', 'val_en', 'monolingual_train_ru', 'multilingual_train_ru', 'val_ru', 'monolingual_train_hi', 'multilingual_train_hi', 'val_hi', 'train', 'val'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import create_dict_of_tensor_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:12<00:00, 2458.20it/s]\n",
      "100%|██████████| 10000/10000 [00:04<00:00, 2495.57it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2536.71it/s]\n",
      "100%|██████████| 30000/30000 [00:11<00:00, 2509.86it/s]\n",
      "100%|██████████| 10000/10000 [00:02<00:00, 3416.25it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3432.12it/s]\n",
      "100%|██████████| 30000/30000 [00:03<00:00, 9523.53it/s] \n",
      "100%|██████████| 10000/10000 [00:01<00:00, 8379.75it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 8834.19it/s]\n",
      "100%|██████████| 30000/30000 [00:08<00:00, 3548.26it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 3593.69it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_tensor_dataset = create_dict_of_tensor_datasets(dict_of_dfs, word_to_index, max_num_tokens=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextData(tokens=tensor([1351385,  947193, 1192460, 1555555,  897342,  963948, 1023433,  947193,\n",
       "        1489225, 1154589, 1238015, 1572037, 1238015,  828367, 1555277, 1368543,\n",
       "        1474962,  981998,  903480, 1154589,  995751,  995637, 1450224, 1303073,\n",
       "         773590, 1303102,  773590, 1425817,  946313,  807784,  926764, 1202620,\n",
       "         947193, 1364344, 1393449, 1110018,  995283, 1101174, 1297707, 1278996,\n",
       "        1393521, 1252914,  791015, 1286208, 1165927, 1202620, 1427468, 1569647,\n",
       "        1106836, 1388280, 1001020, 1213220, 1569653, 1286208, 1432150, 1199826,\n",
       "        1046625, 1469774, 1043967, 1388280, 1001020,  791015, 1384365,  995637,\n",
       "        1062246, 1384383,  995637, 1062246,  987270,  862770, 1286164, 1432136,\n",
       "        1099789,  922696,  749588, 1043967,  922696,  749588, 1388280, 1001020,\n",
       "        1165904, 1425811, 1023434, 1364318, 1393449, 1110018, 1162115, 1110018,\n",
       "        1165891,  985534, 1062229,  856427, 1084971, 1278996, 1301777, 1165788,\n",
       "        1045837, 1498467, 1333686, 1393419,  920275,  828449,  985602, 1062229,\n",
       "        1029395, 1453200, 1251174,  969956, 1450755, 1351386,  862770, 1334192,\n",
       "        1085109, 1075614,  787665, 1428890, 1550405,  947193,  919574, 1344798,\n",
       "        1478061, 1508008,  920275,  920275,  897096,  863095,  957783,  823701,\n",
       "        1437395, 1043962,  985555,  851549,  823713, 1558332, 1165778, 1384381,\n",
       "         765509, 1014977, 1596241, 1428006,  962943,  859402, 1236865, 1114932,\n",
       "        1057979,  963955,  852547, 1251173, 1283594, 1390750,  920275,  920275,\n",
       "         920275,  920275,  897119,  968348, 1227688, 1045837, 1062246, 1594180,\n",
       "         982864,  950598, 1478061, 1317074, 1431845,  947193, 1307876, 1209091,\n",
       "        1333058, 1096370,  985609,  897097, 1062229,  840635, 1192460, 1192460,\n",
       "        1192460,  897096, 1286208, 1154616,  985602,  897097, 1461764,  985602,\n",
       "         897097, 1545965, 1230204, 1154589, 1119390, 1414370, 1062246, 1192460,\n",
       "        1192460, 1555555,  897342,  921500, 1192460,  897097, 1429077, 1251174,\n",
       "        1167042,  985610, 1062229, 1057964,  869892, 1297162, 1469895, 1360193,\n",
       "         819630, 1016563, 1016565, 1192460,  897096, 1429077, 1018497,  995653,\n",
       "        1303155, 1086440, 1119383, 1023406, 1119398,  919545,  947193,  919574,\n",
       "         771890,  920275,  920275, 1567176,  897096,  862770, 1119392,  864712,\n",
       "        1323050, 1384365, 1572037, 1078205, 1062246, 1572037, 1078205, 1066971,\n",
       "        1400427, 1420905, 1400376, 1427023, 1553446, 1132953, 1062246,  920275,\n",
       "         920275, 1567176, 1066967, 1132953,  920275,  920275, 1567176,  897097,\n",
       "        1230523, 1387817,  919546, 1119392,  864695, 1461764,  919545,  946177,\n",
       "        1500059,  947193,  987270, 1384365,  995637, 1062246,  995658, 1045837,\n",
       "         868107,  764094, 1225298, 1500049, 1334192, 1229273, 1062236, 1045726,\n",
       "        1428069, 1333686, 1084971, 1537971,  873385, 1481355,  985473,  850427,\n",
       "        1333705, 1323050,  895355, 1084907, 1441484, 1198886, 1124767, 1045726,\n",
       "        1250897, 1461764,  985556,  919574, 1428890, 1192460, 1007338,  920275,\n",
       "         920275, 1567176,  897097, 1323050,  764007,  865128, 1461779,  868577,\n",
       "         969778, 1471530, 1430569,  829628, 1379853, 1108066, 1108065, 1057979,\n",
       "        1286208, 1471578, 1092077, 1325906, 1227668, 1502310,  862770,  865109,\n",
       "        1461765,  947234, 1418792,  985534, 1108065, 1314152, 1004409,  926342,\n",
       "        1132657,  959940, 1379916, 1246704,  828449,  995646, 1474584, 1275274,\n",
       "         947234, 1418800, 1390333,  921345, 1420895,  919545, 1278996, 1246704,\n",
       "         864253, 1338617, 1403495,  985534, 1090624,  974552, 1137861,  963543,\n",
       "        1140034, 1500058,  919545, 1066971, 1132953,  947193,  919574, 1066971,\n",
       "        1132953, 1323050, 1594180,  783400, 1538442,  861908, 1395340,  897982,\n",
       "         962943, 1408662,  985556, 1323050, 1384365,  980588, 1062246,  980588,\n",
       "         982578,  897366, 1430494, 1191866, 1119401, 1390777, 1251174,  830628,\n",
       "         965722, 1191320, 1018486,  849847, 1018486,  879543,  947194,  873932,\n",
       "        1045837, 1458669, 1388067, 1315738,  985601, 1045851,  995643, 1384365,\n",
       "        1360438, 1446388, 1119392,  864695, 1478061, 1192346, 1192460, 1192460,\n",
       "        1192460,  897342, 1279694, 1387886, 1527254, 1323928, 1030125, 1384365,\n",
       "         745305, 1062246,  745302, 1384365, 1593571, 1062246, 1594180,  783401,\n",
       "        1291820, 1114925,  868577, 1512629, 1538434,  995751, 1317280, 1384365,\n",
       "        1191322,  979098, 1062246, 1191322,  979098, 1512634, 1331972, 1481755,\n",
       "         840347, 1502206,  814670,  840863,  995646, 1062229,  947193, 1446387,\n",
       "         830644, 1365915,  870245, 1333686, 1393419,  962843, 1555555, 1114925,\n",
       "        1427530, 1333686, 1236865, 1557658, 1285729,  947196, 1446387, 1251172,\n",
       "        1114920, 1192460, 1192460, 1192460, 1192460, 1192460,  897264, 1478043,\n",
       "        1553447, 1042601, 1043124, 1227130,  859402, 1333686,  962843,  918195,\n",
       "        1114925, 1323050,  980588, 1230242, 1271690, 1279657, 1286208, 1471578,\n",
       "         862770,  868099,  985645, 1057961,  869765, 1320671, 1226160, 1481751,\n",
       "         995751, 1165892, 1191863, 1461764, 1390750,  920275, 1268966, 1323050,\n",
       "        1384365,  791080, 1062246,  791080,  947193, 1446387, 1267767, 1449676,\n",
       "        1555555,  920275, 1152603, 1555555,  962844, 1557658, 1333686, 1555392,\n",
       "        1566676, 1363042, 1304268, 1544805, 1567176,  962824, 1122713, 1331972,\n",
       "         947194,  968348, 1408662, 1395343, 1461848, 1323050,  980588, 1057956,\n",
       "        1066449, 1250897,  931460, 1343374, 1026768, 1191130, 1474585,  995653,\n",
       "        1045851, 1343373, 1446387,  830644, 1365915, 1205622,  947194, 1030326,\n",
       "        1165899,  897097,  947193, 1227645, 1289642, 1292493,  893876, 1474585,\n",
       "        1384381, 1163783,  862770,  947193, 1446387,  859402, 1461848, 1140035,\n",
       "        1291746, 1421307, 1323050, 1191322,  979098, 1144970,  870245, 1333686,\n",
       "        1192460,  962843,  946177, 1187340, 1427838, 1279736, 1236865,  955446,\n",
       "        1449677, 1333705, 1268497,  995646,  849847, 1018485,  968341,  962823,\n",
       "        1444363, 1278933, 1191130,  995651, 1474585, 1387819, 1513231,  917444,\n",
       "        1421306,  962943, 1192460, 1192460, 1236874, 1250924, 1449687, 1393419,\n",
       "        1192460,  865128, 1449687, 1209089, 1160970,  995750, 1081668, 1268971,\n",
       "         859182,  980588,  962943, 1567176, 1236865,  865116, 1461764,  985556,\n",
       "         967449, 1395402, 1427838, 1279736, 1555555, 1192460, 1192460, 1379365,\n",
       "         872015, 1427838, 1498459, 1279724,  995665, 1461764, 1395343, 1351385,\n",
       "        1441481,  963000, 1090889,  955370, 1285730,  862770,  865109, 1333058,\n",
       "        1114944,  918153, 1119392,  864712,  995634,  980588, 1209089, 1592311,\n",
       "        1429259,  830637,  842378, 1023409,  947193,  861843, 1429077, 1018497,\n",
       "         995653, 1045851,  862770, 1333686, 1114920, 1395343, 1293336, 1018485,\n",
       "        1045851,  879543,  937128, 1307365,  873386,  746963, 1267767,  947193,\n",
       "        1305867,  995750,  920275, 1269030,  962843, 1026768, 1018486, 1384381,\n",
       "        1119401,  864668, 1023434, 1351385,  947193,  970922, 1272662,  995649,\n",
       "        1425878, 1215383,  985645, 1215385, 1242922,  985645, 1243373,  985524,\n",
       "        1410499, 1450224, 1384365,  995637, 1062246, 1309327, 1428454]), len=tensor([719.]), target=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['monolingual_train_en', 'multilingual_train_en', 'val_en', 'monolingual_train_ru', 'multilingual_train_ru', 'val_ru', 'monolingual_train_hi', 'multilingual_train_hi', 'val_hi', 'train', 'val'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_loaders.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE = False\n",
    "# if SAVE:\n",
    "#     # SAVE tensor datasets\n",
    "#     torch.save(wiki_tensor_dataset, f'{PATH_TO_DATA_FOLDER}wiki_tensor_dataset_mixed_en_ru.pt')\n",
    "#     print(\"Saved.\")\n",
    "    \n",
    "# wiki_tensor_dataset = torch.load(f'{PATH_TO_DATA_FOLDER}wiki_tensor_dataset_mixed_en_ru.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load aligned en and ru embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "LOAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for language, lang_dict in LANGUAGES_DICT.items():\n",
    "    if LOAD:\n",
    "        embed_info_dict = torch.load(lang_dict[\"FILE_NAMES_DICT\"][\"embed_matrix\"])\n",
    "        LANGUAGES_DICT[language][\"weights_matrix_ve\"] = embed_info_dict[\"weights_matrix_ve\"]\n",
    "    if SAVE:\n",
    "        language_code = lang_dict[\"language_code\"]\n",
    "        # 2.5 million\n",
    "        embeddings = utils.load_vectors(lang_dict[\"FILE_NAMES_DICT\"][\"fasttext_embeddings\"])\n",
    "        #Creating the weight matrix for pretrained word embeddings\n",
    "        weights_matrix_ve = utils.create_embeddings_matrix(lang_dict[\"index_to_word\"], embeddings)\n",
    "        LANGUAGES_DICT[language][\"weights_matrix_ve\"] = weights_matrix_ve\n",
    "        # SAVE embeddings matrix together with index_to_word\n",
    "        torch.save({\n",
    "            \"index_to_word\" : lang_dict[\"index_to_word\"],\n",
    "            \"weights_matrix_ve\" : weights_matrix_ve,\n",
    "        }, lang_dict[\"FILE_NAMES_DICT\"][\"embed_matrix\"])\n",
    "        print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings matrix shape: torch.Size([2041495, 300]), \n",
      "Vocab size: 2041495\n"
     ]
    }
   ],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "# 0 - <pad>, 1 - <unk> \n",
    "weights_matrix_ve = torch.zeros(len(index_to_word), LANGUAGES_DICT[\"english\"][\"weights_matrix_ve\"].shape[1])\n",
    "start_idx = 2\n",
    "for language, lang_dict in LANGUAGES_DICT.items():\n",
    "    end_idx = start_idx + len(lang_dict[\"index_to_word\"])\n",
    "    assert index_to_word[start_idx:end_idx] == lang_dict[\"index_to_word\"]\n",
    "    assert index_to_word[start_idx] == lang_dict[\"index_to_word\"][0]\n",
    "    assert index_to_word[end_idx-1] == lang_dict[\"index_to_word\"][-1]\n",
    "    weights_matrix_ve[start_idx:end_idx] = lang_dict[\"weights_matrix_ve\"]\n",
    "    start_idx = end_idx\n",
    "#     weights_matrix_ve += lang_dict[\"weights_matrix_ve\"]\n",
    "\n",
    "print(f\"Embeddings matrix shape: {weights_matrix_ve.shape}, \\nVocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE = False\n",
    "# if SAVE:\n",
    "#     # SAVE embeddings matrix\n",
    "#     torch.save(weights_matrix_ve, f'{PATH_TO_SA}embedding_weights_matrix_mixed_en_ru.pt')\n",
    "#     print(\"Saved.\")\n",
    "    \n",
    "# weights_matrix_ve = torch.load(f'{PATH_TO_DATA_FOLDER}embedding_weights_matrix_mixed_en_ru.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  8 11:57:28 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    On   | 00000000:05:00.0 Off |                  N/A |\r\n",
      "| 27%   30C    P8     7W / 180W |     10MiB /  8119MiB |      0%   E. Process |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4497940000000002"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix_ve.element_size() * weights_matrix_ve.nelement() * 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model, evaluate on mix, en, ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model, print_results, train_model, get_train_val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4510612480000002"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 150, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 15}\n",
      "mixed_en_hi_ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_15\n",
      "0 epoch\n",
      "Epoch: [1/10], Step: [101/3750], Train_loss: 0.19563879780471324\n",
      "Epoch: [1/10], Step: [201/3750], Train_loss: 0.1667928509786725\n",
      "Epoch: [1/10], Step: [301/3750], Train_loss: 0.153657601972421\n",
      "Epoch: [1/10], Step: [401/3750], Train_loss: 0.14474958579055966\n",
      "Epoch: [1/10], Step: [501/3750], Train_loss: 0.13652050098776816\n",
      "Epoch: [1/10], Step: [601/3750], Train_loss: 0.1303195236499111\n",
      "Epoch: [1/10], Step: [701/3750], Train_loss: 0.1261022020663534\n",
      "Epoch: [1/10], Step: [801/3750], Train_loss: 0.1222277458012104\n",
      "Precision macro: 0.4066, Recall macro: 0.1743, F1 macro: 0.2193 \n",
      "Precision micro: 0.795, Recall micro: 0.3913, F1 micro: 0.5245 \n",
      "Epoch: [1/10], Step: [901/3750], Train_loss: 0.11876137730975946\n",
      "Epoch: [1/10], Step: [1001/3750], Train_loss: 0.1156963351033628\n",
      "Epoch: [1/10], Step: [1101/3750], Train_loss: 0.11335171716626395\n",
      "Epoch: [1/10], Step: [1201/3750], Train_loss: 0.11122542228084058\n",
      "Epoch: [1/10], Step: [1301/3750], Train_loss: 0.109191356635151\n",
      "Epoch: [1/10], Step: [1401/3750], Train_loss: 0.10734338447052454\n",
      "Epoch: [1/10], Step: [1501/3750], Train_loss: 0.10600430698568622\n",
      "Epoch: [1/10], Step: [1601/3750], Train_loss: 0.10448046105098911\n",
      "Precision macro: 0.4526, Recall macro: 0.2978, F1 macro: 0.3298 \n",
      "Precision micro: 0.7382, Recall micro: 0.542, F1 micro: 0.6251 \n",
      "Epoch: [1/10], Step: [1701/3750], Train_loss: 0.10311271658495945\n",
      "Epoch: [1/10], Step: [1801/3750], Train_loss: 0.10186037592796816\n",
      "Epoch: [1/10], Step: [1901/3750], Train_loss: 0.10077641498297453\n",
      "Epoch: [1/10], Step: [2001/3750], Train_loss: 0.0996851890841499\n",
      "Epoch: [1/10], Step: [2101/3750], Train_loss: 0.09874261716380715\n",
      "Epoch: [1/10], Step: [2201/3750], Train_loss: 0.09800834910690107\n",
      "Epoch: [1/10], Step: [2301/3750], Train_loss: 0.09701415724971373\n",
      "Epoch: [1/10], Step: [2401/3750], Train_loss: 0.09627040383483594\n",
      "Precision macro: 0.5538, Recall macro: 0.2845, F1 macro: 0.3389 \n",
      "Precision micro: 0.7983, Recall micro: 0.5127, F1 micro: 0.6244 \n",
      "Epoch: [1/10], Step: [2501/3750], Train_loss: 0.09554630832001566\n",
      "Epoch: [1/10], Step: [2601/3750], Train_loss: 0.094887487389004\n",
      "Epoch: [1/10], Step: [2701/3750], Train_loss: 0.0941772504095678\n",
      "Epoch: [1/10], Step: [2801/3750], Train_loss: 0.09353733655730528\n",
      "Epoch: [1/10], Step: [2901/3750], Train_loss: 0.09277849295850972\n",
      "Epoch: [1/10], Step: [3001/3750], Train_loss: 0.092164207438007\n",
      "Epoch: [1/10], Step: [3101/3750], Train_loss: 0.09159939163214256\n",
      "Epoch: [1/10], Step: [3201/3750], Train_loss: 0.09106988513260149\n",
      "Precision macro: 0.5735, Recall macro: 0.3373, F1 macro: 0.395 \n",
      "Precision micro: 0.785, Recall micro: 0.5884, F1 micro: 0.6726 \n",
      "Epoch: [1/10], Step: [3301/3750], Train_loss: 0.09050359947494034\n",
      "Epoch: [1/10], Step: [3401/3750], Train_loss: 0.08992312636719468\n",
      "Epoch: [1/10], Step: [3501/3750], Train_loss: 0.08944078223833016\n",
      "Epoch: [1/10], Step: [3601/3750], Train_loss: 0.0889156861220383\n",
      "Epoch: [1/10], Step: [3701/3750], Train_loss: 0.08852258906020104\n",
      "1 epoch\n",
      "Epoch: [2/10], Step: [101/3750], Train_loss: 0.07337744487449527\n",
      "Epoch: [2/10], Step: [201/3750], Train_loss: 0.07345312068238855\n",
      "Epoch: [2/10], Step: [301/3750], Train_loss: 0.07235902639105916\n",
      "Epoch: [2/10], Step: [401/3750], Train_loss: 0.07142635548021645\n",
      "Epoch: [2/10], Step: [501/3750], Train_loss: 0.07089543982967735\n",
      "Epoch: [2/10], Step: [601/3750], Train_loss: 0.07139734875100355\n",
      "Epoch: [2/10], Step: [701/3750], Train_loss: 0.07128758392429778\n",
      "Epoch: [2/10], Step: [801/3750], Train_loss: 0.07132540543330833\n",
      "Precision macro: 0.631, Recall macro: 0.3775, F1 macro: 0.429 \n",
      "Precision micro: 0.7984, Recall micro: 0.5737, F1 micro: 0.6676 \n",
      "Epoch: [2/10], Step: [901/3750], Train_loss: 0.07114135173459847\n",
      "Epoch: [2/10], Step: [1001/3750], Train_loss: 0.07096504037827253\n",
      "Epoch: [2/10], Step: [1101/3750], Train_loss: 0.07088051707568493\n",
      "Epoch: [2/10], Step: [1201/3750], Train_loss: 0.07060833020601422\n",
      "Epoch: [2/10], Step: [1301/3750], Train_loss: 0.07043401504508578\n",
      "Epoch: [2/10], Step: [1401/3750], Train_loss: 0.07019735248094158\n",
      "Epoch: [2/10], Step: [1501/3750], Train_loss: 0.0699237474401792\n",
      "Epoch: [2/10], Step: [1601/3750], Train_loss: 0.0698857884737663\n",
      "Precision macro: 0.6289, Recall macro: 0.4005, F1 macro: 0.4639 \n",
      "Precision micro: 0.8183, Recall micro: 0.5864, F1 micro: 0.6832 \n",
      "Epoch: [2/10], Step: [1701/3750], Train_loss: 0.06991022340405513\n",
      "Epoch: [2/10], Step: [1801/3750], Train_loss: 0.06970078743269874\n",
      "Epoch: [2/10], Step: [1901/3750], Train_loss: 0.0696443982824291\n",
      "Epoch: [2/10], Step: [2001/3750], Train_loss: 0.0697121941326186\n",
      "Epoch: [2/10], Step: [2101/3750], Train_loss: 0.06957662433563244\n",
      "Epoch: [2/10], Step: [2201/3750], Train_loss: 0.06934220314449208\n",
      "Epoch: [2/10], Step: [2301/3750], Train_loss: 0.0692167019682086\n",
      "Epoch: [2/10], Step: [2401/3750], Train_loss: 0.06918714007052283\n",
      "Precision macro: 0.614, Recall macro: 0.4186, F1 macro: 0.477 \n",
      "Precision micro: 0.7706, Recall micro: 0.6223, F1 micro: 0.6886 \n",
      "Epoch: [2/10], Step: [2501/3750], Train_loss: 0.06903648950457573\n",
      "Epoch: [2/10], Step: [2601/3750], Train_loss: 0.06910293521789404\n",
      "Epoch: [2/10], Step: [2701/3750], Train_loss: 0.06905391639236498\n",
      "Epoch: [2/10], Step: [2801/3750], Train_loss: 0.06903710773946452\n",
      "Epoch: [2/10], Step: [2901/3750], Train_loss: 0.06907868283126375\n",
      "Epoch: [2/10], Step: [3001/3750], Train_loss: 0.06919112371156613\n",
      "Epoch: [2/10], Step: [3101/3750], Train_loss: 0.06919218502277809\n",
      "Epoch: [2/10], Step: [3201/3750], Train_loss: 0.06917589930293616\n",
      "Precision macro: 0.6527, Recall macro: 0.4261, F1 macro: 0.4906 \n",
      "Precision micro: 0.7945, Recall micro: 0.6035, F1 micro: 0.6859 \n",
      "Epoch: [2/10], Step: [3301/3750], Train_loss: 0.0692061954460135\n",
      "Epoch: [2/10], Step: [3401/3750], Train_loss: 0.06918370261295315\n",
      "Epoch: [2/10], Step: [3501/3750], Train_loss: 0.06893462279598628\n",
      "Epoch: [2/10], Step: [3601/3750], Train_loss: 0.06892208090853981\n",
      "Epoch: [2/10], Step: [3701/3750], Train_loss: 0.06894143385133027\n",
      "2 epoch\n",
      "Epoch: [3/10], Step: [101/3750], Train_loss: 0.06587925985455513\n",
      "Epoch: [3/10], Step: [201/3750], Train_loss: 0.06434067171067\n",
      "Epoch: [3/10], Step: [301/3750], Train_loss: 0.0649776037906607\n",
      "Epoch: [3/10], Step: [401/3750], Train_loss: 0.0635681984713301\n",
      "Epoch: [3/10], Step: [501/3750], Train_loss: 0.06455535142868757\n",
      "Epoch: [3/10], Step: [601/3750], Train_loss: 0.06549416186908881\n",
      "Epoch: [3/10], Step: [701/3750], Train_loss: 0.06585959375569864\n",
      "Epoch: [3/10], Step: [801/3750], Train_loss: 0.06608039534767159\n",
      "Precision macro: 0.6987, Recall macro: 0.4445, F1 macro: 0.506 \n",
      "Precision micro: 0.7809, Recall micro: 0.6423, F1 micro: 0.7048 \n",
      "Epoch: [3/10], Step: [901/3750], Train_loss: 0.06563523553725746\n",
      "Epoch: [3/10], Step: [1001/3750], Train_loss: 0.0656580324601382\n",
      "Epoch: [3/10], Step: [1101/3750], Train_loss: 0.06583011738109318\n",
      "Epoch: [3/10], Step: [1201/3750], Train_loss: 0.06570505003910512\n",
      "Epoch: [3/10], Step: [1301/3750], Train_loss: 0.06569449432480794\n",
      "Epoch: [3/10], Step: [1401/3750], Train_loss: 0.0657010961004666\n",
      "Epoch: [3/10], Step: [1501/3750], Train_loss: 0.06580110174293319\n",
      "Epoch: [3/10], Step: [1601/3750], Train_loss: 0.06584781145909802\n",
      "Precision macro: 0.6451, Recall macro: 0.4575, F1 macro: 0.5245 \n",
      "Precision micro: 0.7978, Recall micro: 0.616, F1 micro: 0.6952 \n",
      "Epoch: [3/10], Step: [1701/3750], Train_loss: 0.06588515442083864\n",
      "Epoch: [3/10], Step: [1801/3750], Train_loss: 0.06590900863831242\n",
      "Epoch: [3/10], Step: [1901/3750], Train_loss: 0.06577465123447933\n",
      "Epoch: [3/10], Step: [2001/3750], Train_loss: 0.06578347582276911\n",
      "Epoch: [3/10], Step: [2101/3750], Train_loss: 0.06574639273563489\n",
      "Epoch: [3/10], Step: [2201/3750], Train_loss: 0.06572131681107832\n",
      "Epoch: [3/10], Step: [2301/3750], Train_loss: 0.06574109416394291\n",
      "Epoch: [3/10], Step: [2401/3750], Train_loss: 0.06569090445234906\n",
      "Precision macro: 0.731, Recall macro: 0.4675, F1 macro: 0.5385 \n",
      "Precision micro: 0.8181, Recall micro: 0.6147, F1 micro: 0.702 \n",
      "Epoch: [3/10], Step: [2501/3750], Train_loss: 0.0656743662552908\n",
      "Epoch: [3/10], Step: [2601/3750], Train_loss: 0.06571046220013299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/10], Step: [2701/3750], Train_loss: 0.06552771217793364\n",
      "Epoch: [3/10], Step: [2801/3750], Train_loss: 0.06544209234167024\n",
      "Epoch: [3/10], Step: [2901/3750], Train_loss: 0.0653168842885322\n",
      "Epoch: [3/10], Step: [3001/3750], Train_loss: 0.06512825241576259\n",
      "Epoch: [3/10], Step: [3101/3750], Train_loss: 0.06515249841591163\n",
      "Epoch: [3/10], Step: [3201/3750], Train_loss: 0.06510429043744807\n",
      "Precision macro: 0.6885, Recall macro: 0.4909, F1 macro: 0.5522 \n",
      "Precision micro: 0.7915, Recall micro: 0.6314, F1 micro: 0.7024 \n",
      "Epoch: [3/10], Step: [3301/3750], Train_loss: 0.06522483741314235\n",
      "Epoch: [3/10], Step: [3401/3750], Train_loss: 0.0651538763421259\n",
      "Epoch: [3/10], Step: [3501/3750], Train_loss: 0.06504358973314188\n",
      "Epoch: [3/10], Step: [3601/3750], Train_loss: 0.06501947713873556\n",
      "Epoch: [3/10], Step: [3701/3750], Train_loss: 0.06499398450253883\n",
      "3 epoch\n",
      "Epoch: [4/10], Step: [101/3750], Train_loss: 0.06441170182079077\n",
      "Epoch: [4/10], Step: [201/3750], Train_loss: 0.06234286240302026\n",
      "Epoch: [4/10], Step: [301/3750], Train_loss: 0.06265895482773583\n",
      "Epoch: [4/10], Step: [401/3750], Train_loss: 0.061993819610215724\n",
      "Epoch: [4/10], Step: [501/3750], Train_loss: 0.06252970761060715\n",
      "Epoch: [4/10], Step: [601/3750], Train_loss: 0.06203019124921411\n",
      "Epoch: [4/10], Step: [701/3750], Train_loss: 0.062158867292372244\n",
      "Epoch: [4/10], Step: [801/3750], Train_loss: 0.062159678078023715\n",
      "Precision macro: 0.6561, Recall macro: 0.476, F1 macro: 0.5328 \n",
      "Precision micro: 0.8123, Recall micro: 0.6242, F1 micro: 0.7059 \n",
      "Epoch: [4/10], Step: [901/3750], Train_loss: 0.06178764173036648\n",
      "Epoch: [4/10], Step: [1001/3750], Train_loss: 0.061825542734004556\n",
      "Epoch: [4/10], Step: [1101/3750], Train_loss: 0.06214795035818084\n",
      "Epoch: [4/10], Step: [1201/3750], Train_loss: 0.06211762968683615\n",
      "Epoch: [4/10], Step: [1301/3750], Train_loss: 0.06238244120246516\n",
      "Epoch: [4/10], Step: [1401/3750], Train_loss: 0.0622909167668383\n",
      "Epoch: [4/10], Step: [1501/3750], Train_loss: 0.06228035294823348\n",
      "Epoch: [4/10], Step: [1601/3750], Train_loss: 0.06245712237840053\n",
      "Precision macro: 0.724, Recall macro: 0.4521, F1 macro: 0.5296 \n",
      "Precision micro: 0.8249, Recall micro: 0.6109, F1 micro: 0.702 \n",
      "Epoch: [4/10], Step: [1701/3750], Train_loss: 0.06261122377205859\n",
      "Epoch: [4/10], Step: [1801/3750], Train_loss: 0.06280751798871077\n",
      "Epoch: [4/10], Step: [1901/3750], Train_loss: 0.06277214123818435\n",
      "Epoch: [4/10], Step: [2001/3750], Train_loss: 0.06274884930811822\n",
      "Epoch: [4/10], Step: [2101/3750], Train_loss: 0.06295645520445846\n",
      "Epoch: [4/10], Step: [2201/3750], Train_loss: 0.06306405734016814\n",
      "Epoch: [4/10], Step: [2301/3750], Train_loss: 0.06303922967823303\n",
      "Epoch: [4/10], Step: [2401/3750], Train_loss: 0.06284927922300994\n",
      "Precision macro: 0.7224, Recall macro: 0.4754, F1 macro: 0.5463 \n",
      "Precision micro: 0.8071, Recall micro: 0.6187, F1 micro: 0.7004 \n",
      "Epoch: [4/10], Step: [2501/3750], Train_loss: 0.06286734029054641\n",
      "Epoch: [4/10], Step: [2601/3750], Train_loss: 0.06295011459849775\n",
      "Epoch: [4/10], Step: [2701/3750], Train_loss: 0.06295854241690702\n",
      "Epoch: [4/10], Step: [2801/3750], Train_loss: 0.06286536087614618\n",
      "Epoch: [4/10], Step: [2901/3750], Train_loss: 0.0627036325171076\n",
      "Epoch: [4/10], Step: [3001/3750], Train_loss: 0.06269339696131647\n",
      "Epoch: [4/10], Step: [3101/3750], Train_loss: 0.06272466805613329\n",
      "Epoch: [4/10], Step: [3201/3750], Train_loss: 0.06272308719693682\n",
      "Precision macro: 0.7073, Recall macro: 0.456, F1 macro: 0.531 \n",
      "Precision micro: 0.8192, Recall micro: 0.6204, F1 micro: 0.7061 \n",
      "Epoch: [4/10], Step: [3301/3750], Train_loss: 0.06262848056401267\n",
      "Epoch: [4/10], Step: [3401/3750], Train_loss: 0.06272979134145905\n",
      "Epoch: [4/10], Step: [3501/3750], Train_loss: 0.06276513132452964\n",
      "Epoch: [4/10], Step: [3601/3750], Train_loss: 0.06273075294111752\n",
      "Epoch: [4/10], Step: [3701/3750], Train_loss: 0.06270391679494768\n",
      "4 epoch\n",
      "Epoch: [5/10], Step: [101/3750], Train_loss: 0.05941762310452759\n",
      "Epoch: [5/10], Step: [201/3750], Train_loss: 0.05839703710284084\n",
      "Epoch: [5/10], Step: [301/3750], Train_loss: 0.059704159352307516\n",
      "Epoch: [5/10], Step: [401/3750], Train_loss: 0.05953281755326316\n",
      "Epoch: [5/10], Step: [501/3750], Train_loss: 0.060008740082383154\n",
      "Epoch: [5/10], Step: [601/3750], Train_loss: 0.060862654043982424\n",
      "Epoch: [5/10], Step: [701/3750], Train_loss: 0.060765161304069414\n",
      "Epoch: [5/10], Step: [801/3750], Train_loss: 0.06094802404870279\n",
      "Precision macro: 0.7609, Recall macro: 0.4806, F1 macro: 0.5599 \n",
      "Precision micro: 0.8112, Recall micro: 0.6285, F1 micro: 0.7082 \n",
      "Epoch: [5/10], Step: [901/3750], Train_loss: 0.06119349525310099\n",
      "Epoch: [5/10], Step: [1001/3750], Train_loss: 0.0612288835356012\n",
      "Epoch: [5/10], Step: [1101/3750], Train_loss: 0.06096821017309346\n",
      "Epoch: [5/10], Step: [1201/3750], Train_loss: 0.06105529066796104\n",
      "Epoch: [5/10], Step: [1301/3750], Train_loss: 0.06131720929334943\n",
      "Epoch: [5/10], Step: [1401/3750], Train_loss: 0.061444822519219344\n",
      "Epoch: [5/10], Step: [1501/3750], Train_loss: 0.06166800469408432\n",
      "Epoch: [5/10], Step: [1601/3750], Train_loss: 0.06143960466375575\n",
      "Precision macro: 0.6909, Recall macro: 0.5014, F1 macro: 0.5586 \n",
      "Precision micro: 0.7898, Recall micro: 0.6418, F1 micro: 0.7081 \n",
      "Epoch: [5/10], Step: [1701/3750], Train_loss: 0.06140511744491318\n",
      "Epoch: [5/10], Step: [1801/3750], Train_loss: 0.061294733403871454\n",
      "Epoch: [5/10], Step: [1901/3750], Train_loss: 0.06134510089006079\n",
      "Epoch: [5/10], Step: [2001/3750], Train_loss: 0.061362828076817096\n",
      "Epoch: [5/10], Step: [2101/3750], Train_loss: 0.06122034011098246\n",
      "Epoch: [5/10], Step: [2201/3750], Train_loss: 0.0612765011144802\n",
      "Epoch: [5/10], Step: [2301/3750], Train_loss: 0.06141466042553277\n",
      "Epoch: [5/10], Step: [2401/3750], Train_loss: 0.06126774147657367\n",
      "Precision macro: 0.7372, Recall macro: 0.519, F1 macro: 0.5848 \n",
      "Precision micro: 0.8126, Recall micro: 0.6545, F1 micro: 0.725 \n",
      "Epoch: [5/10], Step: [2501/3750], Train_loss: 0.061185725216940046\n",
      "Epoch: [5/10], Step: [2601/3750], Train_loss: 0.06110483495124544\n",
      "Epoch: [5/10], Step: [2701/3750], Train_loss: 0.06103316691497134\n",
      "Epoch: [5/10], Step: [2801/3750], Train_loss: 0.061245475282693015\n",
      "Epoch: [5/10], Step: [2901/3750], Train_loss: 0.0613113873275318\n",
      "Epoch: [5/10], Step: [3001/3750], Train_loss: 0.061216159109957514\n",
      "Epoch: [5/10], Step: [3101/3750], Train_loss: 0.06108588800885745\n",
      "Epoch: [5/10], Step: [3201/3750], Train_loss: 0.061048128935799466\n",
      "Precision macro: 0.7377, Recall macro: 0.5052, F1 macro: 0.5696 \n",
      "Precision micro: 0.8206, Recall micro: 0.6296, F1 micro: 0.7125 \n",
      "Epoch: [5/10], Step: [3301/3750], Train_loss: 0.061090166355680785\n",
      "Epoch: [5/10], Step: [3401/3750], Train_loss: 0.06114774943028083\n",
      "Epoch: [5/10], Step: [3501/3750], Train_loss: 0.06100641992728093\n",
      "Epoch: [5/10], Step: [3601/3750], Train_loss: 0.0610394862437776\n",
      "Epoch: [5/10], Step: [3701/3750], Train_loss: 0.061013914089826114\n",
      "5 epoch\n",
      "Epoch: [6/10], Step: [101/3750], Train_loss: 0.058357251212000846\n",
      "Epoch: [6/10], Step: [201/3750], Train_loss: 0.057966339392587544\n",
      "Epoch: [6/10], Step: [301/3750], Train_loss: 0.05810492610558868\n",
      "Epoch: [6/10], Step: [401/3750], Train_loss: 0.05837823453359306\n",
      "Epoch: [6/10], Step: [501/3750], Train_loss: 0.058541501166298984\n",
      "Epoch: [6/10], Step: [601/3750], Train_loss: 0.059051968216275175\n",
      "Epoch: [6/10], Step: [701/3750], Train_loss: 0.05971575019881129\n",
      "Epoch: [6/10], Step: [801/3750], Train_loss: 0.05982437999919057\n",
      "Precision macro: 0.7337, Recall macro: 0.5006, F1 macro: 0.5701 \n",
      "Precision micro: 0.8099, Recall micro: 0.6353, F1 micro: 0.712 \n",
      "Epoch: [6/10], Step: [901/3750], Train_loss: 0.05984774306416511\n",
      "Epoch: [6/10], Step: [1001/3750], Train_loss: 0.05978557381406426\n",
      "Epoch: [6/10], Step: [1101/3750], Train_loss: 0.059474588710476055\n",
      "Epoch: [6/10], Step: [1201/3750], Train_loss: 0.05938165550585836\n",
      "Epoch: [6/10], Step: [1301/3750], Train_loss: 0.05930481902896785\n",
      "Epoch: [6/10], Step: [1401/3750], Train_loss: 0.0591366094836433\n",
      "Epoch: [6/10], Step: [1501/3750], Train_loss: 0.05906774948599438\n",
      "Epoch: [6/10], Step: [1601/3750], Train_loss: 0.059278689486091024\n",
      "Precision macro: 0.6979, Recall macro: 0.5174, F1 macro: 0.5775 \n",
      "Precision micro: 0.8081, Recall micro: 0.66, F1 micro: 0.7266 \n",
      "Epoch: [6/10], Step: [1701/3750], Train_loss: 0.059061579371199886\n",
      "Epoch: [6/10], Step: [1801/3750], Train_loss: 0.059090478111886315\n",
      "Epoch: [6/10], Step: [1901/3750], Train_loss: 0.059149908519497044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/10], Step: [2001/3750], Train_loss: 0.059073541360907256\n",
      "Epoch: [6/10], Step: [2101/3750], Train_loss: 0.05922750950391804\n",
      "Epoch: [6/10], Step: [2201/3750], Train_loss: 0.059279354941099885\n",
      "Epoch: [6/10], Step: [2301/3750], Train_loss: 0.05939319543297524\n",
      "Epoch: [6/10], Step: [2401/3750], Train_loss: 0.05957529905446184\n",
      "Precision macro: 0.7032, Recall macro: 0.4747, F1 macro: 0.5483 \n",
      "Precision micro: 0.82, Recall micro: 0.6255, F1 micro: 0.7097 \n",
      "Epoch: [6/10], Step: [2501/3750], Train_loss: 0.05946115468703211\n",
      "Epoch: [6/10], Step: [2601/3750], Train_loss: 0.05955935770394997\n",
      "Epoch: [6/10], Step: [2701/3750], Train_loss: 0.059543673600656565\n",
      "Epoch: [6/10], Step: [2801/3750], Train_loss: 0.059592375712402695\n",
      "Epoch: [6/10], Step: [2901/3750], Train_loss: 0.05959198617825991\n",
      "Epoch: [6/10], Step: [3001/3750], Train_loss: 0.059646558834860725\n",
      "Epoch: [6/10], Step: [3101/3750], Train_loss: 0.05958305053983725\n",
      "Epoch: [6/10], Step: [3201/3750], Train_loss: 0.059534772010811136\n",
      "Precision macro: 0.7086, Recall macro: 0.5085, F1 macro: 0.5727 \n",
      "Precision micro: 0.8048, Recall micro: 0.647, F1 micro: 0.7173 \n",
      "Epoch: [6/10], Step: [3301/3750], Train_loss: 0.059550228798479744\n",
      "Epoch: [6/10], Step: [3401/3750], Train_loss: 0.05961847367586897\n",
      "Epoch: [6/10], Step: [3501/3750], Train_loss: 0.05964905261567661\n",
      "Epoch: [6/10], Step: [3601/3750], Train_loss: 0.05968788702403092\n",
      "Epoch: [6/10], Step: [3701/3750], Train_loss: 0.05963931515963899\n",
      "6 epoch\n",
      "Epoch: [7/10], Step: [101/3750], Train_loss: 0.05825961161404848\n",
      "Epoch: [7/10], Step: [201/3750], Train_loss: 0.05756223098374903\n",
      "Epoch: [7/10], Step: [301/3750], Train_loss: 0.05718282341336211\n",
      "Epoch: [7/10], Step: [401/3750], Train_loss: 0.05603890335652977\n",
      "Epoch: [7/10], Step: [501/3750], Train_loss: 0.05713766255602241\n",
      "Epoch: [7/10], Step: [601/3750], Train_loss: 0.05763143873152633\n",
      "Epoch: [7/10], Step: [701/3750], Train_loss: 0.05735216839239001\n",
      "Epoch: [7/10], Step: [801/3750], Train_loss: 0.0570820200140588\n",
      "Precision macro: 0.7276, Recall macro: 0.519, F1 macro: 0.576 \n",
      "Precision micro: 0.8048, Recall micro: 0.6548, F1 micro: 0.7221 \n",
      "Epoch: [7/10], Step: [901/3750], Train_loss: 0.057203389933953686\n",
      "Epoch: [7/10], Step: [1001/3750], Train_loss: 0.05698907661996782\n",
      "Epoch: [7/10], Step: [1101/3750], Train_loss: 0.05704189694909887\n",
      "Epoch: [7/10], Step: [1201/3750], Train_loss: 0.05758795259365191\n",
      "Epoch: [7/10], Step: [1301/3750], Train_loss: 0.057441615520331724\n",
      "Epoch: [7/10], Step: [1401/3750], Train_loss: 0.05755364662501961\n",
      "Epoch: [7/10], Step: [1501/3750], Train_loss: 0.05761200998350978\n",
      "Epoch: [7/10], Step: [1601/3750], Train_loss: 0.057782477169530465\n",
      "Precision macro: 0.7083, Recall macro: 0.525, F1 macro: 0.5871 \n",
      "Precision micro: 0.7972, Recall micro: 0.6646, F1 micro: 0.7249 \n",
      "Epoch: [7/10], Step: [1701/3750], Train_loss: 0.057882215201526005\n",
      "Epoch: [7/10], Step: [1801/3750], Train_loss: 0.05803020189438636\n",
      "Epoch: [7/10], Step: [1901/3750], Train_loss: 0.057840811658013414\n",
      "Epoch: [7/10], Step: [2001/3750], Train_loss: 0.05794120280328207\n",
      "Epoch: [7/10], Step: [2101/3750], Train_loss: 0.05796460413396181\n",
      "Epoch: [7/10], Step: [2201/3750], Train_loss: 0.058064122446177695\n",
      "Epoch: [7/10], Step: [2301/3750], Train_loss: 0.05812804485691468\n",
      "Epoch: [7/10], Step: [2401/3750], Train_loss: 0.05836569354306751\n",
      "Precision macro: 0.7129, Recall macro: 0.5259, F1 macro: 0.5836 \n",
      "Precision micro: 0.79, Recall micro: 0.6622, F1 micro: 0.7205 \n",
      "Epoch: [7/10], Step: [2501/3750], Train_loss: 0.05852214850764722\n",
      "Epoch: [7/10], Step: [2601/3750], Train_loss: 0.05851131501661327\n",
      "Epoch: [7/10], Step: [2701/3750], Train_loss: 0.058547093463820164\n",
      "Epoch: [7/10], Step: [2801/3750], Train_loss: 0.058559204788180066\n",
      "Epoch: [7/10], Step: [2901/3750], Train_loss: 0.05847952125321046\n",
      "Epoch: [7/10], Step: [3001/3750], Train_loss: 0.05850115364898617\n",
      "Epoch: [7/10], Step: [3101/3750], Train_loss: 0.05855042517951299\n",
      "Epoch: [7/10], Step: [3201/3750], Train_loss: 0.05848853411720484\n",
      "Precision macro: 0.7402, Recall macro: 0.5102, F1 macro: 0.5805 \n",
      "Precision micro: 0.8183, Recall micro: 0.6459, F1 micro: 0.7219 \n",
      "Epoch: [7/10], Step: [3301/3750], Train_loss: 0.058525587335426475\n",
      "Epoch: [7/10], Step: [3401/3750], Train_loss: 0.05844138604575111\n",
      "Epoch: [7/10], Step: [3501/3750], Train_loss: 0.05846263723753925\n",
      "Epoch: [7/10], Step: [3601/3750], Train_loss: 0.058503023361036965\n",
      "Epoch: [7/10], Step: [3701/3750], Train_loss: 0.05853839161566685\n",
      "7 epoch\n",
      "Epoch: [8/10], Step: [101/3750], Train_loss: 0.05460030447691679\n",
      "Epoch: [8/10], Step: [201/3750], Train_loss: 0.05525338429491967\n",
      "Epoch: [8/10], Step: [301/3750], Train_loss: 0.054909646492451426\n",
      "Epoch: [8/10], Step: [401/3750], Train_loss: 0.05629835472907871\n",
      "Epoch: [8/10], Step: [501/3750], Train_loss: 0.056101734101772306\n",
      "Epoch: [8/10], Step: [601/3750], Train_loss: 0.056581467735425876\n",
      "Epoch: [8/10], Step: [701/3750], Train_loss: 0.056337937823762854\n",
      "Epoch: [8/10], Step: [801/3750], Train_loss: 0.05659537167230155\n",
      "Precision macro: 0.7172, Recall macro: 0.5335, F1 macro: 0.5941 \n",
      "Precision micro: 0.7863, Recall micro: 0.6695, F1 micro: 0.7232 \n",
      "Epoch: [8/10], Step: [901/3750], Train_loss: 0.05650361982018997\n",
      "Epoch: [8/10], Step: [1001/3750], Train_loss: 0.05646516632428393\n",
      "Epoch: [8/10], Step: [1101/3750], Train_loss: 0.0563596486939456\n",
      "Epoch: [8/10], Step: [1201/3750], Train_loss: 0.05646381364475625\n",
      "Epoch: [8/10], Step: [1301/3750], Train_loss: 0.05639299431493362\n",
      "Epoch: [8/10], Step: [1401/3750], Train_loss: 0.05611590124234291\n",
      "Epoch: [8/10], Step: [1501/3750], Train_loss: 0.0561297391836221\n",
      "Epoch: [8/10], Step: [1601/3750], Train_loss: 0.05629603020468494\n",
      "Precision macro: 0.7215, Recall macro: 0.5422, F1 macro: 0.5966 \n",
      "Precision micro: 0.7882, Recall micro: 0.6831, F1 micro: 0.7319 \n",
      "Epoch: [8/10], Step: [1701/3750], Train_loss: 0.05642548180103083\n",
      "Epoch: [8/10], Step: [1801/3750], Train_loss: 0.05672729978674195\n",
      "Epoch: [8/10], Step: [1901/3750], Train_loss: 0.056723599002794604\n",
      "Epoch: [8/10], Step: [2001/3750], Train_loss: 0.05679178732028231\n",
      "Epoch: [8/10], Step: [2101/3750], Train_loss: 0.05713223840819583\n",
      "Epoch: [8/10], Step: [2201/3750], Train_loss: 0.057289106518622825\n",
      "Epoch: [8/10], Step: [2301/3750], Train_loss: 0.05732253120037849\n",
      "Epoch: [8/10], Step: [2401/3750], Train_loss: 0.05720609227195382\n",
      "Precision macro: 0.7275, Recall macro: 0.5259, F1 macro: 0.5872 \n",
      "Precision micro: 0.8262, Recall micro: 0.6401, F1 micro: 0.7213 \n",
      "Epoch: [8/10], Step: [2501/3750], Train_loss: 0.05720358254648745\n",
      "Epoch: [8/10], Step: [2601/3750], Train_loss: 0.05726426523142996\n",
      "Epoch: [8/10], Step: [2701/3750], Train_loss: 0.057289273730237726\n",
      "Epoch: [8/10], Step: [2801/3750], Train_loss: 0.05749662366462872\n",
      "Epoch: [8/10], Step: [2901/3750], Train_loss: 0.05757692161722687\n",
      "Epoch: [8/10], Step: [3001/3750], Train_loss: 0.05762319471656034\n",
      "Epoch: [8/10], Step: [3101/3750], Train_loss: 0.05772625042516137\n",
      "Epoch: [8/10], Step: [3201/3750], Train_loss: 0.05771166291291593\n",
      "Precision macro: 0.7192, Recall macro: 0.5154, F1 macro: 0.5801 \n",
      "Precision micro: 0.809, Recall micro: 0.654, F1 micro: 0.7233 \n",
      "Epoch: [8/10], Step: [3301/3750], Train_loss: 0.05766207152695367\n",
      "Epoch: [8/10], Step: [3401/3750], Train_loss: 0.05774821543737369\n",
      "Epoch: [8/10], Step: [3501/3750], Train_loss: 0.05780088240999196\n",
      "Epoch: [8/10], Step: [3601/3750], Train_loss: 0.057854498588842236\n",
      "Epoch: [8/10], Step: [3701/3750], Train_loss: 0.05791664368598848\n",
      "8 epoch\n",
      "Epoch: [9/10], Step: [101/3750], Train_loss: 0.06173832137137651\n",
      "Epoch: [9/10], Step: [201/3750], Train_loss: 0.056480116951279344\n",
      "Epoch: [9/10], Step: [301/3750], Train_loss: 0.055567641267552974\n",
      "Epoch: [9/10], Step: [401/3750], Train_loss: 0.05630215189186856\n",
      "Epoch: [9/10], Step: [501/3750], Train_loss: 0.055791177662089464\n",
      "Epoch: [9/10], Step: [601/3750], Train_loss: 0.0560716669075191\n",
      "Epoch: [9/10], Step: [701/3750], Train_loss: 0.05654824067173259\n",
      "Epoch: [9/10], Step: [801/3750], Train_loss: 0.05698269330197945\n",
      "Precision macro: 0.7238, Recall macro: 0.5337, F1 macro: 0.5959 \n",
      "Precision micro: 0.8094, Recall micro: 0.6491, F1 micro: 0.7204 \n",
      "Epoch: [9/10], Step: [901/3750], Train_loss: 0.056737464046519664\n",
      "Epoch: [9/10], Step: [1001/3750], Train_loss: 0.05687173551414162\n",
      "Epoch: [9/10], Step: [1101/3750], Train_loss: 0.05675685285378925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/10], Step: [1201/3750], Train_loss: 0.05666752916799548\n",
      "Epoch: [9/10], Step: [1301/3750], Train_loss: 0.056849758916247925\n",
      "Epoch: [9/10], Step: [1401/3750], Train_loss: 0.05692270423718063\n",
      "Epoch: [9/10], Step: [1501/3750], Train_loss: 0.05685936599380026\n",
      "Epoch: [9/10], Step: [1601/3750], Train_loss: 0.056914731292345096\n",
      "Precision macro: 0.7495, Recall macro: 0.5331, F1 macro: 0.5975 \n",
      "Precision micro: 0.8056, Recall micro: 0.6689, F1 micro: 0.7309 \n",
      "Epoch: [9/10], Step: [1701/3750], Train_loss: 0.05699464652057299\n",
      "Epoch: [9/10], Step: [1801/3750], Train_loss: 0.05685272740226032\n",
      "Epoch: [9/10], Step: [1901/3750], Train_loss: 0.05675114683390252\n",
      "Epoch: [9/10], Step: [2001/3750], Train_loss: 0.056715801470680165\n",
      "Epoch: [9/10], Step: [2101/3750], Train_loss: 0.05683346228030998\n",
      "Epoch: [9/10], Step: [2201/3750], Train_loss: 0.05676426460945302\n",
      "Epoch: [9/10], Step: [2301/3750], Train_loss: 0.05693780287756058\n",
      "Epoch: [9/10], Step: [2401/3750], Train_loss: 0.05686220048441707\n",
      "Precision macro: 0.7445, Recall macro: 0.5475, F1 macro: 0.6138 \n",
      "Precision micro: 0.7885, Recall micro: 0.6736, F1 micro: 0.7266 \n",
      "Epoch: [9/10], Step: [2501/3750], Train_loss: 0.056884649076499046\n",
      "Epoch: [9/10], Step: [2601/3750], Train_loss: 0.05674252835657591\n",
      "Epoch: [9/10], Step: [2701/3750], Train_loss: 0.05672594321598471\n",
      "Epoch: [9/10], Step: [2801/3750], Train_loss: 0.05675642768669474\n",
      "Epoch: [9/10], Step: [2901/3750], Train_loss: 0.056681921000849324\n",
      "Epoch: [9/10], Step: [3001/3750], Train_loss: 0.05678508693398908\n",
      "Epoch: [9/10], Step: [3101/3750], Train_loss: 0.056781377653411076\n",
      "Epoch: [9/10], Step: [3201/3750], Train_loss: 0.05681586442879052\n",
      "Precision macro: 0.7011, Recall macro: 0.5505, F1 macro: 0.6016 \n",
      "Precision micro: 0.8106, Recall micro: 0.6499, F1 micro: 0.7214 \n",
      "Epoch: [9/10], Step: [3301/3750], Train_loss: 0.056794657077442745\n",
      "Epoch: [9/10], Step: [3401/3750], Train_loss: 0.056857268451576064\n",
      "Epoch: [9/10], Step: [3501/3750], Train_loss: 0.05691881434260202\n",
      "Epoch: [9/10], Step: [3601/3750], Train_loss: 0.05699481310309946\n",
      "Epoch: [9/10], Step: [3701/3750], Train_loss: 0.05697876238226387\n",
      "9 epoch\n",
      "Epoch: [10/10], Step: [101/3750], Train_loss: 0.056071498915553096\n",
      "Epoch: [10/10], Step: [201/3750], Train_loss: 0.055658227447420355\n",
      "Epoch: [10/10], Step: [301/3750], Train_loss: 0.05500086935857932\n",
      "Epoch: [10/10], Step: [401/3750], Train_loss: 0.05480874742614105\n",
      "Epoch: [10/10], Step: [501/3750], Train_loss: 0.05477729623951018\n",
      "Epoch: [10/10], Step: [601/3750], Train_loss: 0.055188380762313805\n",
      "Epoch: [10/10], Step: [701/3750], Train_loss: 0.05515390572271177\n",
      "Epoch: [10/10], Step: [801/3750], Train_loss: 0.055225492746103555\n",
      "Precision macro: 0.7282, Recall macro: 0.5297, F1 macro: 0.5918 \n",
      "Precision micro: 0.797, Recall micro: 0.6657, F1 micro: 0.7255 \n",
      "Epoch: [10/10], Step: [901/3750], Train_loss: 0.055904148918473055\n",
      "Epoch: [10/10], Step: [1001/3750], Train_loss: 0.05589863927382976\n",
      "Epoch: [10/10], Step: [1101/3750], Train_loss: 0.05579352151009847\n",
      "Epoch: [10/10], Step: [1201/3750], Train_loss: 0.0560389587841928\n",
      "Epoch: [10/10], Step: [1301/3750], Train_loss: 0.05601615283494959\n",
      "Epoch: [10/10], Step: [1401/3750], Train_loss: 0.056012797367626\n",
      "Epoch: [10/10], Step: [1501/3750], Train_loss: 0.05584013899291555\n",
      "Epoch: [10/10], Step: [1601/3750], Train_loss: 0.0559501882782206\n",
      "Precision macro: 0.7144, Recall macro: 0.5552, F1 macro: 0.6155 \n",
      "Precision micro: 0.797, Recall micro: 0.6809, F1 micro: 0.7344 \n",
      "Epoch: [10/10], Step: [1701/3750], Train_loss: 0.05571795652565711\n",
      "Epoch: [10/10], Step: [1801/3750], Train_loss: 0.05558046417207354\n",
      "Epoch: [10/10], Step: [1901/3750], Train_loss: 0.05535695046774651\n",
      "Epoch: [10/10], Step: [2001/3750], Train_loss: 0.05531907319743186\n",
      "Epoch: [10/10], Step: [2101/3750], Train_loss: 0.05544667244134914\n",
      "Epoch: [10/10], Step: [2201/3750], Train_loss: 0.05552307382141325\n",
      "Epoch: [10/10], Step: [2301/3750], Train_loss: 0.055586412688312326\n",
      "Epoch: [10/10], Step: [2401/3750], Train_loss: 0.055715478075047335\n",
      "Precision macro: 0.7274, Recall macro: 0.5616, F1 macro: 0.6212 \n",
      "Precision micro: 0.8102, Recall micro: 0.6576, F1 micro: 0.726 \n",
      "Epoch: [10/10], Step: [2501/3750], Train_loss: 0.055917892721295354\n",
      "Epoch: [10/10], Step: [2601/3750], Train_loss: 0.05599853887079427\n",
      "Epoch: [10/10], Step: [2701/3750], Train_loss: 0.0560258115503799\n",
      "Epoch: [10/10], Step: [2801/3750], Train_loss: 0.05599151535159243\n",
      "Epoch: [10/10], Step: [2901/3750], Train_loss: 0.055963446430797724\n",
      "Epoch: [10/10], Step: [3001/3750], Train_loss: 0.05611280599205444\n",
      "Epoch: [10/10], Step: [3101/3750], Train_loss: 0.05618704078448636\n",
      "Epoch: [10/10], Step: [3201/3750], Train_loss: 0.05616472496185452\n",
      "Precision macro: 0.7103, Recall macro: 0.5476, F1 macro: 0.6015 \n",
      "Precision micro: 0.8228, Recall micro: 0.6432, F1 micro: 0.722 \n",
      "Epoch: [10/10], Step: [3301/3750], Train_loss: 0.05616594365924935\n",
      "Epoch: [10/10], Step: [3401/3750], Train_loss: 0.05612066023010651\n",
      "Epoch: [10/10], Step: [3501/3750], Train_loss: 0.056145723475542454\n",
      "Epoch: [10/10], Step: [3601/3750], Train_loss: 0.056151633440232315\n",
      "Epoch: [10/10], Step: [3701/3750], Train_loss: 0.05619325878916661\n"
     ]
    }
   ],
   "source": [
    "SAVE_MODEL = False\n",
    "\n",
    "batch_size = 8\n",
    "lr = 0.01\n",
    "num_epochs = 15\n",
    "\n",
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU(),\n",
    "}\n",
    "    \n",
    "result = {\n",
    "    \"optimizer\": \"SWA\", \n",
    "    \"num_hidden\": options[\"num_layers\"],\n",
    "    \"dim_hidden\": options[\"mid_features\"],\n",
    "    \"dropout_rate\": options[\"dropout_rate\"],\n",
    "    \"learning_rate\": lr,\n",
    "    \"num_epochs\": num_epochs\n",
    "}\n",
    "\n",
    "print(\"\\n\", result)\n",
    "\n",
    "# uncommen if train from scratch\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = SWA(base_opt) \n",
    "optimizer = base_opt\n",
    "\n",
    "# train the model\n",
    "model_name = \"mixed_en_hi_ru_\" + \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "print(model_name)\n",
    "train_loader, val_loader = get_train_val_loader(\n",
    "    wiki_tensor_dataset[\"train\"], wiki_tensor_dataset[\"val\"], \n",
    "    collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    ")\n",
    "# loaders = create_data_loaders_for_model(wiki_loaders[\"train\"], wiki_loaders[\"val\"])\n",
    "# # create dataloader\n",
    "# wiki_loaders = {}\n",
    "\n",
    "\n",
    "# for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "#     wiki_loaders[split] = DataLoader(\n",
    "#         wiki_dataset, \n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=True, \n",
    "#         collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "#     )\n",
    "\n",
    "metrics_dict = train_model(train_loader, val_loader, model, criterion, optimizer, options, device,\n",
    "                num_epochs=10, model_name=\"model\", save_model=False)\n",
    "# train_model(\n",
    "#     wiki_loaders, model, criterion, optimizer, options=options, num_epochs=num_epochs, \n",
    "#     model_name=model_name, save_model=SAVE_MODEL\n",
    "# )\n",
    "result.update(metrics_dict)\n",
    "\n",
    "# results_df = results_df.append(result, ignore_index=True)\n",
    "#     results_df.to_csv(\"results/results_tuning_2_3_layers_maxlen_500.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  8 11:36:50 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    On   | 00000000:05:00.0 Off |                  N/A |\r\n",
      "| 27%   32C    P2    39W / 180W |   2817MiB /  8119MiB |      0%   E. Process |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     14686      C   ...2476/miniconda3/envs/my_base/bin/python  2807MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "return TextData(self.input_tensors[idx], self.input_len[idx], self.target_tensors[idx])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextData(tokens=tensor([1806916, 1806707, 1903632, 1692551, 1784451, 2005419, 1906915, 1801147,\n",
       "        1903632, 1692551, 1906914, 1906915, 1642437, 2020582, 1841007, 1962348,\n",
       "        2021359, 1863465, 1858591, 1797053, 2005419, 1842387, 1692551, 1829783,\n",
       "        2033847, 1602461, 1710390, 1692551, 1687349, 1863321, 1806916, 1692551,\n",
       "        1790135, 1806916, 1858591, 1806916, 1708689, 1752630, 1806916, 1718252,\n",
       "        2026730, 1919642, 1692551, 1932671, 1602461, 1710390, 1692551, 1940157,\n",
       "        1806916, 1772961, 1790512, 1784451, 2005419, 1906915, 1602461, 1710390,\n",
       "        1989399, 1982812, 1784451, 2005419, 1906915, 1982812, 1903632, 1692551,\n",
       "        1975487]), len=tensor([65.]), target=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"val_hi\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/mz2476/topic-modeling/topic-modeling/baseline/preprocess.py\u001b[0m(119)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    117 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    118 \u001b[0;31m        \u001b[0;31m# return a (input, target) tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 119 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mTextData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    120 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    121 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> idx\n",
      "'val_hi'\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.695, Recall macro: 0.4866, F1 macro: 0.5611 \n",
      "Precision micro: 0.8123, Recall micro: 0.6333, F1 micro: 0.7117 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mz2476/miniconda3/envs/my_base/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_tensor_dataset[\"val_hi\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print_results(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.7587, Recall macro: 0.5095, F1 macro: 0.5842 \n",
      "Precision micro: 0.8372, Recall micro: 0.6806, F1 micro: 0.7508 \n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_loaders[\"val\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.749, Recall macro: 0.5404, F1 macro: 0.6103 \n",
      "Precision micro: 0.8413, Recall micro: 0.7104, F1 micro: 0.7703 \n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_loaders[\"val_en\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.7055, Recall macro: 0.4951, F1 macro: 0.5599 \n",
      "Precision micro: 0.8311, Recall micro: 0.6838, F1 micro: 0.7503 \n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_loaders[\"val_ru\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.7107, Recall macro: 0.4928, F1 macro: 0.5669 \n",
      "Precision micro: 0.8391, Recall micro: 0.6475, F1 micro: 0.731 \n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_loaders[\"val_hi\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model\n",
    "# torch.save({\n",
    "#         'state_dict': model.state_dict(),\n",
    "#         'opts': options,\n",
    "#         'plot_cache': plot_cache,\n",
    "#             }, \n",
    "#     f'{PATH_TO_MODELS_FOLDER}en_ru_mixed_model_train_10000.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model_names = {\n",
    "    \"frozen\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_frozen.pth\",\n",
    "    },\n",
    "    \"finetuned\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_init_pretrained.pth\",   \n",
    "    },\n",
    "    \"trained\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\",   \n",
    "    },\n",
    "}\n",
    "\n",
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU(),\n",
    "}\n",
    "\n",
    "for model_name in dict_model_names.keys():\n",
    "    model = FinalModel(options)\n",
    "    # load the state dict from file\n",
    "    file_name = dict_model_names[model_name][\"file_name\"]\n",
    "    model.load_state_dict(torch.load(\n",
    "        f\"{PATH_TO_MODELS_FOLDER}{file_name}\",\n",
    "        map_location=torch.device('cpu')\n",
    "    ))\n",
    "    model.to(device)\n",
    "    # save model to dict\n",
    "    dict_model_names[model_name][\"model\"] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- frozen\n",
      "Precision macro: 0.3503, Recall macro: 0.1435, F1 macro: 0.1715 \n",
      "Precision micro: 0.7678, Recall micro: 0.2693, F1 micro: 0.3987 \n",
      "--- finetuned\n",
      "Precision macro: 0.6015, Recall macro: 0.4704, F1 macro: 0.516 \n",
      "Precision micro: 0.8187, Recall micro: 0.7468, F1 micro: 0.7811 \n",
      "--- trained\n",
      "Precision macro: 0.5225, Recall macro: 0.3148, F1 macro: 0.3643 \n",
      "Precision micro: 0.8348, Recall micro: 0.6714, F1 micro: 0.7443 \n"
     ]
    }
   ],
   "source": [
    "from utils import test_model\n",
    "\n",
    "for model_name in dict_model_names.keys():\n",
    "    model = dict_model_names[model_name][\"model\"]\n",
    "    # print aggregated metrics\n",
    "    metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "    metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "    print(\"---\", model_name)\n",
    "    print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "        metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "    ))\n",
    "    print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "        metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "    ))\n",
    "    \n",
    "    # save per class tables\n",
    "    df_per_class_metrics = utils.create_per_class_tables(\n",
    "        wiki_loaders[\"val\"], model, device, classes, threshold=0.5\n",
    "    )\n",
    "    dict_model_names[model_name][\"df_results\"] = df_per_class_metrics\n",
    "    # SAVE to file\n",
    "#     df_per_class_metrics.to_csv(f\"results/ru_per_class_metrics_val_{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>count</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Culture.Arts</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1434</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Culture.Broadcasting</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1418</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Culture.Crafts and hobbies</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1437</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Culture.Entertainment</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1386</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.626506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Culture.Food and drink</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1433</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Culture.Games and toys</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1425</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Culture.Internet culture</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Culture.Language and literature</td>\n",
       "      <td>552.0</td>\n",
       "      <td>848</td>\n",
       "      <td>58</td>\n",
       "      <td>494</td>\n",
       "      <td>43</td>\n",
       "      <td>0.919926</td>\n",
       "      <td>0.894928</td>\n",
       "      <td>0.907254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Culture.Media</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Culture.Music</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.786885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Culture.Performing arts</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1425</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Culture.Philosophy and religion</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1372</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Culture.Plastic arts</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1423</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Culture.Sports</td>\n",
       "      <td>237.0</td>\n",
       "      <td>1189</td>\n",
       "      <td>17</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.928270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Culture.Visual arts</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1412</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Geography.Africa</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1406</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.425532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Geography.Americas</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1219</td>\n",
       "      <td>96</td>\n",
       "      <td>107</td>\n",
       "      <td>21</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.527094</td>\n",
       "      <td>0.646526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Geography.Antarctica</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Geography.Asia</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>84</td>\n",
       "      <td>164</td>\n",
       "      <td>44</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.719298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Geography.Bodies of water</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Geography.Europe</td>\n",
       "      <td>567.0</td>\n",
       "      <td>746</td>\n",
       "      <td>99</td>\n",
       "      <td>468</td>\n",
       "      <td>130</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.803433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Geography.Landforms</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1434</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Geography.Maps</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Geography.Oceania</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1417</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Geography.Parks</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1436</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>History_And_Society.Business and economics</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1410</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>History_And_Society.Education</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1430</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>History_And_Society.History and society</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1278</td>\n",
       "      <td>102</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.382353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>History_And_Society.Military and warfare</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1372</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.720721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>History_And_Society.Politics and government</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1383</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>History_And_Society.Transportation</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1368</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.728814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>STEM.Biology</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1365</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>STEM.Chemistry</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1436</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>STEM.Engineering</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>STEM.Geosciences</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1424</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>STEM.Information science</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>STEM.Mathematics</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1440</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>STEM.Medicine</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1414</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>STEM.Meteorology</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1441</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>STEM.Physics</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>STEM.Science</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1427</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>STEM.Space</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1413</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.983051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>STEM.Technology</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1379</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>STEM.Time</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1437</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class_name  count    TN   FN   TP   FP  \\\n",
       "0                                  Culture.Arts    9.0  1434    8    1    0   \n",
       "1                          Culture.Broadcasting   25.0  1418   22    3    0   \n",
       "2                    Culture.Crafts and hobbies    6.0  1437    6    0    0   \n",
       "3                         Culture.Entertainment   50.0  1386   24   26    7   \n",
       "4                        Culture.Food and drink    9.0  1433    4    5    1   \n",
       "5                        Culture.Games and toys   18.0  1425    5   13    0   \n",
       "6                      Culture.Internet culture    1.0  1442    1    0    0   \n",
       "7               Culture.Language and literature  552.0   848   58  494   43   \n",
       "8                                 Culture.Media    1.0  1442    1    0    0   \n",
       "9                                 Culture.Music   58.0  1369   10   48   16   \n",
       "10                      Culture.Performing arts   18.0  1425   17    1    0   \n",
       "11              Culture.Philosophy and religion   59.0  1372   34   25   12   \n",
       "12                         Culture.Plastic arts   18.0  1423   15    3    2   \n",
       "13                               Culture.Sports  237.0  1189   17  220   17   \n",
       "14                          Culture.Visual arts   30.0  1412   25    5    1   \n",
       "15                             Geography.Africa   31.0  1406   21   10    6   \n",
       "16                           Geography.Americas  203.0  1219   96  107   21   \n",
       "17                         Geography.Antarctica    1.0  1442    1    0    0   \n",
       "18                               Geography.Asia  248.0  1151   84  164   44   \n",
       "19                    Geography.Bodies of water    8.0  1435    8    0    0   \n",
       "20                             Geography.Europe  567.0   746   99  468  130   \n",
       "21                          Geography.Landforms    9.0  1434    9    0    0   \n",
       "22                               Geography.Maps    0.0  1443    0    0    0   \n",
       "23                            Geography.Oceania   25.0  1417   22    3    1   \n",
       "24                              Geography.Parks    7.0  1436    7    0    0   \n",
       "25   History_And_Society.Business and economics   31.0  1410   22    9    2   \n",
       "26                History_And_Society.Education   13.0  1430   13    0    0   \n",
       "27      History_And_Society.History and society  141.0  1278  102   39   24   \n",
       "28     History_And_Society.Military and warfare   66.0  1372   26   40    5   \n",
       "29  History_And_Society.Politics and government   47.0  1383   31   16   13   \n",
       "30           History_And_Society.Transportation   69.0  1368   26   43    6   \n",
       "31                                 STEM.Biology   72.0  1365   20   52    6   \n",
       "32                               STEM.Chemistry    5.0  1436    2    3    2   \n",
       "33                             STEM.Engineering    1.0  1442    1    0    0   \n",
       "34                             STEM.Geosciences   19.0  1424   13    6    0   \n",
       "35                     STEM.Information science    1.0  1442    1    0    0   \n",
       "36                             STEM.Mathematics    3.0  1440    3    0    0   \n",
       "37                                STEM.Medicine   27.0  1414   12   15    2   \n",
       "38                             STEM.Meteorology    2.0  1441    2    0    0   \n",
       "39                                 STEM.Physics    8.0  1435    8    0    0   \n",
       "40                                 STEM.Science   16.0  1427   16    0    0   \n",
       "41                                   STEM.Space   30.0  1413    1   29    0   \n",
       "42                              STEM.Technology   53.0  1379   21   32   11   \n",
       "43                                    STEM.Time    6.0  1437    6    0    0   \n",
       "\n",
       "    precision    recall        f1  \n",
       "0    1.000000  0.111111  0.200000  \n",
       "1    1.000000  0.120000  0.214286  \n",
       "2    0.000000  0.000000  0.000000  \n",
       "3    0.787879  0.520000  0.626506  \n",
       "4    0.833333  0.555556  0.666667  \n",
       "5    1.000000  0.722222  0.838710  \n",
       "6    0.000000  0.000000  0.000000  \n",
       "7    0.919926  0.894928  0.907254  \n",
       "8    0.000000  0.000000  0.000000  \n",
       "9    0.750000  0.827586  0.786885  \n",
       "10   1.000000  0.055556  0.105263  \n",
       "11   0.675676  0.423729  0.520833  \n",
       "12   0.600000  0.166667  0.260870  \n",
       "13   0.928270  0.928270  0.928270  \n",
       "14   0.833333  0.166667  0.277778  \n",
       "15   0.625000  0.322581  0.425532  \n",
       "16   0.835938  0.527094  0.646526  \n",
       "17   0.000000  0.000000  0.000000  \n",
       "18   0.788462  0.661290  0.719298  \n",
       "19   0.000000  0.000000  0.000000  \n",
       "20   0.782609  0.825397  0.803433  \n",
       "21   0.000000  0.000000  0.000000  \n",
       "22   0.000000  0.000000  0.000000  \n",
       "23   0.750000  0.120000  0.206897  \n",
       "24   0.000000  0.000000  0.000000  \n",
       "25   0.818182  0.290323  0.428571  \n",
       "26   0.000000  0.000000  0.000000  \n",
       "27   0.619048  0.276596  0.382353  \n",
       "28   0.888889  0.606061  0.720721  \n",
       "29   0.551724  0.340426  0.421053  \n",
       "30   0.877551  0.623188  0.728814  \n",
       "31   0.896552  0.722222  0.800000  \n",
       "32   0.600000  0.600000  0.600000  \n",
       "33   0.000000  0.000000  0.000000  \n",
       "34   1.000000  0.315789  0.480000  \n",
       "35   0.000000  0.000000  0.000000  \n",
       "36   0.000000  0.000000  0.000000  \n",
       "37   0.882353  0.555556  0.681818  \n",
       "38   0.000000  0.000000  0.000000  \n",
       "39   0.000000  0.000000  0.000000  \n",
       "40   0.000000  0.000000  0.000000  \n",
       "41   1.000000  0.966667  0.983051  \n",
       "42   0.744186  0.603774  0.666667  \n",
       "43   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_model_names[\"trained\"][\"df_results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model. Use pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PATH_TO_MODELS_FOLDER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1314704cba1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPRETRAINED_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH_TO_MODELS_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"en_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m best_params = {\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'SWA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'num_hidden'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PATH_TO_MODELS_FOLDER' is not defined"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL = PATH_TO_MODELS_FOLDER + \"en_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\"\n",
    "\n",
    "best_params = {\n",
    "    'optimizer': 'SWA',\n",
    "    'num_hidden': 2,\n",
    "    'dim_hidden': 150,\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": best_params[\"num_hidden\"],\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": best_params[\"dim_hidden\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "pretrained_state_dict = torch.load(PRETRAINED_MODEL)\n",
    "\n",
    "# take pretrained params\n",
    "model.layer_out[0].weight.data = pretrained_state_dict['layer_out.0.weight']\n",
    "model.layer_out[0].bias.data = pretrained_state_dict['layer_out.0.bias']\n",
    "model.layer_out[2].weight.data = pretrained_state_dict['layer_out.2.weight']\n",
    "model.layer_out[2].bias.data = pretrained_state_dict['layer_out.2.bias']\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (layer_bag_of_words): BagOfWords(\n",
       "    (embed_e): Embedding(376365, 300)\n",
       "  )\n",
       "  (layer_out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained params:\n",
      "\n",
      "Precision macro: 0.3503, Recall macro: 0.1435, F1 macro: 0.1715 \n",
      "Precision micro: 0.7678, Recall micro: 0.2693, F1 micro: 0.3987 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Using pretrained params:\\n\")\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save frozen model\n",
    "# model_name = \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_frozen\"\n",
    "# torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune on Russian articles OR train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, \n",
    "                num_epochs=10, device=device, model_name=\"model\", save_model=False):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch, \"epoch\")\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 100 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "                ))\n",
    "                print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "                ))\n",
    "\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "                    if save_model:\n",
    "                        optimizer.swap_swa_sgd()\n",
    "                        torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}{model_name}.pth\")\n",
    "                        print('Model Saved')\n",
    "                        print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 150, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10\n",
      "0 epoch\n",
      "Epoch: [1/10], Step: [101/361], Train_loss: 0.16394229903817176\n",
      "Precision macro: 0.03774863222660023, Recall macro: 0.018300674097775547, F1 macro: 0.021573619594354748 \n",
      "Precision micro: 0.7364085667215815, Recall micro: 0.15964285714285714, F1 micro: 0.26240093924273555 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [201/361], Train_loss: 0.13773150239139795\n",
      "Precision macro: 0.10007624693922357, Recall macro: 0.051343324197594804, F1 macro: 0.058254934882816585 \n",
      "Precision micro: 0.8041958041958042, Recall micro: 0.32857142857142857, F1 micro: 0.4665314401622718 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [301/361], Train_loss: 0.12528121824065844\n",
      "Precision macro: 0.11422569054993983, Recall macro: 0.07616071949318902, F1 macro: 0.0814212545866872 \n",
      "Precision micro: 0.7639405204460966, Recall micro: 0.44035714285714284, F1 micro: 0.5586769370185772 \n",
      "Model Saved\n",
      "\n",
      "1 epoch\n",
      "Epoch: [2/10], Step: [101/361], Train_loss: 0.08439337681978941\n",
      "Precision macro: 0.2259704472226207, Recall macro: 0.12202399873145725, F1 macro: 0.13984515352159987 \n",
      "Precision micro: 0.8107951247823564, Recall micro: 0.49892857142857144, F1 micro: 0.6177315940747292 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [201/361], Train_loss: 0.08378258358687163\n",
      "Precision macro: 0.22118828289098233, Recall macro: 0.1281634524974851, F1 macro: 0.1469414566364466 \n",
      "Precision micro: 0.8135977337110482, Recall micro: 0.5128571428571429, F1 micro: 0.6291347207009859 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [301/361], Train_loss: 0.08123884287973245\n",
      "Precision macro: 0.27261289098023156, Recall macro: 0.17644959279690553, F1 macro: 0.20337303885016708 \n",
      "Precision micro: 0.8267246061922868, Recall micro: 0.5435714285714286, F1 micro: 0.6558931264813618 \n",
      "Model Saved\n",
      "\n",
      "2 epoch\n",
      "Epoch: [3/10], Step: [101/361], Train_loss: 0.07191145554184913\n",
      "Precision macro: 0.2895974070788268, Recall macro: 0.17686146914913883, F1 macro: 0.20885121877858814 \n",
      "Precision micro: 0.8395130049806309, Recall micro: 0.5417857142857143, F1 micro: 0.6585630562187974 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [201/361], Train_loss: 0.07064960964024067\n",
      "Precision macro: 0.29619340720331605, Recall macro: 0.20542084782192085, F1 macro: 0.22922444832589467 \n",
      "Precision micro: 0.798941798941799, Recall micro: 0.5932142857142857, F1 micro: 0.6808772289403566 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [301/361], Train_loss: 0.06952822438130776\n",
      "Precision macro: 0.3070634989462676, Recall macro: 0.20644742267524885, F1 macro: 0.23889940929966008 \n",
      "Precision micro: 0.837851929092805, Recall micro: 0.5739285714285715, F1 micro: 0.6812208562950403 \n",
      "Model Saved\n",
      "\n",
      "3 epoch\n",
      "Epoch: [4/10], Step: [101/361], Train_loss: 0.06629168134182692\n",
      "Precision macro: 0.3492394480898027, Recall macro: 0.22179094217784695, F1 macro: 0.2588039965868109 \n",
      "Precision micro: 0.833249623304872, Recall micro: 0.5925, F1 micro: 0.6925485284909206 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [201/361], Train_loss: 0.06485318504273892\n",
      "Precision macro: 0.3462230750912139, Recall macro: 0.24015617474623657, F1 macro: 0.26623782756821224 \n",
      "Precision micro: 0.8003605227579991, Recall micro: 0.6342857142857142, F1 micro: 0.7077106993424985 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [301/361], Train_loss: 0.06421152345836162\n",
      "Precision macro: 0.31794633521595944, Recall macro: 0.21844136175437623, F1 macro: 0.2532988159303398 \n",
      "Precision micro: 0.8557291666666667, Recall micro: 0.5867857142857142, F1 micro: 0.6961864406779661 \n",
      "4 epoch\n",
      "Epoch: [5/10], Step: [101/361], Train_loss: 0.060102666020393374\n",
      "Precision macro: 0.4070176861200382, Recall macro: 0.24270818026856134, F1 macro: 0.28334595071318947 \n",
      "Precision micro: 0.8317073170731707, Recall micro: 0.6089285714285714, F1 micro: 0.7030927835051547 \n",
      "Epoch: [5/10], Step: [201/361], Train_loss: 0.05926943069323897\n",
      "Precision macro: 0.4135324773107264, Recall macro: 0.2895450175422235, F1 macro: 0.32576959204145445 \n",
      "Precision micro: 0.8051724137931034, Recall micro: 0.6671428571428571, F1 micro: 0.7296874999999999 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [5/10], Step: [301/361], Train_loss: 0.06048339587946733\n",
      "Precision macro: 0.4034359985267267, Recall macro: 0.270323402247836, F1 macro: 0.30505130652825174 \n",
      "Precision micro: 0.8126410835214447, Recall micro: 0.6428571428571429, F1 micro: 0.7178464606181456 \n",
      "5 epoch\n",
      "Epoch: [6/10], Step: [101/361], Train_loss: 0.060187061801552776\n",
      "Precision macro: 0.43651624782180887, Recall macro: 0.2629348111940314, F1 macro: 0.3098926459443124 \n",
      "Precision micro: 0.8226851851851852, Recall micro: 0.6346428571428572, F1 micro: 0.7165322580645163 \n",
      "Epoch: [6/10], Step: [201/361], Train_loss: 0.05916553379967809\n",
      "Precision macro: 0.4126941476744955, Recall macro: 0.28875025895168543, F1 macro: 0.3283953782019932 \n",
      "Precision micro: 0.8266968325791855, Recall micro: 0.6525, F1 micro: 0.7293413173652693 \n",
      "Epoch: [6/10], Step: [301/361], Train_loss: 0.058904961807032426\n",
      "Precision macro: 0.4221681664130214, Recall macro: 0.2808827728641066, F1 macro: 0.320912943281777 \n",
      "Precision micro: 0.8444551128180509, Recall micro: 0.6282142857142857, F1 micro: 0.7204587343845995 \n",
      "6 epoch\n",
      "Epoch: [7/10], Step: [101/361], Train_loss: 0.05516995422542095\n",
      "Precision macro: 0.4479517735517136, Recall macro: 0.2982639774431381, F1 macro: 0.3440791961949927 \n",
      "Precision micro: 0.8461538461538461, Recall micro: 0.6364285714285715, F1 micro: 0.726457399103139 \n",
      "Epoch: [7/10], Step: [201/361], Train_loss: 0.05530497262254357\n",
      "Precision macro: 0.48861033392508485, Recall macro: 0.32991290047722815, F1 macro: 0.3728456485221045 \n",
      "Precision micro: 0.8097287989668532, Recall micro: 0.6717857142857143, F1 micro: 0.7343353503806364 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [7/10], Step: [301/361], Train_loss: 0.055703533900280794\n",
      "Precision macro: 0.4459257606977166, Recall macro: 0.2969567512728942, F1 macro: 0.3396355991051108 \n",
      "Precision micro: 0.8453510436432637, Recall micro: 0.6364285714285715, F1 micro: 0.726161369193154 \n",
      "7 epoch\n",
      "Epoch: [8/10], Step: [101/361], Train_loss: 0.05569138001650572\n",
      "Precision macro: 0.4503778572875683, Recall macro: 0.30340966614675097, F1 macro: 0.34804913750514305 \n",
      "Precision micro: 0.8425047438330171, Recall micro: 0.6342857142857142, F1 micro: 0.723716381418093 \n",
      "Epoch: [8/10], Step: [201/361], Train_loss: 0.05552267179824412\n",
      "Precision macro: 0.46426841415043046, Recall macro: 0.3031969241703691, F1 macro: 0.34682616872960276 \n",
      "Precision micro: 0.8372420262664165, Recall micro: 0.6375, F1 micro: 0.7238442822384428 \n",
      "Epoch: [8/10], Step: [301/361], Train_loss: 0.055627569252004225\n",
      "Precision macro: 0.4524664831247662, Recall macro: 0.30728966649502715, F1 macro: 0.3496334905751391 \n",
      "Precision micro: 0.8486590038314177, Recall micro: 0.6328571428571429, F1 micro: 0.7250409165302784 \n",
      "8 epoch\n",
      "Epoch: [9/10], Step: [101/361], Train_loss: 0.052995356991887094\n",
      "Precision macro: 0.4715299645030264, Recall macro: 0.3294818635677095, F1 macro: 0.3683402841638571 \n",
      "Precision micro: 0.8296263345195729, Recall micro: 0.6660714285714285, F1 micro: 0.738906497622821 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [9/10], Step: [201/361], Train_loss: 0.05272909205406904\n",
      "Precision macro: 0.4637896776755621, Recall macro: 0.34044027627034923, F1 macro: 0.37779569252020073 \n",
      "Precision micro: 0.8141135972461274, Recall micro: 0.6757142857142857, F1 micro: 0.7384855581576893 \n",
      "Epoch: [9/10], Step: [301/361], Train_loss: 0.05378380390504996\n",
      "Precision macro: 0.4631322923213206, Recall macro: 0.3191371797532066, F1 macro: 0.3618655039152929 \n",
      "Precision micro: 0.8155296229802513, Recall micro: 0.6489285714285714, F1 micro: 0.7227525855210819 \n",
      "9 epoch\n",
      "Epoch: [10/10], Step: [101/361], Train_loss: 0.05339182615280151\n",
      "Precision macro: 0.4592743071930398, Recall macro: 0.3048405479664032, F1 macro: 0.3516314961952886 \n",
      "Precision micro: 0.8420812414422638, Recall micro: 0.6589285714285714, F1 micro: 0.7393307954317772 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [201/361], Train_loss: 0.05300246635451913\n",
      "Precision macro: 0.5239493758717199, Recall macro: 0.31237480248666355, F1 macro: 0.3629296034626441 \n",
      "Precision micro: 0.8358608385370205, Recall micro: 0.6692857142857143, F1 micro: 0.7433558111860373 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [301/361], Train_loss: 0.052334477826952934\n",
      "Precision macro: 0.4913573921795875, Recall macro: 0.3465703832230578, F1 macro: 0.39287590565202724 \n",
      "Precision micro: 0.8040262941659819, Recall micro: 0.6989285714285715, F1 micro: 0.7478028276652656 \n",
      "Model Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAVE_MODEL = False\n",
    "\n",
    "num_epochs = 10\n",
    "    \n",
    "result = {\n",
    "    \"optimizer\": best_params[\"optimizer\"], \n",
    "    \"num_hidden\": best_params[\"num_hidden\"],\n",
    "    \"dim_hidden\": best_params[\"dim_hidden\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"learning_rate\": best_params[\"learning_rate\"],\n",
    "    \"num_epochs\": num_epochs\n",
    "}\n",
    "print(\"\\n\", result)\n",
    "\n",
    "# uncommen if train from scratch\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "optimizer = SWA(base_opt) \n",
    "\n",
    "# train the model\n",
    "model_name = \"ru_\" + \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "print(model_name)\n",
    "metrics_dict = train_model(\n",
    "    wiki_loaders, model, criterion, optimizer, num_epochs=num_epochs, \n",
    "    model_name=model_name, save_model=SAVE_MODEL\n",
    ")\n",
    "result.update(metrics_dict)\n",
    "\n",
    "# results_df = results_df.append(result, ignore_index=True)\n",
    "#     results_df.to_csv(\"results/results_tuning_2_3_layers_maxlen_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.4914, Recall macro: 0.3466, F1 macro: 0.3929 \n",
      "Precision micro: 0.804, Recall micro: 0.6989, F1 micro: 0.7478 \n"
     ]
    }
   ],
   "source": [
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # take only pretrained params of layer_out\n",
    "# pretrained_params = ['layer_out.0.weight', 'layer_out.0.bias', 'layer_out.2.weight', 'layer_out.2.bias']\n",
    "# for param in pretrained_params:\n",
    "#     model.state_dict()[param] = pretrained_state_dict[param]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
