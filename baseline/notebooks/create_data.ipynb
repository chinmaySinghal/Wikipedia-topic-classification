{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import nltk\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'wiki_parser' from '/home/mz2476/topic-modeling/topic-modeling/baseline/wiki_parser.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import utils\n",
    "reload(utils)\n",
    "\n",
    "import wiki_parser\n",
    "reload(wiki_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/aligned_datasets/\"\n",
    "\n",
    "SAVE = False\n",
    "DEBUG = True\n",
    "LOAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess it and save \n",
    "Output files:\n",
    "`vocab_train_en.pt`, `wiki_tensor_dataset_en.pt`, `classes_list.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data (the functions are in `preprocess.py`):\n",
    "<ol>\n",
    "    <li> Remove rows with missing labels. </li>\n",
    "    <li> Remove rows with no tokens. </li>\n",
    "    <li> Create a set of all categories. Binarize the labels. </li>\n",
    "    <li> Split in train/val/test. </li>\n",
    "    <li> Build vocabulary for train. </li>\n",
    "</ol>\n",
    "\n",
    "Make DataLoader:\n",
    "<ol>\n",
    "    <li> Tokenize train/val/test. </li>\n",
    "    <li> Create batches using collate function that pads the short sentences. </li>\n",
    "</ol>\n",
    "\n",
    "Use pretrained embeddings:\n",
    "<ol>\n",
    "    <li> Load pretrained embeddings. </li>\n",
    "    <li> Create embedding matrix for given vocabulary. Words that are in given vocabualry but not in pretrained embeddings have zero embedding vector. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = False\n",
    "SAVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english \n",
      " {'json': 'wikitext_topics_en_filtered.json', 'wiki_df': 'wikitext_tokenized_text_sections_outlinks_en.p', 'vocab': 'data_for_model/vocab_all_en.pt', 'monolingual_train': 'data_for_model/df_wiki_monolingual_train_30000_en.pt', 'multilingual_train': 'data_for_model/df_wiki_multilingual_train_10000_en.pt', 'val': 'data_for_model/df_wiki_valid_1000_en.pt', 'test': 'data_for_model/df_wiki_test_en.pt'}\n",
      "russian \n",
      " {'json': 'wikitext_topics_ru_filtered.json', 'wiki_df': 'wikitext_tokenized_text_sections_outlinks_ru.p', 'vocab': 'data_for_model/vocab_all_ru.pt', 'monolingual_train': 'data_for_model/df_wiki_monolingual_train_30000_ru.pt', 'multilingual_train': 'data_for_model/df_wiki_multilingual_train_10000_ru.pt', 'val': 'data_for_model/df_wiki_valid_1000_ru.pt', 'test': 'data_for_model/df_wiki_test_ru.pt'}\n",
      "hindi \n",
      " {'json': 'wikitext_topics_hi_filtered.json', 'wiki_df': 'wikitext_tokenized_text_sections_outlinks_hi.p', 'vocab': 'data_for_model/vocab_all_hi.pt', 'monolingual_train': 'data_for_model/df_wiki_monolingual_train_30000_hi.pt', 'multilingual_train': 'data_for_model/df_wiki_multilingual_train_10000_hi.pt', 'val': 'data_for_model/df_wiki_valid_1000_hi.pt', 'test': 'data_for_model/df_wiki_test_hi.pt'}\n",
      "Percentage of articles with no mid_level_categories: 0.03 (998 articles)\n",
      "Percentage of articles with no tokens: 0.00082 (27 articles)\n",
      "Percentage of articles with no mid_level_categories: 0.029 (994 articles)\n",
      "Percentage of articles with no tokens: 9.2e-05 (3 articles)\n",
      "Percentage of articles with no mid_level_categories: 0.03 (995 articles)\n",
      "Percentage of articles with no tokens: 0.00061 (20 articles)\n",
      "Num of articles initially: \n",
      " dict_keys(['english', 'russian', 'hindi']) \n",
      " [32798, 32714, 32622]\n",
      "Num of articles after intersection: \n",
      " 32487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mz2476/topic-modeling/topic-modeling/baseline/preprocess.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cur_dict[\"wiki_df\"].sort_values(by=[\"QID\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:  data_for_model/vocab_all_en.pt\n",
      "Saved:  data_for_model/vocab_all_ru.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mz2476/miniconda3/envs/my_base/lib/python3.7/site-packages/ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:  data_for_model/vocab_all_hi.pt\n",
      "Saved:,  data_for_model/df_wiki_monolingual_train_30000_en.pt\n",
      "Saved:,  data_for_model/df_wiki_multilingual_train_10000_en.pt\n",
      "Saved:,  data_for_model/df_wiki_valid_1000_en.pt\n",
      "Saved:,  data_for_model/df_wiki_test_en.pt\n",
      "Saved:,  data_for_model/df_wiki_monolingual_train_30000_ru.pt\n",
      "Saved:,  data_for_model/df_wiki_multilingual_train_10000_ru.pt\n",
      "Saved:,  data_for_model/df_wiki_valid_1000_ru.pt\n",
      "Saved:,  data_for_model/df_wiki_test_ru.pt\n",
      "Saved:,  data_for_model/df_wiki_monolingual_train_30000_hi.pt\n",
      "Saved:,  data_for_model/df_wiki_multilingual_train_10000_hi.pt\n",
      "Saved:,  data_for_model/df_wiki_valid_1000_hi.pt\n",
      "Saved:,  data_for_model/df_wiki_test_hi.pt\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from preprocess import (create_dict_of_tensor_datasets,\n",
    "                        create_lookups_for_vocab, create_vocab_from_tokens,\n",
    "                        remove_non_common_articles_and_sort_by_QID,\n",
    "                        remove_rows_with_empty_column)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/aligned_datasets/\"\n",
    "\n",
    "# Load list of classes\n",
    "classes_list = torch.load(PATH_TO_DATA_FOLDER + '45_classes_list.pt')\n",
    "\n",
    "SAVE = True\n",
    "DEBUG = True\n",
    "LOAD = False\n",
    "\n",
    "monolingual_train_size = 30000\n",
    "multilingual_train_size = 10000\n",
    "val_size = 1000\n",
    "\n",
    "SEED = 57\n",
    "\n",
    "LANGUAGES_LIST = [\"english\", \"russian\", \"hindi\"]\n",
    "LANGUAGES_DICT = defaultdict(dict)\n",
    "\n",
    "for language in LANGUAGES_LIST:\n",
    "    language_code = language[:2]\n",
    "    FILE_NAMES_DICT = {\n",
    "        \"json\": f\"wikitext_topics_{language_code}_filtered.json\",\n",
    "        \"wiki_df\": f\"wikitext_tokenized_text_sections_outlinks_{language_code}.p\",\n",
    "        \"vocab\": f\"data_for_model/vocab_all_{language_code}.pt\",\n",
    "        \"monolingual_train\": f\"data_for_model/df_wiki_monolingual_train_{monolingual_train_size}_{language_code}.pt\",\n",
    "        \"multilingual_train\": f\"data_for_model/df_wiki_multilingual_train_{multilingual_train_size}_{language_code}.pt\",\n",
    "        \"val\": f\"data_for_model/df_wiki_valid_{val_size}_{language_code}.pt\",\n",
    "        \"test\": f\"data_for_model/df_wiki_test_{language_code}.pt\",\n",
    "#         \"tensor_dataset\": f\"wiki_tensor_dataset_{language_code}.pt\",\n",
    "    }\n",
    "    LANGUAGES_DICT[language][\"FILE_NAMES_DICT\"] = FILE_NAMES_DICT\n",
    "    print(language, \"\\n\", FILE_NAMES_DICT)\n",
    "\n",
    "# Load wiki_df and remove rows with empty labels/tokens\n",
    "for language in LANGUAGES_DICT.keys():\n",
    "    wiki_df = pkl.load(open(PATH_TO_DATA_FOLDER + LANGUAGES_DICT[language][\"FILE_NAMES_DICT\"][\"wiki_df\"], \"rb\"))\n",
    "    LANGUAGES_DICT[language][\"wiki_df\"] = wiki_df\n",
    "\n",
    "    remove_rows_with_empty_column(LANGUAGES_DICT[language][\"wiki_df\"], column=\"mid_level_categories\")\n",
    "    remove_rows_with_empty_column(LANGUAGES_DICT[language][\"wiki_df\"], column=\"tokens\")\n",
    "\n",
    "remove_non_common_articles_and_sort_by_QID(LANGUAGES_DICT)\n",
    "\n",
    "# Binarize labels, create vocabulary\n",
    "for cur_dict in LANGUAGES_DICT.values():\n",
    "    mlb = MultiLabelBinarizer(classes_list)\n",
    "    cur_dict[\"wiki_df\"][\"labels\"] =\\\n",
    "        list(mlb.fit_transform(cur_dict[\"wiki_df\"].mid_level_categories))\n",
    "    assert (mlb.classes_ == classes_list).all()\n",
    "\n",
    "    if LOAD:\n",
    "        vocab = torch.load(PATH_TO_DATA_FOLDER + cur_dict[\"FILE_NAMES_DICT\"][\"vocab\"])\n",
    "\n",
    "    if SAVE:\n",
    "        vocab = create_vocab_from_tokens(cur_dict[\"wiki_df\"][\"tokens\"])\n",
    "        torch.save(vocab, PATH_TO_DATA_FOLDER + cur_dict[\"FILE_NAMES_DICT\"][\"vocab\"])\n",
    "        print(\"Saved: \", cur_dict[\"FILE_NAMES_DICT\"][\"vocab\"])\n",
    "\n",
    "    index_to_word, word_to_index = create_lookups_for_vocab(vocab)\n",
    "    cur_dict[\"index_to_word\"], cur_dict[\"word_to_index\"] = index_to_word, word_to_index\n",
    "\n",
    "\n",
    "# train/val/test split by QID\n",
    "QIDs = LANGUAGES_DICT[\"english\"][\"wiki_df\"].QID\n",
    "monolingual_train_QIDs, val_and_test_QIDs = train_test_split(QIDs, train_size=monolingual_train_size, random_state=SEED)\n",
    "multilingual_train_QIDs, _ = train_test_split(monolingual_train_QIDs, train_size=multilingual_train_size, random_state=SEED)\n",
    "val_QIDs, test_QIDs = train_test_split(QIDs, train_size=val_size, random_state=SEED)\n",
    "test_size = len(test_QIDs)\n",
    "\n",
    "for cur_dict in LANGUAGES_DICT.values():\n",
    "    dict_of_dfs = defaultdict()\n",
    "\n",
    "    if LOAD:\n",
    "        dict_of_dfs[\"monolingual_train\"], dict_of_dfs[\"multilingual_train\"], dict_of_dfs[\"val\"], dict_of_dfs[\"test\"] =\\\n",
    "            (torch.load(PATH_TO_DATA_FOLDER + cur_dict[\"FILE_NAMES_DICT\"][\"monolingual_train\"]),\n",
    "             torch.load(PATH_TO_DATA_FOLDER + cur_dict[\"FILE_NAMES_DICT\"][\"multilingual_train\"]),\n",
    "             torch.load(PATH_TO_DATA_FOLDER + cur_dict[\"FILE_NAMES_DICT\"][\"val\"]),\n",
    "             torch.load(PATH_TO_DATA_FOLDER + cur_dict[\"FILE_NAMES_DICT\"][\"test\"]))\n",
    "\n",
    "    if SAVE:\n",
    "        dict_of_dfs[\"monolingual_train\"], dict_of_dfs[\"multilingual_train\"], dict_of_dfs[\"val\"], dict_of_dfs[\"test\"] =\\\n",
    "            (cur_dict[\"wiki_df\"][cur_dict[\"wiki_df\"].QID.isin(monolingual_train_QIDs)],\n",
    "             cur_dict[\"wiki_df\"][cur_dict[\"wiki_df\"].QID.isin(multilingual_train_QIDs)],\n",
    "             cur_dict[\"wiki_df\"][cur_dict[\"wiki_df\"].QID.isin(val_QIDs)],\n",
    "             cur_dict[\"wiki_df\"][cur_dict[\"wiki_df\"].QID.isin(test_QIDs)])\n",
    "        for name in dict_of_dfs.keys():\n",
    "            torch.save(dict_of_dfs[name], PATH_TO_DATA_FOLDER + cur_dict[\"FILE_NAMES_DICT\"][name])\n",
    "            print(\"Saved:, \", cur_dict[\"FILE_NAMES_DICT\"][name])\n",
    "    \n",
    "    cur_dict[\"dict_of_dfs\"] = dict_of_dfs\n",
    "\n",
    "# # Tokenized datasets\n",
    "# for cur_dict in LANGUAGES_DICT.values():\n",
    "#     create_dict_of_tensor_datasets(dict_of_dfs, word_to_index, max_num_tokens=None)\n",
    "# ADD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save wiki parsed df\n",
    "If `LOAD = False`, load json and process it to get df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = True\n",
    "DEBUG = True\n",
    "LOAD = False\n",
    "\n",
    "LANGUAGES_LIST = [\"english\", \"russian\", \"hindi\"]\n",
    "LANGUAGES_DICT = defaultdict(dict)\n",
    "\n",
    "for language in LANGUAGES_LIST:\n",
    "    language_code = language[:2]\n",
    "    FILE_NAMES_DICT = {\n",
    "        \"json\": f\"wikitext_topics_{language_code}_filtered.json\",\n",
    "        \"wiki_df\": f\"wikitext_tokenized_text_sections_outlinks_{language_code}.p\",\n",
    "    }\n",
    "    LANGUAGES_DICT[language][\"FILE_NAMES_DICT\"] = FILE_NAMES_DICT\n",
    "    print(language, \"\\n\", FILE_NAMES_DICT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = wiki_parser.Parser(LANGUAGE)\n",
    "wiki_df = parser.get_wiki_tokenized_dataset(\n",
    "    PATH_TO_DATA_FOLDER + FILE_NAMES_DICT[\"json\"],\n",
    "    extract_section=True, extract_outlinks=True, debug=DEBUG\n",
    ")\n",
    "if SAVE:\n",
    "    pkl.dump(wiki_df, open(\n",
    "        PATH_TO_DATA_FOLDER + FILE_NAMES_DICT[\"wiki_df\"], \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Binarize the labels\n",
    "# # labels list: mlb.classes_\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# wiki_df[\"labels\"] = list(mlb.fit_transform(wiki_df.mid_level_categories))\n",
    "\n",
    "# if SAVE:\n",
    "#     # SAVE classes list\n",
    "#     torch.save(mlb.classes_, PATH_TO_DATA_FOLDER + 'classes_list.pt')\n",
    "#     print(\"Saved.\")\n",
    "\n",
    "# # LOAD\n",
    "# classes = torch.load(PATH_TO_DATA_FOLDER + 'classes_list.pt')\n",
    "# mlb = MultiLabelBinarizer(classes)\n",
    "\n",
    "# print(classes)\n",
    "# wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79968/79968 [00:09<00:00, 8749.83it/s] \n",
      "100%|██████████| 9996/9996 [00:00<00:00, 12383.81it/s]\n",
      "100%|██████████| 9996/9996 [00:00<00:00, 13668.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "# CHANGE max number of tokens per article\n",
    "max_num_tokens = None\n",
    "\n",
    "# # specify vocabulary (word_to_index): 2 options\n",
    "vocab_name = \"vocab_train\" \n",
    "word_to_index = word_to_index_train\n",
    "# OR\n",
    "# vocab_name = \"vocab_all\"\n",
    "# word_to_index = word_to_index_all\n",
    "\n",
    "wiki_tokenized_datasets = {}\n",
    "wiki_tokenized_datasets['X_train'] = tokenize_dataset(wiki_train, word_to_index, max_num_tokens=max_num_tokens)\n",
    "wiki_tokenized_datasets['X_val'] = tokenize_dataset(wiki_valid, word_to_index, max_num_tokens=max_num_tokens)\n",
    "wiki_tokenized_datasets['X_test'] = tokenize_dataset(wiki_test, word_to_index, max_num_tokens=max_num_tokens)\n",
    "\n",
    "wiki_tokenized_datasets['y_train'] = list(wiki_train.labels)\n",
    "wiki_tokenized_datasets['y_val'] = list(wiki_valid.labels)\n",
    "wiki_tokenized_datasets['y_test'] = list(wiki_test.labels)\n",
    "\n",
    "wiki_tensor_dataset = {}\n",
    "wiki_tensor_dataset['train'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_train'], wiki_tokenized_datasets['y_train']\n",
    ")\n",
    "wiki_tensor_dataset['val'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_val'], wiki_tokenized_datasets['y_val']\n",
    ")\n",
    "wiki_tensor_dataset['test'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_test'], wiki_tokenized_datasets['y_test']\n",
    ")\n",
    "\n",
    "if SAVE:\n",
    "    # SAVE tensor datasets\n",
    "    torch.save(wiki_tensor_dataset, f'{PATH_TO_DATA_FOLDER}wiki_tensor_dataset_{vocab_name}_en.pt')\n",
    "    print(\"Saved.\")\n",
    "\n",
    "# LOAD\n",
    "wiki_tensor_dataset = torch.load(f'{PATH_TO_DATA_FOLDER}wiki_tensor_dataset_{vocab_name}_en.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preprocess)\n",
    "from preprocess import TensoredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 595366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([13030,  8330,  3721,  8330,  3721,   132,  2496, 13031,  4719,  3982,\n",
       "         13031,  3178,   303,  5510, 13032,  8334,  2496, 13031,  4719,  1828,\n",
       "          2496,  1985, 13033, 10701, 13034,     7,  5299,  2338,  6948,     5,\n",
       "             9,     9,     8, 10510,   480, 13035, 13036, 11814, 13035, 13036,\n",
       "           965,   933,  2789,     5,   223,    10,   933, 13037,  6777,  1646,\n",
       "          3271, 13038,  2496, 13031,  4719,  1036, 13039,  1985,  2300,  1495,\n",
       "           601, 13040,  1495,     5,     9,   208,     6,     5,     9,     9,\n",
       "            11,   568,     5,     9,     9,   208, 13041,  1467,   403, 13042,\n",
       "          9309,  1065, 13043, 13044, 13043, 13044,  2300,  2189,  1880,  8330,\n",
       "          4719,   452,    10,     8,     8,     8, 13035, 13036,    21, 13045,\n",
       "          2300, 13045,  2641,  3721,  4340,  4251, 13043, 13044, 13046,  2496,\n",
       "         13031,  4719,  4340, 13045, 13047, 13048, 13049, 13050,  5496,  9571,\n",
       "           648,     5,     9,    10,     8,     5,    11, 13051,  6945,  9127,\n",
       "          2496,    53,  5458, 13051,  6945,  9127,  6945,  9127,  2025,   833,\n",
       "          6777,  3721, 13052,  3271, 13053, 13054,  3721, 11814, 13035, 13036,\n",
       "           689,  1261,    10,     8,     5,   253, 13055,  2496, 13031,  4719,\n",
       "         12197, 13052,  3271, 13053,  3721,  5251,   952,  3078,   167,  6674,\n",
       "            10,  8340,   439,  1366, 13056,   952,  1199,    10,     8,     5,\n",
       "             6,  3541,  1816,   794,    10,     8,     8,   611,   532,    10,\n",
       "             8,    10,     8,  1108, 13057,  2471,  5251,   952,   439,  8340,\n",
       "          1140,  2496, 13031,  4719,   132, 10436,  3078,  3271, 13053,   132,\n",
       "            58, 13058,   414, 13031,   132,   277,  2496,  2076, 13031,  1869,\n",
       "           525,    24, 13031,  4719,   526,  8330,  8340,  2496,    81,   397,\n",
       "           398, 13059,  3435,  2869,  2496, 13031,  8330,  8340,  2055,     5,\n",
       "             9,     9,     8,  8330,  8340,  2496, 13035, 13036]),\n",
       " tensor([248.]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vocab size:\", len(word_to_index))\n",
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Next step after loading tensor dataset -- create dataloader\n",
    "# wiki_loaders = {}\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "#     wiki_loaders[split] = DataLoader(\n",
    "#         wiki_dataset, \n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=True, \n",
    "#         collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
