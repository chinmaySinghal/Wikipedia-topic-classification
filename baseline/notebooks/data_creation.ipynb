{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from baseline.MY_PATHS import *\n",
    "import baseline.data_creation.wiki_parser as wiki_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesse raw json and save to pkl\n",
    "- For each language, we read the raw dumped data from `json_path`, process it using `wiki_parser`, save as df to `df_path` pkl file.\n",
    "- Works around 1-2 hours per language, so better to run in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load right paths to json, where to save df\n",
    "\n",
    "language_to_info = {\n",
    "    \"english\": {},\n",
    "    \"russian\": {},\n",
    "    \"hindi\"  : {},\n",
    "}\n",
    "\n",
    "for lang in language_to_info.keys():\n",
    "    # read from \"json_path\"\n",
    "    language_to_info[lang][\"json_path\"] = PATH_TO_DATA_FOLDER + f\"{lang[:2]}_20191201_text_and_topics_common.json\"\n",
    "    # save processed df to \"df_path\"\n",
    "    language_to_info[lang][\"df_path\"] = PATH_TO_DATA_FOLDER + f\"{lang[:2]}_df_full.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function for parsing and saving parsed df to pkl file.\n",
    "\n",
    "def process_lang(lang):\n",
    "    print(lang)\n",
    "    parser = wiki_parser.Parser(lang)\n",
    "    wiki_df = parser.get_wiki_tokenized_dataset(\n",
    "        language_to_info[lang][\"json_path\"],\n",
    "        extract_title=True, extract_tokens=True, extract_categories=True,\n",
    "        extract_section=True, extract_outlinks=True, \n",
    "#         debug=True,\n",
    "    )\n",
    "    wiki_df.to_pickle(language_to_info[lang][\"df_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# # Run in parallel\n",
    "\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# Parallel(n_jobs=3, verbose=1)(\n",
    "#     delayed(process_lang)(lang) for  lang in language_to_info.keys()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wiki_df = pd.read_pickle(PATH_TO_DATA_FOLDER + \"en_df_full.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>title</th>\n",
       "      <th>tokens</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>sections_tokens</th>\n",
       "      <th>raw_outlinks</th>\n",
       "      <th>outlinks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q6199</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>[anarchism, anti, authoritarianism, anti, auth...</td>\n",
       "      <td>[History and Society.Politics and government, ...</td>\n",
       "      <td>[etymology, terminology, definition, history, ...</td>\n",
       "      <td>[[[Anti-authoritarianism|anti-authoritarian]],...</td>\n",
       "      <td>[Anti-authoritarianism, Political philosophy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q38404</td>\n",
       "      <td>Autism</td>\n",
       "      <td>[autism, developmental, disorder, characterize...</td>\n",
       "      <td>[STEM.STEM*, STEM.Biology, STEM.Medicine &amp; Hea...</td>\n",
       "      <td>[characteristics, social, development, communi...</td>\n",
       "      <td>[[[Psychiatry]], [[Interpersonal relationship|...</td>\n",
       "      <td>[Psychiatry, Interpersonal relationship, commu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q101038</td>\n",
       "      <td>Albedo</td>\n",
       "      <td>[sunlight, relative, various, surface, conditi...</td>\n",
       "      <td>[STEM.STEM*, History and Society.Society, STEM...</td>\n",
       "      <td>[terrestrial, albedo, white, sky, black, sky, ...</td>\n",
       "      <td>[[[File:Albedo-e hg.svg|thumb|upright=1.3|The ...</td>\n",
       "      <td>[File:Albedo-e hg.svg, diffuse reflection, sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q173</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>[alabama, state, state, southern, united, stat...</td>\n",
       "      <td>[Geography.Regions.Americas.North America, Geo...</td>\n",
       "      <td>[etymology, history, pre, european, settlement...</td>\n",
       "      <td>[[[Coat of arms of Alabama|Coat of arms]], [[N...</td>\n",
       "      <td>[Coat of arms of Alabama, Northern flicker, Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q41746</td>\n",
       "      <td>Achilles</td>\n",
       "      <td>[dating, three, zero, zero, bc, achilles, troj...</td>\n",
       "      <td>[History and Society.History, Geography.Region...</td>\n",
       "      <td>[etymology, birth, early, years, names, hidden...</td>\n",
       "      <td>[[[File:Achilles fighting against Memnon Leide...</td>\n",
       "      <td>[File:Achilles fighting against Memnon Leiden ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       QID      title                                             tokens  \\\n",
       "0    Q6199  Anarchism  [anarchism, anti, authoritarianism, anti, auth...   \n",
       "1   Q38404     Autism  [autism, developmental, disorder, characterize...   \n",
       "2  Q101038     Albedo  [sunlight, relative, various, surface, conditi...   \n",
       "3     Q173    Alabama  [alabama, state, state, southern, united, stat...   \n",
       "4   Q41746   Achilles  [dating, three, zero, zero, bc, achilles, troj...   \n",
       "\n",
       "                                mid_level_categories  \\\n",
       "0  [History and Society.Politics and government, ...   \n",
       "1  [STEM.STEM*, STEM.Biology, STEM.Medicine & Hea...   \n",
       "2  [STEM.STEM*, History and Society.Society, STEM...   \n",
       "3  [Geography.Regions.Americas.North America, Geo...   \n",
       "4  [History and Society.History, Geography.Region...   \n",
       "\n",
       "                                     sections_tokens  \\\n",
       "0  [etymology, terminology, definition, history, ...   \n",
       "1  [characteristics, social, development, communi...   \n",
       "2  [terrestrial, albedo, white, sky, black, sky, ...   \n",
       "3  [etymology, history, pre, european, settlement...   \n",
       "4  [etymology, birth, early, years, names, hidden...   \n",
       "\n",
       "                                        raw_outlinks  \\\n",
       "0  [[[Anti-authoritarianism|anti-authoritarian]],...   \n",
       "1  [[[Psychiatry]], [[Interpersonal relationship|...   \n",
       "2  [[[File:Albedo-e hg.svg|thumb|upright=1.3|The ...   \n",
       "3  [[[Coat of arms of Alabama|Coat of arms]], [[N...   \n",
       "4  [[[File:Achilles fighting against Memnon Leide...   \n",
       "\n",
       "                                            outlinks  \n",
       "0  [Anti-authoritarianism, Political philosophy, ...  \n",
       "1  [Psychiatry, Interpersonal relationship, commu...  \n",
       "2  [File:Albedo-e hg.svg, diffuse reflection, sun...  \n",
       "3  [Coat of arms of Alabama, Northern flicker, Di...  \n",
       "4  [File:Achilles fighting against Memnon Leiden ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline.utils import get_classes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = get_classes_list(PATH_TO_DATA_FOLDER + \"classes.txt\")\n",
    "len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligned preprocess of dfs for en, ru, hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from baseline.data_creation.preprocess import (create_lookups_for_vocab, create_vocab_from_tokens,\n",
    "                          remove_non_common_articles_and_sort_by_QID,\n",
    "                          remove_rows_with_empty_column)\n",
    "\n",
    "def get_dict_of_split_sizes_and_QIDs(QIDs, LANGUAGES_LIST, train_size=0.8, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Assumes there are: \n",
    "    splits = [\"full\", \"monolingual_train\", \"multilingual_train\", \"val\", \"test\"].\n",
    "    \n",
    "    \"\"\"\n",
    "    splits = [\"full\", \"monolingual_train\", \"multilingual_train\", \"val\", \"test\"]\n",
    "    SPLIT_DICT = {split: {} for split in splits}\n",
    "    SPLIT_DICT[\"full\"][\"size\"]               = len(QIDs)\n",
    "    SPLIT_DICT[\"monolingual_train\"][\"size\"]  = int(train_size * SPLIT_DICT[\"full\"][\"size\"])\n",
    "    SPLIT_DICT[\"multilingual_train\"][\"size\"] = SPLIT_DICT[\"monolingual_train\"][\"size\"] // len(LANGUAGES_LIST)\n",
    "    SPLIT_DICT[\"val\"][\"size\"]                = int(val_size * SPLIT_DICT[\"full\"][\"size\"])\n",
    "    SPLIT_DICT[\"test\"][\"size\"] = (\n",
    "        SPLIT_DICT[\"full\"][\"size\"]\n",
    "        - SPLIT_DICT[\"monolingual_train\"][\"size\"]\n",
    "        - SPLIT_DICT[\"val\"][\"size\"]\n",
    "    )\n",
    "    print(*SPLIT_DICT.items(), sep=\"\\n\")\n",
    "    \n",
    "    SPLIT_DICT[\"monolingual_train\"][\"QIDs\"], val_and_test_QIDs = train_test_split(\n",
    "        QIDs, \n",
    "        train_size=SPLIT_DICT[\"monolingual_train\"][\"size\"], \n",
    "        random_state=SEED\n",
    "    )\n",
    "    SPLIT_DICT[\"multilingual_train\"][\"QIDs\"], _ = train_test_split(\n",
    "        SPLIT_DICT[\"monolingual_train\"][\"QIDs\"], \n",
    "        train_size=SPLIT_DICT[\"multilingual_train\"][\"size\"], \n",
    "        random_state=SEED\n",
    "    )\n",
    "    SPLIT_DICT[\"val\"][\"QIDs\"], SPLIT_DICT[\"test\"][\"QIDs\"] = train_test_split(\n",
    "        val_and_test_QIDs, \n",
    "        train_size=SPLIT_DICT[\"val\"][\"size\"], \n",
    "        random_state=SEED\n",
    "    )\n",
    "    return SPLIT_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n",
      "Percentage of articles with no mid_level_categories: 0.0 (0 articles)\n",
      "Percentage of articles with no tokens: 0.0 (0 articles)\n",
      "russian\n",
      "Percentage of articles with no mid_level_categories: 0.0 (0 articles)\n",
      "Percentage of articles with no tokens: 0.0 (0 articles)\n",
      "hindi\n",
      "Percentage of articles with no mid_level_categories: 0.0 (0 articles)\n",
      "Percentage of articles with no tokens: 0.011 (387 articles)\n",
      "Num of articles initially: \n",
      " dict_keys(['english', 'russian', 'hindi']) \n",
      " [33843, 33843, 33456]\n",
      "Num of articles after intersection: \n",
      " 33456\n",
      "('full', {'size': 33456})\n",
      "('monolingual_train', {'size': 26764})\n",
      "('multilingual_train', {'size': 8921})\n",
      "('val', {'size': 3345})\n",
      "('test', {'size': 3347})\n"
     ]
    }
   ],
   "source": [
    "# Load list of classes\n",
    "classes_list = classes\n",
    "\n",
    "SAVE = False\n",
    "LOAD = True\n",
    "\n",
    "SEED = 57\n",
    "\n",
    "LANGUAGES_LIST = [\"english\", \"russian\", \"hindi\"]\n",
    "LANGUAGES_DICT = defaultdict(dict)\n",
    "\n",
    "for language in LANGUAGES_LIST:\n",
    "    print(language)\n",
    "    # Get paths to files\n",
    "    LANGUAGES_DICT[language][\"FILE_NAMES_DICT\"] = get_paths(language)\n",
    "\n",
    "    # Load wiki_df\n",
    "    wiki_df = pkl.load(open(LANGUAGES_DICT[language][\"FILE_NAMES_DICT\"][\"wiki_df\"], \"rb\"))\n",
    "    LANGUAGES_DICT[language][\"wiki_df\"] = wiki_df\n",
    "\n",
    "    # Remove rows with empty labels/tokens\n",
    "    remove_rows_with_empty_column(LANGUAGES_DICT[language][\"wiki_df\"], column=\"mid_level_categories\")\n",
    "    remove_rows_with_empty_column(LANGUAGES_DICT[language][\"wiki_df\"], column=\"tokens\")\n",
    "\n",
    "# This step should be done BEFORE the splits\n",
    "remove_non_common_articles_and_sort_by_QID(LANGUAGES_DICT)\n",
    "\n",
    "for cur_dict in LANGUAGES_DICT.values():\n",
    "    # Binarize labels\n",
    "    mlb = MultiLabelBinarizer(classes_list)\n",
    "    cur_dict[\"wiki_df\"][\"labels\"] =\\\n",
    "        list(mlb.fit_transform(cur_dict[\"wiki_df\"].mid_level_categories))\n",
    "    assert (mlb.classes_ == classes_list).all()\n",
    "    \n",
    "    # Create and save OR load vocabulary\n",
    "    if SAVE:\n",
    "        vocab = create_vocab_from_tokens(cur_dict[\"wiki_df\"][\"tokens\"])\n",
    "        torch.save(vocab, cur_dict[\"FILE_NAMES_DICT\"][\"vocab\"])\n",
    "        print(\"Saved: \", cur_dict[\"FILE_NAMES_DICT\"][\"vocab\"])\n",
    "    if LOAD:\n",
    "        vocab = torch.load(cur_dict[\"FILE_NAMES_DICT\"][\"vocab\"])\n",
    "\n",
    "    index_to_word, word_to_index = create_lookups_for_vocab(vocab)\n",
    "    cur_dict[\"index_to_word\"], cur_dict[\"word_to_index\"] = index_to_word, word_to_index\n",
    "\n",
    "# train/val/test sizes and QIDs\n",
    "splits = [\"monolingual_train\", \"multilingual_train\", \"val\", \"test\"]\n",
    "QIDs = LANGUAGES_DICT[\"english\"][\"wiki_df\"].QID\n",
    "SPLIT_DICT = get_dict_of_split_sizes_and_QIDs(QIDs, LANGUAGES_LIST)\n",
    "\n",
    "# Create and save OR load splitted dfs\n",
    "for cur_dict in LANGUAGES_DICT.values():\n",
    "    dict_of_dfs = defaultdict()\n",
    "    if SAVE:\n",
    "        for split in [\"monolingual_train\", \"multilingual_train\", \"val\", \"test\"]:\n",
    "            dict_of_dfs[split] = cur_dict[\"wiki_df\"][cur_dict[\"wiki_df\"].QID.isin(SPLIT_DICT[split][\"QIDs\"])]\n",
    "            # save\n",
    "            torch.save(dict_of_dfs[split], cur_dict[\"FILE_NAMES_DICT\"][split])\n",
    "            print(\"Saved:, \", cur_dict[\"FILE_NAMES_DICT\"][split])\n",
    "    if LOAD:\n",
    "        for split in [\"monolingual_train\", \"multilingual_train\", \"val\", \"test\"]:\n",
    "            dict_of_dfs[split] = torch.load(cur_dict[\"FILE_NAMES_DICT\"][split])\n",
    "\n",
    "\n",
    "    cur_dict[\"dict_of_dfs\"] = dict_of_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['english', 'russian', 'hindi']\r\n",
      "english\r\n"
     ]
    }
   ],
   "source": [
    "! python3 ../data_creation/run_aligned_en_ru_hi_df_preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes.txt\r\n",
      "en_20191201_text_and_topics_common.json\r\n",
      "en_20191201_text_and_topics_common.json.bz2\r\n",
      "en_df_full.pkl\r\n",
      "en_df_wiki_monolingual_train.pt\r\n",
      "en_df_wiki_multilingual_train.pt\r\n",
      "en_df_wiki_test.pt\r\n",
      "en_df_wiki_valid.pt\r\n",
      "en_vocab_all.pt\r\n",
      "hi_20191201_text_and_topics_common.json\r\n",
      "hi_20191201_text_and_topics_common.json.bz2\r\n",
      "hi_df_full.pkl\r\n",
      "hi_df_wiki_monolingual_train.pt\r\n",
      "hi_df_wiki_multilingual_train.pt\r\n",
      "hi_df_wiki_test.pt\r\n",
      "hi_df_wiki_valid.pt\r\n",
      "hi_vocab_all.pt\r\n",
      "ru_20191201_text_and_topics_common.json\r\n",
      "ru_20191201_text_and_topics_common.json.bz2\r\n",
      "ru_df_full.pkl\r\n",
      "ru_df_wiki_monolingual_train.pt\r\n",
      "ru_df_wiki_multilingual_train.pt\r\n",
      "ru_df_wiki_test.pt\r\n",
      "ru_df_wiki_valid.pt\r\n",
      "ru_vocab_all.pt\r\n",
      "vocab_all_en.pt\r\n",
      "vocab_all_hi.pt\r\n",
      "vocab_all_ru.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls $PATH_TO_DATA_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "1. Get list of labels, save it.  \n",
    "2. Create multilingual train, val, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
