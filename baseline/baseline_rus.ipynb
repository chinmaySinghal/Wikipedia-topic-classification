{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import spacy\n",
    "import string\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import mwparserfromhell\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from spacy.lang.ru import Russian\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/\"\n",
    "PATH_TO_EMBEDDINGS_FOLDER = \"/scratch/mz2476/wiki/embeddings/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data, preprocess, save to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load Russian tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = Russian()\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# set file names for dumping pickle files and input data source\n",
    "FILE_NAME = PATH_TO_DATA_FOLDER + \"wikitext_ru_sample.json\"\n",
    "\n",
    "PRETRAINED_EMBEDDINGS = PATH_TO_EMBEDDINGS_FOLDER + 'wiki.ru.align.vec'\n",
    "\n",
    "TEXT_OUTPUT_FILE = PATH_TO_DATA_FOLDER + 'wikitext_ru_tokenized.p'\n",
    "SECTION_OUTPUT_FILE = PATH_TO_DATA_FOLDER + 'wikisection_ru_tokenized.p'\n",
    "\n",
    "# MAX_ARTICLE_LENGTH = 500\n",
    "\n",
    "# downloading and setting stop word list from NLTK\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rus_data_processing' from '/home/mz2476/topic-modeling/topic-modeling/baseline/rus_data_processing.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rus_data_processing\n",
    "from importlib import reload\n",
    "reload(rus_data_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #save the Wiki SECTION dataset in pickle file for subsequent use\n",
    "# wiki_df = rus_data_processing.get_wiki_tokenized_dataset(FILE_NAME, True)\n",
    "# pkl.dump(wiki_df, open(SECTION_OUTPUT_FILE, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #save the Wiki TEXT dataset in pickle file for subsequent use\n",
    "# # Download the pickle files from Google Drive (else creating new will take ~4 hours)\n",
    "# # https://drive.google.com/open?id=1DlNxxNh6WA5ds7px844LnBbhEzV0Ydyo\n",
    "# wiki_df = rus_data_processing.get_wiki_tokenized_dataset(FILE_NAME)\n",
    "# pkl.dump(wiki_df, open(TEXT_OUTPUT_FILE, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe from pickle file - Wiki Text\n",
    "wiki_df =  pkl.load(open(TEXT_OUTPUT_FILE, \"rb\"))\n",
    "\n",
    "# load the dataframe from pickle file - Wiki Sections\n",
    "#wiki_df =  pkl.load(open(SECTION_OUTPUT_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_missing_category(categories_list, category_name='Culture.Architecture'):\n",
    "#     return [categ for categ in categories_list if categ != category_name]\n",
    "\n",
    "def rename_missing_category(categories_list, missing_category_name='Culture.Architecture', rename_to='Culture.Plastic arts'):\n",
    "    renamed_categories_list = []\n",
    "    for categ in categories_list:\n",
    "        if categ == missing_category_name:\n",
    "            categ = rename_to\n",
    "        renamed_categories_list.append(categ)\n",
    "    return renamed_categories_list\n",
    "\n",
    "wiki_df.mid_level_categories = wiki_df.mid_level_categories.apply(rename_missing_categoryame_missing_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "classes = torch.load(PATH_TO_DATA_FOLDER + \"classes_list.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Culture.Arts', 'Culture.Broadcasting',\n",
       "       'Culture.Crafts and hobbies', 'Culture.Entertainment',\n",
       "       'Culture.Food and drink', 'Culture.Games and toys',\n",
       "       'Culture.Internet culture', 'Culture.Language and literature',\n",
       "       'Culture.Media', 'Culture.Music', 'Culture.Performing arts',\n",
       "       'Culture.Philosophy and religion', 'Culture.Plastic arts',\n",
       "       'Culture.Sports', 'Culture.Visual arts', 'Geography.Africa',\n",
       "       'Geography.Americas', 'Geography.Antarctica', 'Geography.Asia',\n",
       "       'Geography.Bodies of water', 'Geography.Europe',\n",
       "       'Geography.Landforms', 'Geography.Maps', 'Geography.Oceania',\n",
       "       'Geography.Parks', 'History_And_Society.Business and economics',\n",
       "       'History_And_Society.Education',\n",
       "       'History_And_Society.History and society',\n",
       "       'History_And_Society.Military and warfare',\n",
       "       'History_And_Society.Politics and government',\n",
       "       'History_And_Society.Transportation', 'STEM.Biology',\n",
       "       'STEM.Chemistry', 'STEM.Engineering', 'STEM.Geosciences',\n",
       "       'STEM.Information science', 'STEM.Mathematics', 'STEM.Medicine',\n",
       "       'STEM.Meteorology', 'STEM.Physics', 'STEM.Science', 'STEM.Space',\n",
       "       'STEM.Technology', 'STEM.Time'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes 'Culture.Plastic arts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q515</td>\n",
       "      <td>[]</td>\n",
       "      <td>[файл, два, ноль, один, один, токио, япония, ф...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q415638</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[нитри, тита, на, бинарные, соединения, бинарн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q333936</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[файл, один, нитрилы, нитри, лы, органические,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q7016</td>\n",
       "      <td>[STEM.Time, History_And_Society.History and so...</td>\n",
       "      <td>[файл, три, века, один, семь, век, сытин, один...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q2622089</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[файл, памятник, на, территории, словацкого, т...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                               mid_level_categories  \\\n",
       "0      Q515                                                 []   \n",
       "1   Q415638                                   [STEM.Chemistry]   \n",
       "2   Q333936                                   [STEM.Chemistry]   \n",
       "3     Q7016  [STEM.Time, History_And_Society.History and so...   \n",
       "4  Q2622089                                   [STEM.Chemistry]   \n",
       "\n",
       "                                              tokens  \n",
       "0  [файл, два, ноль, один, один, токио, япония, ф...  \n",
       "1  [нитри, тита, на, бинарные, соединения, бинарн...  \n",
       "2  [файл, один, нитрилы, нитри, лы, органические,...  \n",
       "3  [файл, три, века, один, семь, век, сытин, один...  \n",
       "4  [файл, памятник, на, территории, словацкого, т...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14787 articles in total.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{wiki_df.shape[0]} articles in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df['tokens'] = wiki_df['tokens'].apply(rus_data_processing.remove_short_words)\n",
    "wiki_df['tokens'] = wiki_df['tokens'].apply(rus_data_processing.remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x2b022b6c3450>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = wiki_df['tokens'].apply(lambda x: len(x))\n",
    "count_df = b.value_counts().sort_index().rename_axis('count').reset_index(name='frequency')\n",
    "count_df.hist(column='count',bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique words in pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888423it [02:29, 12605.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained fastText embeddings\n",
    "# def load_vectors(fname):\n",
    "#     fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, fin.readline().split())\n",
    "#     data = {}\n",
    "#     for line in fin:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         data[tokens[0]] = np.array(list(map(float, tokens[1:]))).astype('float32')\n",
    "#     return data\n",
    "import utils\n",
    "\n",
    "pretrained_embs = utils.load_vectors(PRETRAINED_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of unique Wiki tokens in fastText : 85.17%\n",
      "unique tokens not in fastText : 56317\n",
      "unique tokens in fastText : 323331\n"
     ]
    }
   ],
   "source": [
    "# get all unique tokens in the training dataset\n",
    "all_train_tokens = []\n",
    "\n",
    "for tokens in wiki_df['tokens']:\n",
    "    for token in tokens:\n",
    "        all_train_tokens.append(token)\n",
    "        \n",
    "all_train_tokens = list(set(all_train_tokens))\n",
    "\n",
    "# Analyse the wiki tokens and fastText embeddings\n",
    "train_token_count = len(all_train_tokens)\n",
    "token_in_fasttext = []\n",
    "token_not_in_fasttext = []\n",
    "train_token_in_fasttxt = 0\n",
    "\n",
    "for token in all_train_tokens:\n",
    "    if token in pretrained_embs.keys():\n",
    "        token_in_fasttext.append(token)\n",
    "        train_token_in_fasttxt = train_token_in_fasttxt + 1\n",
    "    else:\n",
    "        token_not_in_fasttext.append(token)\n",
    "        \n",
    "print(\"% of unique Wiki tokens in fastText : {:4.2f}%\".format(train_token_in_fasttxt/train_token_count*100))\n",
    "print('unique tokens not in fastText :', len(token_not_in_fasttext))\n",
    "print('unique tokens in fastText :', len(token_in_fasttext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens not in the fastText\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['клисурица',\n",
       " 'фромете',\n",
       " 'румън',\n",
       " 'либфраумильх',\n",
       " 'павкара',\n",
       " 'еротикон',\n",
       " 'рнов',\n",
       " 'посткритическую',\n",
       " 'бейларбекства',\n",
       " 'злодеечку']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample tokens not in the fastText\")\n",
    "token_not_in_fasttext[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import preprocess\n",
    "import importlib\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from preprocess import remove_stop_words, train_validate_test_split\n",
    "from preprocess import tokenize_dataset, TensoredDataset, pad_collate_fn\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14428, 3)\n",
      "(14427, 3)\n"
     ]
    }
   ],
   "source": [
    "#Removing stop words\n",
    "wiki_df['tokens'] = wiki_df[\"tokens\"].apply(partial(remove_stop_words, language=\"russian\"))\n",
    "wiki_df.head()\n",
    "\n",
    "#Removing rows with missing labels\n",
    "mask = wiki_df.mid_level_categories.apply(lambda x: len(x) > 0)\n",
    "wiki_df = wiki_df[mask]\n",
    "wiki_df = wiki_df.reset_index(drop=True)\n",
    "print(wiki_df.shape)\n",
    "\n",
    "#Removing rows with no tokens\n",
    "mask = wiki_df.tokens.apply(lambda x: len(x) > 0)\n",
    "wiki_df = wiki_df[mask]\n",
    "wiki_df = wiki_df.reset_index(drop=True)\n",
    "print(wiki_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q415638</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[нитри, тита, бинарные, соединения, бинарное, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q333936</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[файл, нитрилы, нитри, лы, органические, соеди...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q7016</td>\n",
       "      <td>[STEM.Time, History_And_Society.History and so...</td>\n",
       "      <td>[файл, века, семь, век, сытин, девять, век, за...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q2622089</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[файл, памятник, территории, словацкого, техно...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q43440</td>\n",
       "      <td>[Culture.People, Culture.Language and literatu...</td>\n",
       "      <td>[джозуэ, карду, ччи, семь, июля, восемь, пять,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                               mid_level_categories  \\\n",
       "0   Q415638                                   [STEM.Chemistry]   \n",
       "1   Q333936                                   [STEM.Chemistry]   \n",
       "2     Q7016  [STEM.Time, History_And_Society.History and so...   \n",
       "3  Q2622089                                   [STEM.Chemistry]   \n",
       "4    Q43440  [Culture.People, Culture.Language and literatu...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [нитри, тита, бинарные, соединения, бинарное, ...   \n",
       "1  [файл, нитрилы, нитри, лы, органические, соеди...   \n",
       "2  [файл, века, семь, век, сытин, девять, век, за...   \n",
       "3  [файл, памятник, территории, словацкого, техно...   \n",
       "4  [джозуэ, карду, ччи, семь, июля, восемь, пять,...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize the labels\n",
    "# labels list: mlb.classes_\n",
    "mlb = MultiLabelBinarizer()\n",
    "wiki_df[\"labels\"] = list(mlb.fit_transform(wiki_df.mid_level_categories))\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val/test split\n",
    "wiki_train, wiki_valid, wiki_test = train_validate_test_split(wiki_df, seed=1)\n",
    "\n",
    "wiki_train = wiki_train.reset_index(drop=True)\n",
    "wiki_valid = wiki_valid.reset_index(drop=True)\n",
    "wiki_test = wiki_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is: 332489\n"
     ]
    }
   ],
   "source": [
    "# Building vocabulary\n",
    "vocab = list(set([y for x in list(wiki_train['tokens']) for y in x]))\n",
    "\n",
    "print(\"Vocab size is: {}\".format(len(vocab)))\n",
    "\n",
    "word_to_index = {\"<pad>\":0, \"<unk>\":1}\n",
    "for word in vocab:\n",
    "    if word not in word_to_index:\n",
    "        word_to_index[word] = len(word_to_index)\n",
    "index_to_word = {v:k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277730"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index[\"мышь\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11541/11541 [00:01<00:00, 9404.09it/s]\n",
      "100%|██████████| 1442/1442 [00:00<00:00, 7020.94it/s]\n",
      "100%|██████████| 1444/1444 [00:00<00:00, 9538.32it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_tokenized_train = tokenize_dataset(wiki_train, word_to_index)\n",
    "wiki_tokenized_val = tokenize_dataset(wiki_valid, word_to_index)\n",
    "wiki_tokenized_test = tokenize_dataset(wiki_test, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tokenized_datasets = {}\n",
    "wiki_tokenized_datasets['X_train'] = wiki_tokenized_train\n",
    "wiki_tokenized_datasets['X_val'] = wiki_tokenized_val\n",
    "wiki_tokenized_datasets['X_test'] = wiki_tokenized_test\n",
    "\n",
    "wiki_tokenized_datasets['y_train'] = list(wiki_train.labels)\n",
    "wiki_tokenized_datasets['y_val'] = list(wiki_valid.labels)\n",
    "wiki_tokenized_datasets['y_test'] = list(wiki_test.labels)\n",
    "\n",
    "wiki_tensor_dataset = {}\n",
    "wiki_tensor_dataset['train'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_train'], wiki_tokenized_datasets['y_train']\n",
    ")\n",
    "wiki_tensor_dataset['val'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_val'], wiki_tokenized_datasets['y_val']\n",
    ")\n",
    "wiki_tensor_dataset['test'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_test'], wiki_tokenized_datasets['y_test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([158689, 279219, 316721, 156498,  44864, 123093,   2590, 108238, 158689,\n",
       "         230013,  13455, 119888,  60467,  34684, 141356,  27179,   2590, 310783,\n",
       "         223079,  98802, 303579, 248746,  49147,  32507,  27179,  63860,  54789,\n",
       "         120288, 158689, 122677, 158689,  59938,   2470, 223079, 271108, 331376,\n",
       "          66087, 297025, 223079, 158689,  25462, 158689, 299266, 164477, 237875,\n",
       "         223079, 299266, 164477, 237875, 316153, 108205,  88585,  25462, 223079,\n",
       "         158689, 277357, 277357, 158689, 234629, 192106, 235615, 154363, 101128,\n",
       "         235615, 299082, 314030, 158689, 186289, 145230, 158689, 281565, 123439,\n",
       "         323076, 107862, 294028, 191634, 197001]),\n",
       " tensor([77.]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load aligned Russian embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888423it [02:29, 12629.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2.5 million\n",
    "embeddings = utils.load_vectors(PATH_TO_FOLDER + \"wiki.ru.align.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 332489\n",
      "No. of words from vocab found in fastText: 287175\n"
     ]
    }
   ],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "vocab_size = len(index_to_word)\n",
    "embed_dim = len(embeddings[\"apple\"])\n",
    "weights_matrix = np.zeros((vocab_size,embed_dim))\n",
    "\n",
    "words_found = 0\n",
    "for i, word in enumerate(word_to_index):\n",
    "    if word in embeddings.keys():\n",
    "        weights_matrix[i] = embeddings[word]\n",
    "        words_found += 1\n",
    "    else:\n",
    "        weights_matrix[i] = np.zeros(embed_dim)\n",
    "weights_matrix = torch.FloatTensor(weights_matrix)\n",
    "\n",
    "print(\"Total words in vocab: {}\".format(len(vocab)))\n",
    "print(\"No. of words from vocab found in fastText: {}\".format(words_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model. Use pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = \"../../baseline_models_params/optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\"\n",
    "\n",
    "best_params = {\n",
    "    'optimizer': 'SWA',\n",
    "    'num_hidden': 2,\n",
    "    'dim_hidden': 150,\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from English\n",
    "# array(['Culture.Arts', 'Culture.Broadcasting',\n",
    "#        'Culture.Crafts and hobbies', 'Culture.Entertainment',\n",
    "#        'Culture.Food and drink', 'Culture.Games and toys',\n",
    "#        'Culture.Internet culture', 'Culture.Language and literature',\n",
    "#        'Culture.Media', 'Culture.Music', 'Culture.Performing arts',\n",
    "#        'Culture.Philosophy and religion', 'Culture.Plastic arts',\n",
    "#        'Culture.Sports', 'Culture.Visual arts', 'Geography.Africa',\n",
    "#        'Geography.Americas', 'Geography.Antarctica', 'Geography.Asia',\n",
    "#        'Geography.Bodies of water', 'Geography.Europe',\n",
    "#        'Geography.Landforms', 'Geography.Maps', 'Geography.Oceania',\n",
    "#        'Geography.Parks', 'History_And_Society.Business and economics',\n",
    "#        'History_And_Society.Education',\n",
    "#        'History_And_Society.History and society',\n",
    "#        'History_And_Society.Military and warfare',\n",
    "#        'History_And_Society.Politics and government',\n",
    "#        'History_And_Society.Transportation', 'STEM.Biology',\n",
    "#        'STEM.Chemistry', 'STEM.Engineering', 'STEM.Geosciences',\n",
    "#        'STEM.Information science', 'STEM.Mathematics', 'STEM.Medicine',\n",
    "#        'STEM.Meteorology', 'STEM.Physics', 'STEM.Science', 'STEM.Space',\n",
    "#        'STEM.Technology', 'STEM.Time'], dtype=object)\n",
    "\n",
    "# # from Russian\n",
    "# # one more category: 'Culture.Architecture'\n",
    "# array(['Culture.Architecture', 'Culture.Arts', 'Culture.Broadcasting',\n",
    "#        'Culture.Crafts and hobbies', 'Culture.Entertainment',\n",
    "#        'Culture.Food and drink', 'Culture.Games and toys',\n",
    "#        'Culture.Internet culture', 'Culture.Language and literature',\n",
    "#        'Culture.Media', 'Culture.Music', 'Culture.People',\n",
    "#        'Culture.Performing arts', 'Culture.Philosophy and religion',\n",
    "#        'Culture.Sports', 'Culture.Visual arts', 'Geography.Africa',\n",
    "#        'Geography.Americas', 'Geography.Antarctica', 'Geography.Asia',\n",
    "#        'Geography.Bodies of water', 'Geography.Europe',\n",
    "#        'Geography.Landforms', 'Geography.Maps', 'Geography.Oceania',\n",
    "#        'Geography.Parks', 'History_And_Society.Business and economics',\n",
    "#        'History_And_Society.Education',\n",
    "#        'History_And_Society.History and society',\n",
    "#        'History_And_Society.Military and warfare',\n",
    "#        'History_And_Society.Politics and government',\n",
    "#        'History_And_Society.Transportation', 'STEM.Biology',\n",
    "#        'STEM.Chemistry', 'STEM.Engineering', 'STEM.Geosciences',\n",
    "#        'STEM.Information science', 'STEM.Mathematics', 'STEM.Medicine',\n",
    "#        'STEM.Meteorology', 'STEM.Physics', 'STEM.Science', 'STEM.Space',\n",
    "#        'STEM.Technology', 'STEM.Time'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix,\n",
    "    \"num_layers\": best_params[\"num_hidden\"],\n",
    "    \"num_classes\": len(mlb.classes_),\n",
    "    \"mid_features\": best_params[\"dim_hidden\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "pretrained_state_dict = torch.load(PRETRAINED_MODEL)\n",
    "\n",
    "# take only pretrained params of layer_out\n",
    "pretrained_params = ['layer_out.0.weight', 'layer_out.0.bias', 'layer_out.2.weight', 'layer_out.2.bias']\n",
    "for param in pretrained_params:\n",
    "    model.state_dict()[param] = pretrained_state_dict[param]\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.0218, Recall macro: 0.4559, F1 macro: 0.0374 \n",
      "Precision micro: 0.0474, Recall micro: 0.4898, F1 micro: 0.0864 \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune on Russian articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, \n",
    "                num_epochs=10, device=device, model_name=\"model\", save_model=False):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch, \"epoch\")\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 100 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "                ))\n",
    "                print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "                ))\n",
    "\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "                    if save_model:\n",
    "                        optimizer.swap_swa_sgd()\n",
    "                        torch.save(model.state_dict(), f\"../../baseline_models_params/{model_name}.pth\")\n",
    "                        print('Model Saved')\n",
    "                        print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 150, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "0 epoch\n",
      "Epoch: [1/10], Step: [101/361], Train_loss: 0.1645046380907297\n",
      "Precision macro: 0.03537699407993464, Recall macro: 0.02155597959754273, F1 macro: 0.026785399083836394 \n",
      "Precision micro: 0.7433035714285714, Recall micro: 0.11871657754010695, F1 micro: 0.2047340916077467 \n",
      "Epoch: [1/10], Step: [201/361], Train_loss: 0.13725534178316592\n",
      "Precision macro: 0.07273745667869681, Recall macro: 0.04890773218096716, F1 macro: 0.05589089305575609 \n",
      "Precision micro: 0.8069651741293532, Recall micro: 0.289126559714795, F1 micro: 0.42572178477690287 \n",
      "Epoch: [1/10], Step: [301/361], Train_loss: 0.12381432245175043\n",
      "Precision macro: 0.17689836412870966, Recall macro: 0.07799647054053015, F1 macro: 0.09206487379316092 \n",
      "Precision micro: 0.7808683853459973, Recall micro: 0.4103386809269162, F1 micro: 0.5379761626548258 \n",
      "1 epoch\n",
      "Epoch: [2/10], Step: [101/361], Train_loss: 0.07884904693812132\n",
      "Precision macro: 0.3190387341957216, Recall macro: 0.15673069045162838, F1 macro: 0.1845945975504999 \n",
      "Precision micro: 0.8210463733650416, Recall micro: 0.4923351158645276, F1 micro: 0.6155560508134611 \n",
      "Epoch: [2/10], Step: [201/361], Train_loss: 0.07557351440191269\n",
      "Precision macro: 0.33288823139044815, Recall macro: 0.1906876978518973, F1 macro: 0.22117033696586363 \n",
      "Precision micro: 0.8045857217300677, Recall micro: 0.550445632798574, F1 micro: 0.6536833192209992 \n",
      "Epoch: [2/10], Step: [301/361], Train_loss: 0.07306771968801816\n",
      "Precision macro: 0.34734987495512465, Recall macro: 0.20719569574062796, F1 macro: 0.23744331706259977 \n",
      "Precision micro: 0.8024132730015083, Recall micro: 0.5689839572192513, F1 micro: 0.6658322903629537 \n",
      "2 epoch\n",
      "Epoch: [3/10], Step: [101/361], Train_loss: 0.0652179641276598\n",
      "Precision macro: 0.4108483014972876, Recall macro: 0.21491749992376977, F1 macro: 0.24974234482315605 \n",
      "Precision micro: 0.8326891220320265, Recall micro: 0.5376114081996435, F1 micro: 0.6533795493934141 \n",
      "Epoch: [3/10], Step: [201/361], Train_loss: 0.06387117190286518\n",
      "Precision macro: 0.4558400393396045, Recall macro: 0.23143740275596808, F1 macro: 0.26983324916905177 \n",
      "Precision micro: 0.8317610062893082, Recall micro: 0.5657754010695187, F1 micro: 0.6734563971992362 \n",
      "Epoch: [3/10], Step: [301/361], Train_loss: 0.06334499388933182\n",
      "Precision macro: 0.3922854041454429, Recall macro: 0.24183637784574438, F1 macro: 0.27621795435125623 \n",
      "Precision micro: 0.809407153356198, Recall micro: 0.5889483065953655, F1 micro: 0.6817994222038795 \n",
      "3 epoch\n",
      "Epoch: [4/10], Step: [101/361], Train_loss: 0.058185292892158035\n",
      "Precision macro: 0.4172443437322866, Recall macro: 0.25345744305656376, F1 macro: 0.29028295909608887 \n",
      "Precision micro: 0.8051401869158878, Recall micro: 0.6142602495543672, F1 micro: 0.6968655207280081 \n",
      "Epoch: [4/10], Step: [201/361], Train_loss: 0.05793677937239408\n",
      "Precision macro: 0.46740687702450145, Recall macro: 0.28601815939577624, F1 macro: 0.3275386134288007 \n",
      "Precision micro: 0.7923250564334086, Recall micro: 0.6256684491978609, F1 micro: 0.6992031872509961 \n",
      "Epoch: [4/10], Step: [301/361], Train_loss: 0.057941862692435585\n",
      "Precision macro: 0.4583167654931602, Recall macro: 0.2747699597483463, F1 macro: 0.3169713119882968 \n",
      "Precision micro: 0.8253731343283582, Recall micro: 0.5914438502673797, F1 micro: 0.6890965732087228 \n",
      "4 epoch\n",
      "Epoch: [5/10], Step: [101/361], Train_loss: 0.05683896258473396\n",
      "Precision macro: 0.49681986041260673, Recall macro: 0.3024744258811504, F1 macro: 0.3578765668036303 \n",
      "Precision micro: 0.826251180358829, Recall micro: 0.6238859180035651, F1 micro: 0.7109486085720089 \n",
      "Epoch: [5/10], Step: [201/361], Train_loss: 0.05611103756353259\n",
      "Precision macro: 0.5021663437136706, Recall macro: 0.3013216174489824, F1 macro: 0.354365983360819 \n",
      "Precision micro: 0.8248803827751197, Recall micro: 0.6146167557932264, F1 micro: 0.7043922369765068 \n",
      "Epoch: [5/10], Step: [301/361], Train_loss: 0.055569807390371956\n",
      "Precision macro: 0.4907005179033605, Recall macro: 0.3058166957385204, F1 macro: 0.3594305772331923 \n",
      "Precision micro: 0.8317399617590823, Recall micro: 0.6203208556149733, F1 micro: 0.710639166836839 \n",
      "5 epoch\n",
      "Epoch: [6/10], Step: [101/361], Train_loss: 0.05393588624894619\n",
      "Precision macro: 0.492434909040543, Recall macro: 0.3165994450041214, F1 macro: 0.37078210648060445 \n",
      "Precision micro: 0.8375363724539282, Recall micro: 0.615686274509804, F1 micro: 0.7096774193548386 \n",
      "Epoch: [6/10], Step: [201/361], Train_loss: 0.0528690838534385\n",
      "Precision macro: 0.5184697262030727, Recall macro: 0.3276489256148233, F1 macro: 0.382882375762871 \n",
      "Precision micro: 0.7977430555555556, Recall micro: 0.6552584670231729, F1 micro: 0.719514582110002 \n",
      "Epoch: [6/10], Step: [301/361], Train_loss: 0.05313393696521719\n",
      "Precision macro: 0.5552363817469634, Recall macro: 0.3420518676840303, F1 macro: 0.39870498805614535 \n",
      "Precision micro: 0.8078118064802485, Recall micro: 0.6488413547237076, F1 micro: 0.7196520363780149 \n",
      "6 epoch\n",
      "Epoch: [7/10], Step: [101/361], Train_loss: 0.05148959558457136\n",
      "Precision macro: 0.5143272563858371, Recall macro: 0.34194806138373024, F1 macro: 0.39283172305296327 \n",
      "Precision micro: 0.8128735632183908, Recall micro: 0.6303030303030303, F1 micro: 0.7100401606425701 \n",
      "Epoch: [7/10], Step: [201/361], Train_loss: 0.05120967837050557\n",
      "Precision macro: 0.5596827364332682, Recall macro: 0.3302806370411052, F1 macro: 0.38758670896290787 \n",
      "Precision micro: 0.8174640037157455, Recall micro: 0.6274509803921569, F1 micro: 0.7099636950383219 \n",
      "Epoch: [7/10], Step: [301/361], Train_loss: 0.05135120051602522\n",
      "Precision macro: 0.5385290606569977, Recall macro: 0.3353194803105421, F1 macro: 0.39542460180181477 \n",
      "Precision micro: 0.8223234624145785, Recall micro: 0.64349376114082, F1 micro: 0.722 \n",
      "7 epoch\n",
      "Epoch: [8/10], Step: [101/361], Train_loss: 0.0507252998277545\n",
      "Precision macro: 0.5408210609362342, Recall macro: 0.36441834156975333, F1 macro: 0.41982847904698795 \n",
      "Precision micro: 0.8324942791762013, Recall micro: 0.6484848484848484, F1 micro: 0.7290581162324649 \n",
      "Epoch: [8/10], Step: [201/361], Train_loss: 0.05006409741006792\n",
      "Precision macro: 0.5566843987012529, Recall macro: 0.3836592598091203, F1 macro: 0.43222658200875247 \n",
      "Precision micro: 0.8043844856661045, Recall micro: 0.6802139037433155, F1 micro: 0.7371064322966969 \n",
      "Epoch: [8/10], Step: [301/361], Train_loss: 0.049871395515898864\n",
      "Precision macro: 0.5840532319754184, Recall macro: 0.34857266423417993, F1 macro: 0.41423421876944067 \n",
      "Precision micro: 0.8347107438016529, Recall micro: 0.6481283422459893, F1 micro: 0.7296809151113787 \n",
      "8 epoch\n",
      "Epoch: [9/10], Step: [101/361], Train_loss: 0.04794450344517827\n",
      "Precision macro: 0.5828915779963675, Recall macro: 0.38389714748974774, F1 macro: 0.4405557857803004 \n",
      "Precision micro: 0.8140043763676149, Recall micro: 0.6631016042780749, F1 micro: 0.7308447937131631 \n",
      "Epoch: [9/10], Step: [201/361], Train_loss: 0.048294080104678866\n",
      "Precision macro: 0.5432858637615658, Recall macro: 0.38685607536149375, F1 macro: 0.4329009336946003 \n",
      "Precision micro: 0.808936170212766, Recall micro: 0.6777183600713013, F1 micro: 0.7375363724539282 \n",
      "Epoch: [9/10], Step: [301/361], Train_loss: 0.048515940482417745\n",
      "Precision macro: 0.6087833238734297, Recall macro: 0.36764768112010826, F1 macro: 0.4362024368533619 \n",
      "Precision micro: 0.8238202247191011, Recall micro: 0.653475935828877, F1 micro: 0.7288270377733599 \n",
      "9 epoch\n",
      "Epoch: [10/10], Step: [101/361], Train_loss: 0.048151666652411225\n",
      "Precision macro: 0.5895282078470383, Recall macro: 0.3489817261242663, F1 macro: 0.4184417999092227 \n",
      "Precision micro: 0.8591549295774648, Recall micro: 0.6089126559714795, F1 micro: 0.7127060296265386 \n",
      "Epoch: [10/10], Step: [201/361], Train_loss: 0.04803409159183502\n",
      "Precision macro: 0.5859276701669601, Recall macro: 0.39132223050382675, F1 macro: 0.44225628083928786 \n",
      "Precision micro: 0.8083403538331929, Recall micro: 0.6841354723707664, F1 micro: 0.741069704576173 \n",
      "Epoch: [10/10], Step: [301/361], Train_loss: 0.04805733332410455\n",
      "Precision macro: 0.6203694461101005, Recall macro: 0.4090875517528344, F1 macro: 0.46356145893305895 \n",
      "Precision micro: 0.8158567774936062, Recall micro: 0.6823529411764706, F1 micro: 0.7431566686080373 \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "    \n",
    "result = {\n",
    "    \"optimizer\": best_params[\"optimizer\"], \n",
    "    \"num_hidden\": best_params[\"num_hidden\"],\n",
    "    \"dim_hidden\": best_params[\"dim_hidden\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"learning_rate\": best_params[\"learning_rate\"],\n",
    "    \"num_epochs\": num_epochs\n",
    "}\n",
    "print(\"\\n\", result)\n",
    "\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "optimizer = SWA(base_opt) \n",
    "\n",
    "# train the model\n",
    "model_name = \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "#     print(model_name)\n",
    "metrics_dict = train_model(\n",
    "    wiki_loaders, model, criterion, optimizer, num_epochs=num_epochs, \n",
    "    model_name=model_name, save_model=False\n",
    ")\n",
    "result.update(metrics_dict)\n",
    "\n",
    "# results_df = results_df.append(result, ignore_index=True)\n",
    "#     results_df.to_csv(\"results/results_tuning_2_3_layers_maxlen_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11541"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_loaders[\"train\"].dataset.input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.6204, Recall macro: 0.4091, F1 macro: 0.4636 \n",
      "Precision micro: 0.8159, Recall micro: 0.6824, F1 micro: 0.7432 \n"
     ]
    }
   ],
   "source": [
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
