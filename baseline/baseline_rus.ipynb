{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import spacy\n",
    "import string\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import mwparserfromhell\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from spacy.lang.ru import Russian\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_FOLDER = \"/scratch/mz2476/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data, preprocess, save to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load Russian tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = Russian()\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# set file names for dumping pickle files and input data source\n",
    "FILE_NAME = PATH_TO_FOLDER + \"wikitext_ru_sample.json\"\n",
    "\n",
    "PRETRAINED_EMBEDDINGS = PATH_TO_FOLDER + 'wiki.ru.align.vec'\n",
    "\n",
    "TEXT_OUTPUT_FILE = '../../tokenized-data/wikitext_ru_tokenized.p'\n",
    "SECTION_OUTPUT_FILE = '../../tokenized-data/wikisection_ru_tokenized.p'\n",
    "\n",
    "MAX_ARTICLE_LENGTH = 500\n",
    "\n",
    "# downloading and setting stop word list from NLTK\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rus_data_processing' from '/home/mz2476/topic-modeling/topic-modeling/baseline/rus_data_processing.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rus_data_processing\n",
    "from importlib import reload\n",
    "reload(rus_data_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #save the Wiki SECTION dataset in pickle file for subsequent use\n",
    "# wiki_df = rus_data_processing.get_wiki_tokenized_dataset(FILE_NAME, True)\n",
    "# pkl.dump(wiki_df, open(SECTION_OUTPUT_FILE, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #save the Wiki TEXT dataset in pickle file for subsequent use\n",
    "# # Download the pickle files from Google Drive (else creating new will take ~4 hours)\n",
    "# # https://drive.google.com/open?id=1DlNxxNh6WA5ds7px844LnBbhEzV0Ydyo\n",
    "# wiki_df = rus_data_processing.get_wiki_tokenized_dataset(FILE_NAME)\n",
    "# pkl.dump(wiki_df, open(TEXT_OUTPUT_FILE, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe from pickle file - Wiki Text\n",
    "wiki_df =  pkl.load(open(TEXT_OUTPUT_FILE, \"rb\"))\n",
    "\n",
    "# load the dataframe from pickle file - Wiki Sections\n",
    "#wiki_df =  pkl.load(open(SECTION_OUTPUT_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q515</td>\n",
       "      <td>[]</td>\n",
       "      <td>[файл, два, ноль, один, один, токио, япония, ф...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q415638</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[нитри, тита, на, бинарные, соединения, бинарн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q333936</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[файл, один, нитрилы, нитри, лы, органические,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q7016</td>\n",
       "      <td>[STEM.Time, History_And_Society.History and so...</td>\n",
       "      <td>[файл, три, века, один, семь, век, сытин, один...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q2622089</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[файл, памятник, на, территории, словацкого, т...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                               mid_level_categories  \\\n",
       "0      Q515                                                 []   \n",
       "1   Q415638                                   [STEM.Chemistry]   \n",
       "2   Q333936                                   [STEM.Chemistry]   \n",
       "3     Q7016  [STEM.Time, History_And_Society.History and so...   \n",
       "4  Q2622089                                   [STEM.Chemistry]   \n",
       "\n",
       "                                              tokens  \n",
       "0  [файл, два, ноль, один, один, токио, япония, ф...  \n",
       "1  [нитри, тита, на, бинарные, соединения, бинарн...  \n",
       "2  [файл, один, нитрилы, нитри, лы, органические,...  \n",
       "3  [файл, три, века, один, семь, век, сытин, один...  \n",
       "4  [файл, памятник, на, территории, словацкого, т...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14787 articles in total.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{wiki_df.shape[0]} articles in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df['tokens'] = wiki_df['tokens'].apply(rus_data_processing.remove_short_words)\n",
    "wiki_df['tokens'] = wiki_df['tokens'].apply(rus_data_processing.remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x2ad88518de90>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = wiki_df['tokens'].apply(lambda x: len(x))\n",
    "count_df = b.value_counts().sort_index().rename_axis('count').reset_index(name='frequency')\n",
    "count_df.hist(column='count',bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique words in pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained fastText embeddings\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:]))).astype('float32')\n",
    "    return data\n",
    "\n",
    "pretrained_embs = load_vectors(PRETRAINED_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of unique Wiki tokens in fastText : 85.17%\n",
      "unique tokens not in fastText : 56317\n",
      "unique tokens in fastText : 323331\n"
     ]
    }
   ],
   "source": [
    "# get all unique tokens in the training dataset\n",
    "all_train_tokens = []\n",
    "\n",
    "for tokens in wiki_df['tokens']:\n",
    "    for token in tokens:\n",
    "        all_train_tokens.append(token)\n",
    "        \n",
    "all_train_tokens = list(set(all_train_tokens))\n",
    "\n",
    "# Analyse the wiki tokens and fastText embeddings\n",
    "train_token_count = len(all_train_tokens)\n",
    "token_in_fasttext = []\n",
    "token_not_in_fasttext = []\n",
    "train_token_in_fasttxt = 0\n",
    "\n",
    "for token in all_train_tokens:\n",
    "    if token in pretrained_embs.keys():\n",
    "        token_in_fasttext.append(token)\n",
    "        train_token_in_fasttxt = train_token_in_fasttxt + 1\n",
    "    else:\n",
    "        token_not_in_fasttext.append(token)\n",
    "        \n",
    "print(\"% of unique Wiki tokens in fastText : {:4.2f}%\".format(train_token_in_fasttxt/train_token_count*100))\n",
    "print('unique tokens not in fastText :', len(token_not_in_fasttext))\n",
    "print('unique tokens in fastText :', len(token_in_fasttext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens not in the fastText\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['дилеммах',\n",
       " 'фраиццоли',\n",
       " 'прементора',\n",
       " 'тэйяб',\n",
       " 'гиддха',\n",
       " 'рапаттони',\n",
       " 'мазохом',\n",
       " 'пьяджиа',\n",
       " 'миусах',\n",
       " 'брахмавид']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample tokens not in the fastText\")\n",
    "token_not_in_fasttext[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import preprocess\n",
    "import importlib\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from preprocess import remove_stop_words, train_validate_test_split\n",
    "from preprocess import tokenize_dataset, TensoredDataset, pad_collate_fn\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14439, 3)\n",
      "(14438, 3)\n"
     ]
    }
   ],
   "source": [
    "#Removing stop words\n",
    "wiki_df['tokens'] = wiki_df[\"tokens\"].apply(partial(remove_stop_words, language=\"russian\"))\n",
    "wiki_df.head()\n",
    "\n",
    "#Removing rows with missing labels\n",
    "mask = wiki_df.mid_level_categories.apply(lambda x: len(x) > 0)\n",
    "wiki_df = wiki_df[mask]\n",
    "wiki_df = wiki_df.reset_index(drop=True)\n",
    "print(wiki_df.shape)\n",
    "\n",
    "#Removing rows with no tokens\n",
    "mask = wiki_df.tokens.apply(lambda x: len(x) > 0)\n",
    "wiki_df = wiki_df[mask]\n",
    "wiki_df = wiki_df.reset_index(drop=True)\n",
    "print(wiki_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q415638</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[нитри, тита, бинарные, соединения, бинарное, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q333936</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[файл, нитрилы, нитри, лы, органические, соеди...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q7016</td>\n",
       "      <td>[STEM.Time, History_And_Society.History and so...</td>\n",
       "      <td>[файл, века, семь, век, сытин, девять, век, за...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q2622089</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[файл, памятник, территории, словацкого, техно...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q43440</td>\n",
       "      <td>[Culture.People, Culture.Language and literatu...</td>\n",
       "      <td>[джозуэ, карду, ччи, семь, июля, восемь, пять,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                               mid_level_categories  \\\n",
       "0   Q415638                                   [STEM.Chemistry]   \n",
       "1   Q333936                                   [STEM.Chemistry]   \n",
       "2     Q7016  [STEM.Time, History_And_Society.History and so...   \n",
       "3  Q2622089                                   [STEM.Chemistry]   \n",
       "4    Q43440  [Culture.People, Culture.Language and literatu...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [нитри, тита, бинарные, соединения, бинарное, ...   \n",
       "1  [файл, нитрилы, нитри, лы, органические, соеди...   \n",
       "2  [файл, века, семь, век, сытин, девять, век, за...   \n",
       "3  [файл, памятник, территории, словацкого, техно...   \n",
       "4  [джозуэ, карду, ччи, семь, июля, восемь, пять,...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize the labels\n",
    "# labels list: mlb.classes_\n",
    "mlb = MultiLabelBinarizer()\n",
    "wiki_df[\"labels\"] = list(mlb.fit_transform(wiki_df.mid_level_categories))\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val/test split\n",
    "wiki_train, wiki_valid, wiki_test = train_validate_test_split(wiki_df, seed=1)\n",
    "\n",
    "wiki_train = wiki_train.reset_index(drop=True)\n",
    "wiki_valid = wiki_valid.reset_index(drop=True)\n",
    "wiki_test = wiki_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is: 332286\n"
     ]
    }
   ],
   "source": [
    "# Building vocabulary\n",
    "vocab = list(set([y for x in list(wiki_train['tokens']) for y in x]))\n",
    "\n",
    "print(\"Vocab size is: {}\".format(len(vocab)))\n",
    "\n",
    "word_to_index = {\"<pad>\":0, \"<unk>\":1}\n",
    "for word in vocab:\n",
    "    if word not in word_to_index:\n",
    "        word_to_index[word] = len(word_to_index)\n",
    "index_to_word = {v:k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243516"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index[\"мышь\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11550/11550 [00:01<00:00, 8273.62it/s]\n",
      "100%|██████████| 1443/1443 [00:00<00:00, 7788.27it/s]\n",
      "100%|██████████| 1445/1445 [00:00<00:00, 5319.92it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_tokenized_train = tokenize_dataset(wiki_train, word_to_index)\n",
    "wiki_tokenized_val = tokenize_dataset(wiki_valid, word_to_index)\n",
    "wiki_tokenized_test = tokenize_dataset(wiki_test, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tokenized_datasets = {}\n",
    "wiki_tokenized_datasets['X_train'] = wiki_tokenized_train\n",
    "wiki_tokenized_datasets['X_val'] = wiki_tokenized_val\n",
    "wiki_tokenized_datasets['X_test'] = wiki_tokenized_test\n",
    "\n",
    "wiki_tokenized_datasets['y_train'] = list(wiki_train.labels)\n",
    "wiki_tokenized_datasets['y_val'] = list(wiki_valid.labels)\n",
    "wiki_tokenized_datasets['y_test'] = list(wiki_test.labels)\n",
    "\n",
    "wiki_tensor_dataset = {}\n",
    "wiki_tensor_dataset['train'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_train'], wiki_tokenized_datasets['y_train']\n",
    ")\n",
    "wiki_tensor_dataset['val'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_val'], wiki_tokenized_datasets['y_val']\n",
    ")\n",
    "wiki_tensor_dataset['test'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_test'], wiki_tokenized_datasets['y_test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([328770, 239804, 102415, 325113, 158786, 220977, 328030,  43325, 221888,\n",
       "         127505,  98824,  96066, 140931,  97442, 220823, 257025,  30344,  10292,\n",
       "         154233, 154233, 135475, 103867, 254246,  67001,  99975, 207493, 260339,\n",
       "         226821,  21905, 254618, 328770,  37987,  39043, 236966,  44727, 273113,\n",
       "         310671, 290354, 290354, 310671, 245919, 116975,  74613, 306608, 100693,\n",
       "         213306, 229436, 229436,  21514, 328770,  91248, 328770, 239804, 102415,\n",
       "         253144, 280790, 154233, 154233, 135475,  67370, 253144,  44639, 280790,\n",
       "         253144, 280790, 181887, 253144, 278368, 165038, 177782]),\n",
       " tensor([70.]),\n",
       " tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load aligned Russian embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888423it [02:27, 12814.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2.5 million\n",
    "embeddings = utils.load_vectors(PATH_TO_FOLDER + \"wiki.ru.align.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 332286\n",
      "No. of words from vocab found in fastText: 287017\n"
     ]
    }
   ],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "vocab_size = len(index_to_word)\n",
    "embed_dim = len(embeddings[\"apple\"])\n",
    "weights_matrix = np.zeros((vocab_size,embed_dim))\n",
    "\n",
    "words_found = 0\n",
    "for i, word in enumerate(word_to_index):\n",
    "    if word in embeddings.keys():\n",
    "        weights_matrix[i] = embeddings[word]\n",
    "        words_found += 1\n",
    "    else:\n",
    "        weights_matrix[i] = np.zeros(embed_dim)\n",
    "weights_matrix = torch.FloatTensor(weights_matrix)\n",
    "\n",
    "print(\"Total words in vocab: {}\".format(len(vocab)))\n",
    "print(\"No. of words from vocab found in fastText: {}\".format(words_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(mlb.classes_),\n",
    "    \"mid_features\": 100,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "    \n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = SWA(base_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
