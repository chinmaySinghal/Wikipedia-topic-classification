{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add plot loss, micro F1 score, ... (store in plot_cache !!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import spacy\n",
    "import string\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import mwparserfromhell\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_EMBEDDINGS_FOLDER = \"/scratch/mz2476/wiki/embeddings/\"\n",
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/\"\n",
    "PATH_TO_MODELS_FOLDER = \"/scratch/mz2476/wiki/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import preprocess\n",
    "import importlib\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from preprocess import remove_stop_words, train_validate_test_split\n",
    "from preprocess import tokenize_dataset, TensoredDataset, pad_collate_fn\n",
    "from preprocess import create_vocab_from_tokens, create_lookups_for_vocab\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_data_dict = {\n",
    "    \"en\": {\n",
    "        \"full_name\": \"english\",\n",
    "    },\n",
    "    \"ru\": {\n",
    "        \"full_name\": \"russian\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en vocab size is: 682850\n",
      "Number of articles: 99960\n",
      "ru vocab size is: 376365\n",
      "Number of articles: 14438\n"
     ]
    }
   ],
   "source": [
    "# LOAD vocab, tensor dataset, classes\n",
    "\n",
    "classes = torch.load(PATH_TO_DATA_FOLDER + \"classes_list.pt\")\n",
    "mlb = MultiLabelBinarizer(classes)\n",
    "\n",
    "for language in language_data_dict.keys():\n",
    "    vocab = torch.load(PATH_TO_DATA_FOLDER + f\"vocab_all_{language}.pt\")\n",
    "    print(f\"{language} vocab size is:\", len(vocab))\n",
    "    \n",
    "    wiki_df =  pkl.load(open(PATH_TO_DATA_FOLDER + f\"wikitext_tokenized_{language}.p\", \"rb\"))\n",
    "    \n",
    "    #Removing stop words\n",
    "    wiki_df['tokens'] = wiki_df[\"tokens\"].apply(partial(\n",
    "        remove_stop_words, language=language_data_dict[language][\"full_name\"]))\n",
    "\n",
    "    #Removing rows with missing labels\n",
    "    mask = wiki_df.mid_level_categories.apply(lambda x: len(x) > 0)\n",
    "    wiki_df = wiki_df[mask]\n",
    "    wiki_df = wiki_df.reset_index(drop=True)\n",
    "\n",
    "    #Removing rows with no tokens\n",
    "    mask = wiki_df.tokens.apply(lambda x: len(x) > 0)\n",
    "    wiki_df = wiki_df[mask]\n",
    "    wiki_df = wiki_df.reset_index(drop=True)\n",
    "    print(\"Number of articles:\", wiki_df.shape[0])\n",
    "    \n",
    "    # Binarize labels\n",
    "    wiki_df[\"labels\"] = list(mlb.fit_transform(wiki_df.mid_level_categories))\n",
    "    \n",
    "    # Save to dict\n",
    "    language_data_dict[language][\"vocab\"] = vocab\n",
    "    language_data_dict[language][\"wiki_df\"] = wiki_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['en', 'ru'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order: dict_keys(['en', 'ru'])\n"
     ]
    }
   ],
   "source": [
    "# Create combined vocab, index_to_word, word_to_index\n",
    "# 0 - <pad>, 1 - <unk> \n",
    "vocab = [\"<pad>\", \"<unk>\"]\n",
    "print(\"Order:\", language_data_dict.keys())\n",
    "for language in language_data_dict.keys(): # .keys() keep same order in Python version >= 3.7\n",
    "    vocab += language_data_dict[language][\"vocab\"][2:] # remove 0 - <pad>, 1 - <unk> \n",
    "    \n",
    "index_to_word, word_to_index = create_lookups_for_vocab(vocab)\n",
    "assert len(set(word_to_index)) == len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train size: 20000 \n",
      "Combined val size: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>labels</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>mid_level_categories_initial</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q3277682</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[STEM.Chemistry]</td>\n",
       "      <td>[тетрагидридоборат, алюминия, неорганическое, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q5366142</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[Geography.Americas]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ellsworth, town, pierce, county, wisconsin, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q1564037</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[History_And_Society.Transportation, History_A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[two, ships, royal, australian, navy, borne, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q386119</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[STEM.Medicine]</td>\n",
       "      <td>[STEM.Medicine]</td>\n",
       "      <td>[натализумаб, препарат, лечения, рассеянный, с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q32380</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[Geography.Asia]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[sergeyevka, references, populated, places, no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                                             labels  \\\n",
       "0  Q3277682  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  Q5366142  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  Q1564037  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3   Q386119  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    Q32380  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                mid_level_categories  \\\n",
       "0                                   [STEM.Chemistry]   \n",
       "1                               [Geography.Americas]   \n",
       "2  [History_And_Society.Transportation, History_A...   \n",
       "3                                    [STEM.Medicine]   \n",
       "4                                   [Geography.Asia]   \n",
       "\n",
       "  mid_level_categories_initial  \\\n",
       "0             [STEM.Chemistry]   \n",
       "1                          NaN   \n",
       "2                          NaN   \n",
       "3              [STEM.Medicine]   \n",
       "4                          NaN   \n",
       "\n",
       "                                              tokens  \n",
       "0  [тетрагидридоборат, алюминия, неорганическое, ...  \n",
       "1  [ellsworth, town, pierce, county, wisconsin, p...  \n",
       "2  [two, ships, royal, australian, navy, borne, n...  \n",
       "3  [натализумаб, препарат, лечения, рассеянный, с...  \n",
       "4  [sergeyevka, references, populated, places, no...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Take 10000 articles for train, 1000 for val for each language\n",
    "# combine them in one train set\n",
    "train_size = 10000\n",
    "val_size = 1000\n",
    "SEED = 57\n",
    "\n",
    "wiki_train, wiki_valid = [], []\n",
    "\n",
    "for language in language_data_dict.keys():\n",
    "    train, val = train_test_split(\n",
    "        language_data_dict[language][\"wiki_df\"], \n",
    "        train_size=train_size, test_size=val_size, random_state=SEED\n",
    "    )\n",
    "    wiki_train.append(train)\n",
    "    wiki_valid.append(val)\n",
    "    # save val df to evaluate the model on each language\n",
    "    language_data_dict[language][\"val_df\"] = val\n",
    "\n",
    "wiki_train = pd.concat(wiki_train).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "wiki_valid = pd.concat(wiki_valid).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined train size: {wiki_train.shape[0]} \\nCombined val size: {wiki_valid.shape[0]}\")\n",
    "wiki_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:02<00:00, 9404.46it/s] \n",
      "100%|██████████| 2000/2000 [00:00<00:00, 8183.25it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 8561.17it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 8734.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create tokenized datasets\n",
    "\n",
    "# CHANGE max number of tokens per article\n",
    "max_num_tokens = None\n",
    "\n",
    "wiki_tokenized_datasets = {}\n",
    "wiki_tokenized_datasets['X_train'] = tokenize_dataset(wiki_train, word_to_index, max_num_tokens=max_num_tokens)\n",
    "wiki_tokenized_datasets['X_val'] = tokenize_dataset(wiki_valid, word_to_index, max_num_tokens=max_num_tokens)\n",
    "wiki_tokenized_datasets['X_val_en'] = tokenize_dataset(language_data_dict[\"en\"][\"val_df\"], word_to_index, max_num_tokens=max_num_tokens)\n",
    "wiki_tokenized_datasets['X_val_ru'] = tokenize_dataset(language_data_dict[\"ru\"][\"val_df\"], word_to_index, max_num_tokens=max_num_tokens)\n",
    "\n",
    "\n",
    "wiki_tokenized_datasets['y_train'] = list(wiki_train.labels)\n",
    "wiki_tokenized_datasets['y_val'] = list(wiki_valid.labels)\n",
    "wiki_tokenized_datasets['y_val_en'] = list(language_data_dict[\"en\"][\"val_df\"].labels)\n",
    "wiki_tokenized_datasets['y_val_ru'] = list(language_data_dict[\"ru\"][\"val_df\"].labels)\n",
    "\n",
    "wiki_tensor_dataset = {}\n",
    "wiki_tensor_dataset['train'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_train'], wiki_tokenized_datasets['y_train']\n",
    ")\n",
    "wiki_tensor_dataset['val'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_val'], wiki_tokenized_datasets['y_val']\n",
    ")\n",
    "wiki_tensor_dataset['val_en'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_val_en'], wiki_tokenized_datasets['y_val_en']\n",
    ")\n",
    "wiki_tensor_dataset['val_ru'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_val_ru'], wiki_tokenized_datasets['y_val_ru']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([745956, 760146, 703528, 831442, 773820, 773819, 705726, 689135, 682869,\n",
       "         682955, 682869, 683091, 682955, 683091, 685870, 683446, 703529, 910230,\n",
       "         831445, 697369, 831442, 683711, 831442, 683155, 739292, 703186, 887234,\n",
       "         709503, 682854, 720851, 728411, 910231, 699855, 690736, 716010, 910232,\n",
       "         684890, 701520, 910233, 769653, 690046, 750457, 715540, 694654, 910234,\n",
       "         718589, 910235, 910236, 910237, 684604, 684996, 690696, 683636, 683005,\n",
       "         696930, 692770, 739292, 703186, 739292, 698833, 682966, 683379, 689555,\n",
       "         750651, 686270, 739924, 723006, 773819, 910238, 910239, 910240, 683725,\n",
       "         685634, 771348, 707096, 685120, 684095, 747467, 806538, 683736, 806538,\n",
       "         689476, 701421, 910237, 716655, 688173, 910239, 698943, 687992, 689775,\n",
       "         682880, 682852, 683112, 682852, 683219, 685068, 689947, 683091, 683112,\n",
       "         682869, 682852, 684436, 910237, 794887, 713828, 685911, 727474, 838683,\n",
       "         742214, 696409, 910241, 910239, 689549, 682986, 683091, 682853, 682852,\n",
       "         682852, 683219, 693497, 683426, 831442, 910237, 745377, 801305, 801302,\n",
       "         801305, 910242, 910243, 910244, 687693, 685068, 683638, 740488, 701387,\n",
       "         682869, 683091, 682852, 683219, 689549, 682986, 683091, 682880, 682869,\n",
       "         683112, 694229, 682950, 842222, 683561, 801305, 689035, 683143, 720892,\n",
       "         910237, 685947, 745669, 687161, 910245, 910246, 683639, 703529, 801311,\n",
       "         755172, 684038, 701421, 683413, 720851, 728411, 683030, 689792, 683781,\n",
       "         701065, 773789, 735994, 692113, 683143, 910237, 685947, 682986, 691605,\n",
       "         716562, 685890, 690245, 693598, 773821, 773822, 740377, 731130, 684031,\n",
       "         686270, 684277, 718945, 683781, 806538, 772281, 689435, 910237, 745377,\n",
       "         717600, 801302, 694865, 820483, 683736, 910247, 688028, 801302, 910248,\n",
       "         910249, 910250, 683038, 689605, 717312, 689280, 685068, 683638, 740488,\n",
       "         910251, 910252, 701387, 682880, 683049, 682852, 682852, 683219, 689947,\n",
       "         685068, 683091, 683112, 682852, 683219, 691625, 910253, 698197, 689546,\n",
       "         685068, 910254, 692058, 701338, 683091, 683112, 685078, 683219, 704050,\n",
       "         910237, 721810, 910255, 753040, 682966, 709521, 695073, 690032, 682950,\n",
       "         683561, 901522, 701421, 686188, 687408, 722797, 910256, 683564, 807491,\n",
       "         762538, 682966, 683711, 831442, 688758, 703166, 701421, 910237, 687362,\n",
       "         733053, 703166, 910257, 703528, 683636, 773819, 685890, 801311, 683030,\n",
       "         683091, 683112, 682853, 682869, 688489, 910258, 758096, 735532, 759098,\n",
       "         694996, 910253, 690046, 831445, 910259, 712106, 910260, 910261, 714661,\n",
       "         702619, 683824, 910262, 690142, 683966, 694247, 775031, 910237, 720695,\n",
       "         682884, 693727, 684832, 683091, 682880, 682955, 683091, 682880, 682962,\n",
       "         695003, 682986, 685890, 682966, 801311, 747483, 682966, 690046, 910260,\n",
       "         685844, 892082, 910263, 720695, 910264, 683763, 693296, 693267, 910265,\n",
       "         910266, 682966, 910267, 716614, 730844, 683781, 910260, 910268, 684129,\n",
       "         705986, 739292, 703186, 910269, 766250, 910237, 684436, 716655, 686363,\n",
       "         689476, 801311, 686870, 692100, 801014, 910260, 910270, 702664, 683638,\n",
       "         740488, 689549, 685892, 683038, 684320, 692024, 685068, 692535, 683038,\n",
       "         910271, 910268, 910271, 683781, 910272, 910273, 910274, 769653, 721592,\n",
       "         683030, 683112, 682852, 683112, 689792, 739943, 910275, 743806, 910237,\n",
       "         703528, 910276, 718710, 802420, 683112, 682852, 683112, 724224, 910237,\n",
       "         744508, 683038, 703004, 713646, 683635, 689983, 715440, 706841, 773857,\n",
       "         686612, 693294, 705175, 686320, 705175, 693278, 713828, 685911, 688915,\n",
       "         692693, 683038, 722059, 910253, 690253, 783975, 783976, 692615, 683736,\n",
       "         699908, 687909, 686188, 687408, 722797, 910253, 910256, 683564, 807491,\n",
       "         762538, 682966, 683446, 703529, 910230, 831445, 697369, 831442, 683711,\n",
       "         831442, 766766, 690142, 683966, 707529, 910277, 683636, 690002, 684217,\n",
       "         910278, 690648, 910253, 692779, 723904, 698342, 827451, 910237, 802827,\n",
       "         743083, 769653, 721592, 688234, 686129, 690431, 696239, 689940, 689549,\n",
       "         696700, 724878, 708197, 726208, 704695, 695213, 686213, 773840, 773841,\n",
       "         708908, 724224, 687664, 910237, 686758, 910279, 691780, 743806, 801240,\n",
       "         910280, 750457, 773856, 773857, 773795, 683091, 691494, 706703, 682880,\n",
       "         729091, 692357, 910281, 699693, 682869, 702195, 910282, 724224, 683884,\n",
       "         689549, 697069, 910283, 700736, 910253, 694976, 910253, 757650, 695085,\n",
       "         910284, 685984, 689908, 743823, 757444, 701555, 722059, 690046, 797149,\n",
       "         687302, 684977, 705726, 910237, 683049, 685612, 686445, 683091, 753584,\n",
       "         682853, 753584, 718784, 683446, 692022, 773846, 738089, 910253, 682947,\n",
       "         683446, 773846, 738089, 910253, 773866, 773815, 683446, 683541, 682954,\n",
       "         685837, 685096, 682954, 704621, 831445]),\n",
       " tensor([554.]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'val', 'val_en', 'val_ru'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_loaders.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "SAVE = False\n",
    "if SAVE:\n",
    "    # SAVE tensor datasets\n",
    "    torch.save(wiki_tensor_dataset, f'{PATH_TO_DATA_FOLDER}wiki_tensor_dataset_mixed_en_ru.pt')\n",
    "    print(\"Saved.\")\n",
    "    \n",
    "wiki_tensor_dataset = torch.load(f'{PATH_TO_DATA_FOLDER}wiki_tensor_dataset_mixed_en_ru.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load aligned en and ru embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2519370it [03:25, 12252.01it/s]\n",
      "665it [00:00, 6647.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 1059213\n",
      "No. of words from vocab found in embeddings: 538883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888423it [02:37, 11992.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 1059213\n",
      "No. of words from vocab found in embeddings: 485732\n"
     ]
    }
   ],
   "source": [
    "# for language in language_data_dict.keys():\n",
    "#     # 2.5 million\n",
    "#     embeddings = utils.load_vectors(PATH_TO_EMBEDDINGS_FOLDER + f\"wiki.{language}.align.vec\")\n",
    "#     #Creating the weight matrix for pretrained word embeddings\n",
    "#     weights_matrix_ve = utils.create_embeddings_matrix(word_to_index, embeddings)\n",
    "#     language_data_dict[language][\"weights_matrix_ve\"] = weights_matrix_ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings matrix shape: torch.Size([1059213, 300]), \n",
      "Vocab size: 1059213\n"
     ]
    }
   ],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "weights_matrix_ve = torch.zeros_like(language_data_dict[\"en\"][\"weights_matrix_ve\"])\n",
    "for language in language_data_dict.keys():\n",
    "    weights_matrix_ve += language_data_dict[language][\"weights_matrix_ve\"]\n",
    "\n",
    "assert weights_matrix_ve.shape[0] == len(vocab)\n",
    "print(f\"Embeddings matrix shape: {weights_matrix_ve.shape}, \\nVocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "if SAVE:\n",
    "    # SAVE embeddings matrix\n",
    "    torch.save(weights_matrix_ve, f'{PATH_TO_DATA_FOLDER}embedding_weights_matrix_mixed_en_ru.pt')\n",
    "    print(\"Saved.\")\n",
    "    \n",
    "weights_matrix_ve = torch.load(f'{PATH_TO_DATA_FOLDER}embedding_weights_matrix_mixed_en_ru.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model, evaluate on mix, en, ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, \n",
    "                num_epochs=10, device=device, model_name=\"model\", save_model=False):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch, \"epoch\")\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 100 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "                ))\n",
    "                print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "                ))\n",
    "\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "                    if save_model:\n",
    "                        optimizer.swap_swa_sgd()\n",
    "                        torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}{model_name}.pth\")\n",
    "                        print('Model Saved')\n",
    "                        print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 150, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "mixed_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10\n",
      "0 epoch\n",
      "Epoch: [1/10], Step: [101/625], Train_loss: 0.15629961982369422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.09897870950396125, Recall macro: 0.04141563920160813, F1 macro: 0.052314969086616744 \n",
      "Precision micro: 0.8114343029087262, Recall micro: 0.21983695652173912, F1 micro: 0.3459482574299765 \n",
      "Epoch: [1/10], Step: [201/625], Train_loss: 0.1267361579090357\n",
      "Precision macro: 0.16468062701480776, Recall macro: 0.08158205814452976, F1 macro: 0.0908571957810535 \n",
      "Precision micro: 0.766798418972332, Recall micro: 0.4217391304347826, F1 micro: 0.544179523141655 \n",
      "Epoch: [1/10], Step: [301/625], Train_loss: 0.11240997451047102\n",
      "Precision macro: 0.19501567930680716, Recall macro: 0.13021071963514766, F1 macro: 0.14798581153546586 \n",
      "Precision micro: 0.7859262399321747, Recall micro: 0.503804347826087, F1 micro: 0.6140089418777943 \n",
      "Epoch: [1/10], Step: [401/625], Train_loss: 0.10305273577570916\n",
      "Precision macro: 0.2960245276284442, Recall macro: 0.1433648817483208, F1 macro: 0.16406513284452193 \n",
      "Precision micro: 0.8042016806722689, Recall micro: 0.5201086956521739, F1 micro: 0.6316831683168317 \n",
      "Epoch: [1/10], Step: [501/625], Train_loss: 0.09665625710785389\n",
      "Precision macro: 0.3338008402834616, Recall macro: 0.19810671660924742, F1 macro: 0.22665284324300472 \n",
      "Precision micro: 0.7824663514005092, Recall micro: 0.5845108695652174, F1 micro: 0.6691553896406907 \n",
      "Epoch: [1/10], Step: [601/625], Train_loss: 0.0914112397780021\n",
      "Precision macro: 0.3595040859509076, Recall macro: 0.20208189066639637, F1 macro: 0.23693969725393363 \n",
      "Precision micro: 0.8092725216735771, Recall micro: 0.5834239130434783, F1 micro: 0.6780356860887415 \n",
      "1 epoch\n",
      "Epoch: [2/10], Step: [101/625], Train_loss: 0.06409710120409727\n",
      "Precision macro: 0.4002891964370619, Recall macro: 0.2298801879703947, F1 macro: 0.2643220709944694 \n",
      "Precision micro: 0.8044895003620565, Recall micro: 0.6038043478260869, F1 micro: 0.6898478733312636 \n",
      "Epoch: [2/10], Step: [201/625], Train_loss: 0.06303695803508162\n",
      "Precision macro: 0.42980003768804503, Recall macro: 0.24584539404145564, F1 macro: 0.28524650129948337 \n",
      "Precision micro: 0.8294573643410853, Recall micro: 0.6105978260869566, F1 micro: 0.7033964626702145 \n",
      "Epoch: [2/10], Step: [301/625], Train_loss: 0.06158515842010578\n",
      "Precision macro: 0.4585843072074296, Recall macro: 0.27034387572720253, F1 macro: 0.3157610460563967 \n",
      "Precision micro: 0.8152554233729881, Recall micro: 0.6331521739130435, F1 micro: 0.7127561945549098 \n",
      "Epoch: [2/10], Step: [401/625], Train_loss: 0.06106146601960063\n",
      "Precision macro: 0.5071584890162616, Recall macro: 0.29465490539128114, F1 macro: 0.34224439810173596 \n",
      "Precision micro: 0.8180260452364634, Recall micro: 0.6486413043478261, F1 micro: 0.7235525916944529 \n",
      "Epoch: [2/10], Step: [501/625], Train_loss: 0.06048950374126434\n",
      "Precision macro: 0.49525831322395975, Recall macro: 0.296654073575826, F1 macro: 0.346708030557094 \n",
      "Precision micro: 0.8176737687740133, Recall micro: 0.6361413043478261, F1 micro: 0.7155738957664681 \n",
      "Epoch: [2/10], Step: [601/625], Train_loss: 0.05980222996945182\n",
      "Precision macro: 0.47929247612526554, Recall macro: 0.3066376464345416, F1 macro: 0.347663278687125 \n",
      "Precision micro: 0.8035828534868842, Recall micro: 0.6826086956521739, F1 micro: 0.7381722009991184 \n",
      "2 epoch\n",
      "Epoch: [3/10], Step: [101/625], Train_loss: 0.05405363073572517\n",
      "Precision macro: 0.5545674279055958, Recall macro: 0.31901503954323646, F1 macro: 0.37177735557928565 \n",
      "Precision micro: 0.8280276816608997, Recall micro: 0.6502717391304348, F1 micro: 0.7284627092846272 \n",
      "Epoch: [3/10], Step: [201/625], Train_loss: 0.05462730170227587\n",
      "Precision macro: 0.5311031385555616, Recall macro: 0.32805783960637613, F1 macro: 0.3823610710280306 \n",
      "Precision micro: 0.808346709470305, Recall micro: 0.6842391304347826, F1 micro: 0.7411331861662988 \n",
      "Epoch: [3/10], Step: [301/625], Train_loss: 0.05446447714542349\n",
      "Precision macro: 0.5250178385348918, Recall macro: 0.3518057563707909, F1 macro: 0.39472380841360727 \n",
      "Precision micro: 0.8193484698914116, Recall micro: 0.6766304347826086, F1 micro: 0.7411817234707546 \n",
      "Epoch: [3/10], Step: [401/625], Train_loss: 0.05439717789646238\n",
      "Precision macro: 0.5156872953997524, Recall macro: 0.3145977624948228, F1 macro: 0.3670241504954823 \n",
      "Precision micro: 0.8210064167510976, Recall micro: 0.6605978260869565, F1 micro: 0.7321186568287908 \n",
      "Epoch: [3/10], Step: [501/625], Train_loss: 0.05440118435397744\n",
      "Precision macro: 0.536138031747211, Recall macro: 0.3189890860611116, F1 macro: 0.37419741851537514 \n",
      "Precision micro: 0.8301090397467464, Recall micro: 0.6413043478260869, F1 micro: 0.7235934386018702 \n",
      "Epoch: [3/10], Step: [601/625], Train_loss: 0.05389001622796059\n",
      "Precision macro: 0.5416000271142473, Recall macro: 0.34033842597148584, F1 macro: 0.3947372695952563 \n",
      "Precision micro: 0.8273789649415693, Recall micro: 0.6733695652173913, F1 micro: 0.7424719101123596 \n",
      "3 epoch\n",
      "Epoch: [4/10], Step: [101/625], Train_loss: 0.05170378137379885\n",
      "Precision macro: 0.5386262972959618, Recall macro: 0.3450353214490173, F1 macro: 0.3955933021067235 \n",
      "Precision micro: 0.8230059327620303, Recall micro: 0.6785326086956521, F1 micro: 0.7438188859100388 \n",
      "Epoch: [4/10], Step: [201/625], Train_loss: 0.05129063015803695\n",
      "Precision macro: 0.5788151767196928, Recall macro: 0.35351975280384507, F1 macro: 0.41745749466100274 \n",
      "Precision micro: 0.8475588338602037, Recall micro: 0.6557065217391305, F1 micro: 0.7393902252183239 \n",
      "Epoch: [4/10], Step: [301/625], Train_loss: 0.050807129591703415\n",
      "Precision macro: 0.563357816239737, Recall macro: 0.3490505474037171, F1 macro: 0.41064174004605825 \n",
      "Precision micro: 0.8328130419701699, Recall micro: 0.652445652173913, F1 micro: 0.7316775864696022 \n",
      "Epoch: [4/10], Step: [401/625], Train_loss: 0.0508897096849978\n",
      "Precision macro: 0.5365112175245182, Recall macro: 0.36073578061755646, F1 macro: 0.41785328949715655 \n",
      "Precision micro: 0.8478571428571429, Recall micro: 0.6451086956521739, F1 micro: 0.732716049382716 \n",
      "Epoch: [4/10], Step: [501/625], Train_loss: 0.050953211970627306\n",
      "Precision macro: 0.5498767157317173, Recall macro: 0.358401132305157, F1 macro: 0.41544174786054255 \n",
      "Precision micro: 0.8383736559139785, Recall micro: 0.6779891304347826, F1 micro: 0.7496995192307692 \n",
      "Epoch: [4/10], Step: [601/625], Train_loss: 0.05074954556301236\n",
      "Precision macro: 0.5831146875761379, Recall macro: 0.4100078045801448, F1 macro: 0.4640771424139878 \n",
      "Precision micro: 0.8170426065162907, Recall micro: 0.7086956521739131, F1 micro: 0.7590221187427241 \n",
      "4 epoch\n",
      "Epoch: [5/10], Step: [101/625], Train_loss: 0.04823776740580797\n",
      "Precision macro: 0.5857027536289917, Recall macro: 0.3951585356104404, F1 macro: 0.44910076482391226 \n",
      "Precision micro: 0.8592565860700109, Recall micro: 0.6470108695652174, F1 micro: 0.7381801271120757 \n",
      "Epoch: [5/10], Step: [201/625], Train_loss: 0.04824793668463826\n",
      "Precision macro: 0.5530955216398132, Recall macro: 0.4043526186309688, F1 macro: 0.4511320663404758 \n",
      "Precision micro: 0.8162230671736375, Recall micro: 0.7, F1 micro: 0.7536571094207137 \n",
      "Epoch: [5/10], Step: [301/625], Train_loss: 0.04867528652772307\n",
      "Precision macro: 0.6109912113811553, Recall macro: 0.40792879625420964, F1 macro: 0.4631294595334989 \n",
      "Precision micro: 0.8273358585858586, Recall micro: 0.7122282608695653, F1 micro: 0.7654789719626168 \n",
      "Epoch: [5/10], Step: [401/625], Train_loss: 0.04862292892765254\n",
      "Precision macro: 0.5708930743372012, Recall macro: 0.39999175642030965, F1 macro: 0.45107160236526606 \n",
      "Precision micro: 0.826128417037508, Recall micro: 0.70625, F1 micro: 0.7615001464986815 \n",
      "Epoch: [5/10], Step: [501/625], Train_loss: 0.048755429916083816\n",
      "Precision macro: 0.5463186465046522, Recall macro: 0.387010508844754, F1 macro: 0.43649355086239267 \n",
      "Precision micro: 0.8489361702127659, Recall micro: 0.6505434782608696, F1 micro: 0.7366153846153846 \n",
      "Epoch: [5/10], Step: [601/625], Train_loss: 0.04874203288927674\n",
      "Precision macro: 0.576877171660943, Recall macro: 0.4023561273942734, F1 macro: 0.45966009558109633 \n",
      "Precision micro: 0.8409858203916273, Recall micro: 0.6769021739130435, F1 micro: 0.7500752785305631 \n",
      "5 epoch\n",
      "Epoch: [6/10], Step: [101/625], Train_loss: 0.04659154713153839\n",
      "Precision macro: 0.5789710354989935, Recall macro: 0.41881740286814434, F1 macro: 0.4706441618566005 \n",
      "Precision micro: 0.8261429491307147, Recall micro: 0.6972826086956522, F1 micro: 0.7562628941939288 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/10], Step: [201/625], Train_loss: 0.046873171385377645\n",
      "Precision macro: 0.6068166103398616, Recall macro: 0.41598617492709516, F1 macro: 0.47578397257063876 \n",
      "Precision micro: 0.8423823626192827, Recall micro: 0.6956521739130435, F1 micro: 0.7620181574639082 \n",
      "Epoch: [6/10], Step: [301/625], Train_loss: 0.04696578973904252\n",
      "Precision macro: 0.5823945184830888, Recall macro: 0.418430200601339, F1 macro: 0.4730836892375312 \n",
      "Precision micro: 0.8402891883010187, Recall micro: 0.6948369565217392, F1 micro: 0.7606723189052508 \n",
      "Epoch: [6/10], Step: [401/625], Train_loss: 0.04692789550870657\n",
      "Precision macro: 0.5983847352105974, Recall macro: 0.4027496071202367, F1 macro: 0.4639237194864389 \n",
      "Precision micro: 0.8362068965517241, Recall micro: 0.7116847826086956, F1 micro: 0.7689371697005284 \n",
      "Epoch: [6/10], Step: [501/625], Train_loss: 0.04685528987646103\n",
      "Precision macro: 0.6290039716007341, Recall macro: 0.4371147125187365, F1 macro: 0.4919223402684526 \n",
      "Precision micro: 0.8229742922225838, Recall micro: 0.6872282608695652, F1 micro: 0.7490004442470014 \n",
      "Epoch: [6/10], Step: [601/625], Train_loss: 0.04692223392737409\n",
      "Precision macro: 0.6140861471728797, Recall macro: 0.411019054061123, F1 macro: 0.4718535093784179 \n",
      "Precision micro: 0.8486616334934798, Recall micro: 0.6720108695652174, F1 micro: 0.7500758265089474 \n",
      "6 epoch\n",
      "Epoch: [7/10], Step: [101/625], Train_loss: 0.045567294340580704\n",
      "Precision macro: 0.5991503888076671, Recall macro: 0.40400228506599767, F1 macro: 0.4602696299665011 \n",
      "Precision micro: 0.8158536585365853, Recall micro: 0.7271739130434782, F1 micro: 0.7689655172413793 \n",
      "Epoch: [7/10], Step: [201/625], Train_loss: 0.045280752824619415\n",
      "Precision macro: 0.5985911017863356, Recall macro: 0.4353069138738565, F1 macro: 0.48193933891623997 \n",
      "Precision micro: 0.8194308145240432, Recall micro: 0.6807065217391305, F1 micro: 0.7436544455989312 \n",
      "Epoch: [7/10], Step: [301/625], Train_loss: 0.04523230699201425\n",
      "Precision macro: 0.6078326337345943, Recall macro: 0.40804180929596306, F1 macro: 0.46861384543866963 \n",
      "Precision micro: 0.8406666666666667, Recall micro: 0.6853260869565218, F1 micro: 0.7550898203592815 \n",
      "Epoch: [7/10], Step: [401/625], Train_loss: 0.045295675201341506\n",
      "Precision macro: 0.5917547406166445, Recall macro: 0.42187094980992296, F1 macro: 0.47370522870451043 \n",
      "Precision micro: 0.8077395577395577, Recall micro: 0.7146739130434783, F1 micro: 0.7583621683967705 \n",
      "Epoch: [7/10], Step: [501/625], Train_loss: 0.045860162042081355\n",
      "Precision macro: 0.6310426328262092, Recall macro: 0.4118688411837739, F1 macro: 0.4789695286600235 \n",
      "Precision micro: 0.8421750663129973, Recall micro: 0.6902173913043478, F1 micro: 0.7586618876941458 \n",
      "Epoch: [7/10], Step: [601/625], Train_loss: 0.04575975338617961\n",
      "Precision macro: 0.5911472184174411, Recall macro: 0.4101921991111364, F1 macro: 0.4646682654483353 \n",
      "Precision micro: 0.8257234726688103, Recall micro: 0.6978260869565217, F1 micro: 0.7564064801178204 \n",
      "7 epoch\n",
      "Epoch: [8/10], Step: [101/625], Train_loss: 0.04460199451074004\n",
      "Precision macro: 0.6191341443388446, Recall macro: 0.44258215596809425, F1 macro: 0.492938213653056 \n",
      "Precision micro: 0.804093567251462, Recall micro: 0.7472826086956522, F1 micro: 0.7746478873239437 \n",
      "Epoch: [8/10], Step: [201/625], Train_loss: 0.04577644589357078\n",
      "Precision macro: 0.5787005259675185, Recall macro: 0.41985458865397335, F1 macro: 0.4692945350333439 \n",
      "Precision micro: 0.8380462724935732, Recall micro: 0.7086956521739131, F1 micro: 0.767962308598351 \n",
      "Epoch: [8/10], Step: [301/625], Train_loss: 0.04505472969263792\n",
      "Precision macro: 0.6036719047762944, Recall macro: 0.4094092337431954, F1 macro: 0.4630789301099229 \n",
      "Precision micro: 0.8321678321678322, Recall micro: 0.7114130434782608, F1 micro: 0.7670670963961326 \n",
      "Epoch: [8/10], Step: [401/625], Train_loss: 0.04471001305617392\n",
      "Precision macro: 0.6196893138244703, Recall macro: 0.42735890074283084, F1 macro: 0.48824302767733524 \n",
      "Precision micro: 0.8309859154929577, Recall micro: 0.7054347826086956, F1 micro: 0.7630805408583186 \n",
      "Epoch: [8/10], Step: [501/625], Train_loss: 0.04470896954089403\n",
      "Precision macro: 0.630357895975737, Recall macro: 0.431003341167747, F1 macro: 0.49120925418177247 \n",
      "Precision micro: 0.829746835443038, Recall micro: 0.7125, F1 micro: 0.7666666666666668 \n",
      "Epoch: [8/10], Step: [601/625], Train_loss: 0.04469806455386182\n",
      "Precision macro: 0.5797118393162916, Recall macro: 0.4156732649949145, F1 macro: 0.4716592948450749 \n",
      "Precision micro: 0.8221665623043206, Recall micro: 0.7135869565217391, F1 micro: 0.7640384055862672 \n",
      "8 epoch\n",
      "Epoch: [9/10], Step: [101/625], Train_loss: 0.04441060412675142\n",
      "Precision macro: 0.6503854382193857, Recall macro: 0.4424562723469249, F1 macro: 0.5035583684240603 \n",
      "Precision micro: 0.8299874529485571, Recall micro: 0.7190217391304348, F1 micro: 0.7705299941758882 \n",
      "Epoch: [9/10], Step: [201/625], Train_loss: 0.04431283989921212\n",
      "Precision macro: 0.6190319827930227, Recall macro: 0.4697731956482745, F1 macro: 0.5250659749069885 \n",
      "Precision micro: 0.8352903840050777, Recall micro: 0.7152173913043478, F1 micro: 0.7706045966915531 \n",
      "Epoch: [9/10], Step: [301/625], Train_loss: 0.043844279889017346\n",
      "Precision macro: 0.6056563795630253, Recall macro: 0.4519419331388086, F1 macro: 0.5047752506381156 \n",
      "Precision micro: 0.8392396907216495, Recall micro: 0.7078804347826086, F1 micro: 0.7679834905660377 \n",
      "Epoch: [9/10], Step: [401/625], Train_loss: 0.04367922376375646\n",
      "Precision macro: 0.5646464984277494, Recall macro: 0.4404029992546004, F1 macro: 0.48380468193714726 \n",
      "Precision micro: 0.8358395989974937, Recall micro: 0.725, F1 micro: 0.7764842840512223 \n",
      "Epoch: [9/10], Step: [501/625], Train_loss: 0.04372751133143902\n",
      "Precision macro: 0.591550978176976, Recall macro: 0.47070862468120184, F1 macro: 0.5061007236004247 \n",
      "Precision micro: 0.814577615126563, Recall micro: 0.7258152173913044, F1 micro: 0.7676390285960627 \n",
      "Epoch: [9/10], Step: [601/625], Train_loss: 0.04374653988517821\n",
      "Precision macro: 0.5983545076464809, Recall macro: 0.42298170377448213, F1 macro: 0.4817703444233896 \n",
      "Precision micro: 0.844599072233267, Recall micro: 0.6926630434782609, F1 micro: 0.7611227232009554 \n",
      "9 epoch\n",
      "Epoch: [10/10], Step: [101/625], Train_loss: 0.04298324730247259\n",
      "Precision macro: 0.6099303958379084, Recall macro: 0.419793394535874, F1 macro: 0.4754360619766013 \n",
      "Precision micro: 0.8228295819935691, Recall micro: 0.6953804347826087, F1 micro: 0.7537555228276879 \n",
      "Epoch: [10/10], Step: [201/625], Train_loss: 0.04242834186181426\n",
      "Precision macro: 0.6188912061985749, Recall macro: 0.4590108385598942, F1 macro: 0.5038903067574099 \n",
      "Precision micro: 0.8243827160493827, Recall micro: 0.7258152173913044, F1 micro: 0.7719653179190751 \n",
      "Epoch: [10/10], Step: [301/625], Train_loss: 0.04235137321054935\n",
      "Precision macro: 0.6084635537619392, Recall macro: 0.44548263846340297, F1 macro: 0.4881869511226291 \n",
      "Precision micro: 0.8211330362826226, Recall micro: 0.7010869565217391, F1 micro: 0.7563764291996482 \n",
      "Epoch: [10/10], Step: [401/625], Train_loss: 0.042764213695190845\n",
      "Precision macro: 0.6268878222869598, Recall macro: 0.43471173297005206, F1 macro: 0.4939314378545521 \n",
      "Precision micro: 0.8362235067437379, Recall micro: 0.7076086956521739, F1 micro: 0.7665587282896674 \n",
      "Epoch: [10/10], Step: [501/625], Train_loss: 0.043073407098650936\n",
      "Precision macro: 0.6155801643396549, Recall macro: 0.4418823623382353, F1 macro: 0.493515316689933 \n",
      "Precision micro: 0.8193819381938194, Recall micro: 0.7421195652173913, F1 micro: 0.7788392984457436 \n",
      "Epoch: [10/10], Step: [601/625], Train_loss: 0.04318242601429423\n",
      "Precision macro: 0.5898088466829535, Recall macro: 0.43307115424197967, F1 macro: 0.4821143715166483 \n",
      "Precision micro: 0.8188142770719903, Recall micro: 0.7355978260869566, F1 micro: 0.7749785284855425 \n"
     ]
    }
   ],
   "source": [
    "SAVE_MODEL = False\n",
    "\n",
    "lr = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU(),\n",
    "}\n",
    "    \n",
    "result = {\n",
    "    \"optimizer\": \"SWA\", \n",
    "    \"num_hidden\": options[\"num_layers\"],\n",
    "    \"dim_hidden\": options[\"mid_features\"],\n",
    "    \"dropout_rate\": options[\"dropout_rate\"],\n",
    "    \"learning_rate\": lr,\n",
    "    \"num_epochs\": num_epochs\n",
    "}\n",
    "\n",
    "print(\"\\n\", result)\n",
    "\n",
    "# uncommen if train from scratch\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = SWA(base_opt) \n",
    "\n",
    "# train the model\n",
    "model_name = \"mixed_\" + \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "print(model_name)\n",
    "metrics_dict = train_model(\n",
    "    wiki_loaders, model, criterion, optimizer, num_epochs=num_epochs, \n",
    "    model_name=model_name, save_model=SAVE_MODEL\n",
    ")\n",
    "result.update(metrics_dict)\n",
    "\n",
    "# results_df = results_df.append(result, ignore_index=True)\n",
    "#     results_df.to_csv(\"results/results_tuning_2_3_layers_maxlen_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.6156, Recall macro: 0.4419, F1 macro: 0.4935 \n",
      "Precision micro: 0.8194, Recall micro: 0.7421, F1 micro: 0.7788 \n"
     ]
    }
   ],
   "source": [
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_macro': 0.5479288798111839,\n",
       " 'recall_macro': 0.4152753851829956,\n",
       " 'f1_macro': 0.45539408698406114,\n",
       " 'precision_micro': 0.8394495412844036,\n",
       " 'recall_micro': 0.7332570120206068,\n",
       " 'f1_micro': 0.7827681026581118}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(wiki_loaders[\"val_en\"], model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_macro': 0.5302778805071525,\n",
       " 'recall_macro': 0.3201758063143576,\n",
       " 'f1_macro': 0.37577594460618263,\n",
       " 'precision_micro': 0.8476128188358404,\n",
       " 'recall_micro': 0.6704604242110709,\n",
       " 'f1_micro': 0.7487001733102252}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(wiki_loaders[\"val_ru\"], model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model_names = {\n",
    "    \"frozen\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_frozen.pth\",\n",
    "    },\n",
    "    \"finetuned\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_init_pretrained.pth\",   \n",
    "    },\n",
    "    \"trained\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\",   \n",
    "    },\n",
    "}\n",
    "\n",
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU(),\n",
    "}\n",
    "\n",
    "for model_name in dict_model_names.keys():\n",
    "    model = FinalModel(options)\n",
    "    # load the state dict from file\n",
    "    file_name = dict_model_names[model_name][\"file_name\"]\n",
    "    model.load_state_dict(torch.load(\n",
    "        f\"{PATH_TO_MODELS_FOLDER}{file_name}\",\n",
    "        map_location=torch.device('cpu')\n",
    "    ))\n",
    "    model.to(device)\n",
    "    # save model to dict\n",
    "    dict_model_names[model_name][\"model\"] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- frozen\n",
      "Precision macro: 0.3503, Recall macro: 0.1435, F1 macro: 0.1715 \n",
      "Precision micro: 0.7678, Recall micro: 0.2693, F1 micro: 0.3987 \n",
      "--- finetuned\n",
      "Precision macro: 0.6015, Recall macro: 0.4704, F1 macro: 0.516 \n",
      "Precision micro: 0.8187, Recall micro: 0.7468, F1 micro: 0.7811 \n",
      "--- trained\n",
      "Precision macro: 0.5225, Recall macro: 0.3148, F1 macro: 0.3643 \n",
      "Precision micro: 0.8348, Recall micro: 0.6714, F1 micro: 0.7443 \n"
     ]
    }
   ],
   "source": [
    "from utils import test_model\n",
    "\n",
    "for model_name in dict_model_names.keys():\n",
    "    model = dict_model_names[model_name][\"model\"]\n",
    "    # print aggregated metrics\n",
    "    metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "    metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "    print(\"---\", model_name)\n",
    "    print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "        metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "    ))\n",
    "    print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "        metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "    ))\n",
    "    \n",
    "    # save per class tables\n",
    "    df_per_class_metrics = utils.create_per_class_tables(\n",
    "        wiki_loaders[\"val\"], model, device, classes, threshold=0.5\n",
    "    )\n",
    "    dict_model_names[model_name][\"df_results\"] = df_per_class_metrics\n",
    "    # SAVE to file\n",
    "#     df_per_class_metrics.to_csv(f\"results/ru_per_class_metrics_val_{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>count</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Culture.Arts</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1434</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Culture.Broadcasting</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1418</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Culture.Crafts and hobbies</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1437</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Culture.Entertainment</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1386</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.626506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Culture.Food and drink</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1433</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Culture.Games and toys</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1425</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Culture.Internet culture</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Culture.Language and literature</td>\n",
       "      <td>552.0</td>\n",
       "      <td>848</td>\n",
       "      <td>58</td>\n",
       "      <td>494</td>\n",
       "      <td>43</td>\n",
       "      <td>0.919926</td>\n",
       "      <td>0.894928</td>\n",
       "      <td>0.907254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Culture.Media</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Culture.Music</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.786885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Culture.Performing arts</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1425</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Culture.Philosophy and religion</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1372</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Culture.Plastic arts</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1423</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Culture.Sports</td>\n",
       "      <td>237.0</td>\n",
       "      <td>1189</td>\n",
       "      <td>17</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.928270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Culture.Visual arts</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1412</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Geography.Africa</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1406</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.425532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Geography.Americas</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1219</td>\n",
       "      <td>96</td>\n",
       "      <td>107</td>\n",
       "      <td>21</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.527094</td>\n",
       "      <td>0.646526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Geography.Antarctica</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Geography.Asia</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>84</td>\n",
       "      <td>164</td>\n",
       "      <td>44</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.719298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Geography.Bodies of water</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Geography.Europe</td>\n",
       "      <td>567.0</td>\n",
       "      <td>746</td>\n",
       "      <td>99</td>\n",
       "      <td>468</td>\n",
       "      <td>130</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.803433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Geography.Landforms</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1434</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Geography.Maps</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Geography.Oceania</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1417</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Geography.Parks</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1436</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>History_And_Society.Business and economics</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1410</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>History_And_Society.Education</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1430</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>History_And_Society.History and society</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1278</td>\n",
       "      <td>102</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.382353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>History_And_Society.Military and warfare</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1372</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.720721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>History_And_Society.Politics and government</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1383</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>History_And_Society.Transportation</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1368</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.728814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>STEM.Biology</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1365</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>STEM.Chemistry</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1436</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>STEM.Engineering</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>STEM.Geosciences</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1424</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>STEM.Information science</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>STEM.Mathematics</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1440</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>STEM.Medicine</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1414</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>STEM.Meteorology</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1441</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>STEM.Physics</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>STEM.Science</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1427</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>STEM.Space</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1413</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.983051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>STEM.Technology</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1379</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>STEM.Time</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1437</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class_name  count    TN   FN   TP   FP  \\\n",
       "0                                  Culture.Arts    9.0  1434    8    1    0   \n",
       "1                          Culture.Broadcasting   25.0  1418   22    3    0   \n",
       "2                    Culture.Crafts and hobbies    6.0  1437    6    0    0   \n",
       "3                         Culture.Entertainment   50.0  1386   24   26    7   \n",
       "4                        Culture.Food and drink    9.0  1433    4    5    1   \n",
       "5                        Culture.Games and toys   18.0  1425    5   13    0   \n",
       "6                      Culture.Internet culture    1.0  1442    1    0    0   \n",
       "7               Culture.Language and literature  552.0   848   58  494   43   \n",
       "8                                 Culture.Media    1.0  1442    1    0    0   \n",
       "9                                 Culture.Music   58.0  1369   10   48   16   \n",
       "10                      Culture.Performing arts   18.0  1425   17    1    0   \n",
       "11              Culture.Philosophy and religion   59.0  1372   34   25   12   \n",
       "12                         Culture.Plastic arts   18.0  1423   15    3    2   \n",
       "13                               Culture.Sports  237.0  1189   17  220   17   \n",
       "14                          Culture.Visual arts   30.0  1412   25    5    1   \n",
       "15                             Geography.Africa   31.0  1406   21   10    6   \n",
       "16                           Geography.Americas  203.0  1219   96  107   21   \n",
       "17                         Geography.Antarctica    1.0  1442    1    0    0   \n",
       "18                               Geography.Asia  248.0  1151   84  164   44   \n",
       "19                    Geography.Bodies of water    8.0  1435    8    0    0   \n",
       "20                             Geography.Europe  567.0   746   99  468  130   \n",
       "21                          Geography.Landforms    9.0  1434    9    0    0   \n",
       "22                               Geography.Maps    0.0  1443    0    0    0   \n",
       "23                            Geography.Oceania   25.0  1417   22    3    1   \n",
       "24                              Geography.Parks    7.0  1436    7    0    0   \n",
       "25   History_And_Society.Business and economics   31.0  1410   22    9    2   \n",
       "26                History_And_Society.Education   13.0  1430   13    0    0   \n",
       "27      History_And_Society.History and society  141.0  1278  102   39   24   \n",
       "28     History_And_Society.Military and warfare   66.0  1372   26   40    5   \n",
       "29  History_And_Society.Politics and government   47.0  1383   31   16   13   \n",
       "30           History_And_Society.Transportation   69.0  1368   26   43    6   \n",
       "31                                 STEM.Biology   72.0  1365   20   52    6   \n",
       "32                               STEM.Chemistry    5.0  1436    2    3    2   \n",
       "33                             STEM.Engineering    1.0  1442    1    0    0   \n",
       "34                             STEM.Geosciences   19.0  1424   13    6    0   \n",
       "35                     STEM.Information science    1.0  1442    1    0    0   \n",
       "36                             STEM.Mathematics    3.0  1440    3    0    0   \n",
       "37                                STEM.Medicine   27.0  1414   12   15    2   \n",
       "38                             STEM.Meteorology    2.0  1441    2    0    0   \n",
       "39                                 STEM.Physics    8.0  1435    8    0    0   \n",
       "40                                 STEM.Science   16.0  1427   16    0    0   \n",
       "41                                   STEM.Space   30.0  1413    1   29    0   \n",
       "42                              STEM.Technology   53.0  1379   21   32   11   \n",
       "43                                    STEM.Time    6.0  1437    6    0    0   \n",
       "\n",
       "    precision    recall        f1  \n",
       "0    1.000000  0.111111  0.200000  \n",
       "1    1.000000  0.120000  0.214286  \n",
       "2    0.000000  0.000000  0.000000  \n",
       "3    0.787879  0.520000  0.626506  \n",
       "4    0.833333  0.555556  0.666667  \n",
       "5    1.000000  0.722222  0.838710  \n",
       "6    0.000000  0.000000  0.000000  \n",
       "7    0.919926  0.894928  0.907254  \n",
       "8    0.000000  0.000000  0.000000  \n",
       "9    0.750000  0.827586  0.786885  \n",
       "10   1.000000  0.055556  0.105263  \n",
       "11   0.675676  0.423729  0.520833  \n",
       "12   0.600000  0.166667  0.260870  \n",
       "13   0.928270  0.928270  0.928270  \n",
       "14   0.833333  0.166667  0.277778  \n",
       "15   0.625000  0.322581  0.425532  \n",
       "16   0.835938  0.527094  0.646526  \n",
       "17   0.000000  0.000000  0.000000  \n",
       "18   0.788462  0.661290  0.719298  \n",
       "19   0.000000  0.000000  0.000000  \n",
       "20   0.782609  0.825397  0.803433  \n",
       "21   0.000000  0.000000  0.000000  \n",
       "22   0.000000  0.000000  0.000000  \n",
       "23   0.750000  0.120000  0.206897  \n",
       "24   0.000000  0.000000  0.000000  \n",
       "25   0.818182  0.290323  0.428571  \n",
       "26   0.000000  0.000000  0.000000  \n",
       "27   0.619048  0.276596  0.382353  \n",
       "28   0.888889  0.606061  0.720721  \n",
       "29   0.551724  0.340426  0.421053  \n",
       "30   0.877551  0.623188  0.728814  \n",
       "31   0.896552  0.722222  0.800000  \n",
       "32   0.600000  0.600000  0.600000  \n",
       "33   0.000000  0.000000  0.000000  \n",
       "34   1.000000  0.315789  0.480000  \n",
       "35   0.000000  0.000000  0.000000  \n",
       "36   0.000000  0.000000  0.000000  \n",
       "37   0.882353  0.555556  0.681818  \n",
       "38   0.000000  0.000000  0.000000  \n",
       "39   0.000000  0.000000  0.000000  \n",
       "40   0.000000  0.000000  0.000000  \n",
       "41   1.000000  0.966667  0.983051  \n",
       "42   0.744186  0.603774  0.666667  \n",
       "43   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_model_names[\"trained\"][\"df_results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model. Use pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PATH_TO_MODELS_FOLDER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1314704cba1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPRETRAINED_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH_TO_MODELS_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"en_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m best_params = {\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'SWA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'num_hidden'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PATH_TO_MODELS_FOLDER' is not defined"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL = PATH_TO_MODELS_FOLDER + \"en_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\"\n",
    "\n",
    "best_params = {\n",
    "    'optimizer': 'SWA',\n",
    "    'num_hidden': 2,\n",
    "    'dim_hidden': 150,\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": best_params[\"num_hidden\"],\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": best_params[\"dim_hidden\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "pretrained_state_dict = torch.load(PRETRAINED_MODEL)\n",
    "\n",
    "# take pretrained params\n",
    "model.layer_out[0].weight.data = pretrained_state_dict['layer_out.0.weight']\n",
    "model.layer_out[0].bias.data = pretrained_state_dict['layer_out.0.bias']\n",
    "model.layer_out[2].weight.data = pretrained_state_dict['layer_out.2.weight']\n",
    "model.layer_out[2].bias.data = pretrained_state_dict['layer_out.2.bias']\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (layer_bag_of_words): BagOfWords(\n",
       "    (embed_e): Embedding(376365, 300)\n",
       "  )\n",
       "  (layer_out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained params:\n",
      "\n",
      "Precision macro: 0.3503, Recall macro: 0.1435, F1 macro: 0.1715 \n",
      "Precision micro: 0.7678, Recall micro: 0.2693, F1 micro: 0.3987 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Using pretrained params:\\n\")\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save frozen model\n",
    "# model_name = \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_frozen\"\n",
    "# torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune on Russian articles OR train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, \n",
    "                num_epochs=10, device=device, model_name=\"model\", save_model=False):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch, \"epoch\")\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 100 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "                ))\n",
    "                print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "                ))\n",
    "\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "                    if save_model:\n",
    "                        optimizer.swap_swa_sgd()\n",
    "                        torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}{model_name}.pth\")\n",
    "                        print('Model Saved')\n",
    "                        print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 150, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10\n",
      "0 epoch\n",
      "Epoch: [1/10], Step: [101/361], Train_loss: 0.16394229903817176\n",
      "Precision macro: 0.03774863222660023, Recall macro: 0.018300674097775547, F1 macro: 0.021573619594354748 \n",
      "Precision micro: 0.7364085667215815, Recall micro: 0.15964285714285714, F1 micro: 0.26240093924273555 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [201/361], Train_loss: 0.13773150239139795\n",
      "Precision macro: 0.10007624693922357, Recall macro: 0.051343324197594804, F1 macro: 0.058254934882816585 \n",
      "Precision micro: 0.8041958041958042, Recall micro: 0.32857142857142857, F1 micro: 0.4665314401622718 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [301/361], Train_loss: 0.12528121824065844\n",
      "Precision macro: 0.11422569054993983, Recall macro: 0.07616071949318902, F1 macro: 0.0814212545866872 \n",
      "Precision micro: 0.7639405204460966, Recall micro: 0.44035714285714284, F1 micro: 0.5586769370185772 \n",
      "Model Saved\n",
      "\n",
      "1 epoch\n",
      "Epoch: [2/10], Step: [101/361], Train_loss: 0.08439337681978941\n",
      "Precision macro: 0.2259704472226207, Recall macro: 0.12202399873145725, F1 macro: 0.13984515352159987 \n",
      "Precision micro: 0.8107951247823564, Recall micro: 0.49892857142857144, F1 micro: 0.6177315940747292 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [201/361], Train_loss: 0.08378258358687163\n",
      "Precision macro: 0.22118828289098233, Recall macro: 0.1281634524974851, F1 macro: 0.1469414566364466 \n",
      "Precision micro: 0.8135977337110482, Recall micro: 0.5128571428571429, F1 micro: 0.6291347207009859 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [301/361], Train_loss: 0.08123884287973245\n",
      "Precision macro: 0.27261289098023156, Recall macro: 0.17644959279690553, F1 macro: 0.20337303885016708 \n",
      "Precision micro: 0.8267246061922868, Recall micro: 0.5435714285714286, F1 micro: 0.6558931264813618 \n",
      "Model Saved\n",
      "\n",
      "2 epoch\n",
      "Epoch: [3/10], Step: [101/361], Train_loss: 0.07191145554184913\n",
      "Precision macro: 0.2895974070788268, Recall macro: 0.17686146914913883, F1 macro: 0.20885121877858814 \n",
      "Precision micro: 0.8395130049806309, Recall micro: 0.5417857142857143, F1 micro: 0.6585630562187974 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [201/361], Train_loss: 0.07064960964024067\n",
      "Precision macro: 0.29619340720331605, Recall macro: 0.20542084782192085, F1 macro: 0.22922444832589467 \n",
      "Precision micro: 0.798941798941799, Recall micro: 0.5932142857142857, F1 micro: 0.6808772289403566 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [301/361], Train_loss: 0.06952822438130776\n",
      "Precision macro: 0.3070634989462676, Recall macro: 0.20644742267524885, F1 macro: 0.23889940929966008 \n",
      "Precision micro: 0.837851929092805, Recall micro: 0.5739285714285715, F1 micro: 0.6812208562950403 \n",
      "Model Saved\n",
      "\n",
      "3 epoch\n",
      "Epoch: [4/10], Step: [101/361], Train_loss: 0.06629168134182692\n",
      "Precision macro: 0.3492394480898027, Recall macro: 0.22179094217784695, F1 macro: 0.2588039965868109 \n",
      "Precision micro: 0.833249623304872, Recall micro: 0.5925, F1 micro: 0.6925485284909206 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [201/361], Train_loss: 0.06485318504273892\n",
      "Precision macro: 0.3462230750912139, Recall macro: 0.24015617474623657, F1 macro: 0.26623782756821224 \n",
      "Precision micro: 0.8003605227579991, Recall micro: 0.6342857142857142, F1 micro: 0.7077106993424985 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [301/361], Train_loss: 0.06421152345836162\n",
      "Precision macro: 0.31794633521595944, Recall macro: 0.21844136175437623, F1 macro: 0.2532988159303398 \n",
      "Precision micro: 0.8557291666666667, Recall micro: 0.5867857142857142, F1 micro: 0.6961864406779661 \n",
      "4 epoch\n",
      "Epoch: [5/10], Step: [101/361], Train_loss: 0.060102666020393374\n",
      "Precision macro: 0.4070176861200382, Recall macro: 0.24270818026856134, F1 macro: 0.28334595071318947 \n",
      "Precision micro: 0.8317073170731707, Recall micro: 0.6089285714285714, F1 micro: 0.7030927835051547 \n",
      "Epoch: [5/10], Step: [201/361], Train_loss: 0.05926943069323897\n",
      "Precision macro: 0.4135324773107264, Recall macro: 0.2895450175422235, F1 macro: 0.32576959204145445 \n",
      "Precision micro: 0.8051724137931034, Recall micro: 0.6671428571428571, F1 micro: 0.7296874999999999 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [5/10], Step: [301/361], Train_loss: 0.06048339587946733\n",
      "Precision macro: 0.4034359985267267, Recall macro: 0.270323402247836, F1 macro: 0.30505130652825174 \n",
      "Precision micro: 0.8126410835214447, Recall micro: 0.6428571428571429, F1 micro: 0.7178464606181456 \n",
      "5 epoch\n",
      "Epoch: [6/10], Step: [101/361], Train_loss: 0.060187061801552776\n",
      "Precision macro: 0.43651624782180887, Recall macro: 0.2629348111940314, F1 macro: 0.3098926459443124 \n",
      "Precision micro: 0.8226851851851852, Recall micro: 0.6346428571428572, F1 micro: 0.7165322580645163 \n",
      "Epoch: [6/10], Step: [201/361], Train_loss: 0.05916553379967809\n",
      "Precision macro: 0.4126941476744955, Recall macro: 0.28875025895168543, F1 macro: 0.3283953782019932 \n",
      "Precision micro: 0.8266968325791855, Recall micro: 0.6525, F1 micro: 0.7293413173652693 \n",
      "Epoch: [6/10], Step: [301/361], Train_loss: 0.058904961807032426\n",
      "Precision macro: 0.4221681664130214, Recall macro: 0.2808827728641066, F1 macro: 0.320912943281777 \n",
      "Precision micro: 0.8444551128180509, Recall micro: 0.6282142857142857, F1 micro: 0.7204587343845995 \n",
      "6 epoch\n",
      "Epoch: [7/10], Step: [101/361], Train_loss: 0.05516995422542095\n",
      "Precision macro: 0.4479517735517136, Recall macro: 0.2982639774431381, F1 macro: 0.3440791961949927 \n",
      "Precision micro: 0.8461538461538461, Recall micro: 0.6364285714285715, F1 micro: 0.726457399103139 \n",
      "Epoch: [7/10], Step: [201/361], Train_loss: 0.05530497262254357\n",
      "Precision macro: 0.48861033392508485, Recall macro: 0.32991290047722815, F1 macro: 0.3728456485221045 \n",
      "Precision micro: 0.8097287989668532, Recall micro: 0.6717857142857143, F1 micro: 0.7343353503806364 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [7/10], Step: [301/361], Train_loss: 0.055703533900280794\n",
      "Precision macro: 0.4459257606977166, Recall macro: 0.2969567512728942, F1 macro: 0.3396355991051108 \n",
      "Precision micro: 0.8453510436432637, Recall micro: 0.6364285714285715, F1 micro: 0.726161369193154 \n",
      "7 epoch\n",
      "Epoch: [8/10], Step: [101/361], Train_loss: 0.05569138001650572\n",
      "Precision macro: 0.4503778572875683, Recall macro: 0.30340966614675097, F1 macro: 0.34804913750514305 \n",
      "Precision micro: 0.8425047438330171, Recall micro: 0.6342857142857142, F1 micro: 0.723716381418093 \n",
      "Epoch: [8/10], Step: [201/361], Train_loss: 0.05552267179824412\n",
      "Precision macro: 0.46426841415043046, Recall macro: 0.3031969241703691, F1 macro: 0.34682616872960276 \n",
      "Precision micro: 0.8372420262664165, Recall micro: 0.6375, F1 micro: 0.7238442822384428 \n",
      "Epoch: [8/10], Step: [301/361], Train_loss: 0.055627569252004225\n",
      "Precision macro: 0.4524664831247662, Recall macro: 0.30728966649502715, F1 macro: 0.3496334905751391 \n",
      "Precision micro: 0.8486590038314177, Recall micro: 0.6328571428571429, F1 micro: 0.7250409165302784 \n",
      "8 epoch\n",
      "Epoch: [9/10], Step: [101/361], Train_loss: 0.052995356991887094\n",
      "Precision macro: 0.4715299645030264, Recall macro: 0.3294818635677095, F1 macro: 0.3683402841638571 \n",
      "Precision micro: 0.8296263345195729, Recall micro: 0.6660714285714285, F1 micro: 0.738906497622821 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [9/10], Step: [201/361], Train_loss: 0.05272909205406904\n",
      "Precision macro: 0.4637896776755621, Recall macro: 0.34044027627034923, F1 macro: 0.37779569252020073 \n",
      "Precision micro: 0.8141135972461274, Recall micro: 0.6757142857142857, F1 micro: 0.7384855581576893 \n",
      "Epoch: [9/10], Step: [301/361], Train_loss: 0.05378380390504996\n",
      "Precision macro: 0.4631322923213206, Recall macro: 0.3191371797532066, F1 macro: 0.3618655039152929 \n",
      "Precision micro: 0.8155296229802513, Recall micro: 0.6489285714285714, F1 micro: 0.7227525855210819 \n",
      "9 epoch\n",
      "Epoch: [10/10], Step: [101/361], Train_loss: 0.05339182615280151\n",
      "Precision macro: 0.4592743071930398, Recall macro: 0.3048405479664032, F1 macro: 0.3516314961952886 \n",
      "Precision micro: 0.8420812414422638, Recall micro: 0.6589285714285714, F1 micro: 0.7393307954317772 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [201/361], Train_loss: 0.05300246635451913\n",
      "Precision macro: 0.5239493758717199, Recall macro: 0.31237480248666355, F1 macro: 0.3629296034626441 \n",
      "Precision micro: 0.8358608385370205, Recall micro: 0.6692857142857143, F1 micro: 0.7433558111860373 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [301/361], Train_loss: 0.052334477826952934\n",
      "Precision macro: 0.4913573921795875, Recall macro: 0.3465703832230578, F1 macro: 0.39287590565202724 \n",
      "Precision micro: 0.8040262941659819, Recall micro: 0.6989285714285715, F1 micro: 0.7478028276652656 \n",
      "Model Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAVE_MODEL = False\n",
    "\n",
    "num_epochs = 10\n",
    "    \n",
    "result = {\n",
    "    \"optimizer\": best_params[\"optimizer\"], \n",
    "    \"num_hidden\": best_params[\"num_hidden\"],\n",
    "    \"dim_hidden\": best_params[\"dim_hidden\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"learning_rate\": best_params[\"learning_rate\"],\n",
    "    \"num_epochs\": num_epochs\n",
    "}\n",
    "print(\"\\n\", result)\n",
    "\n",
    "# uncommen if train from scratch\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "optimizer = SWA(base_opt) \n",
    "\n",
    "# train the model\n",
    "model_name = \"ru_\" + \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "print(model_name)\n",
    "metrics_dict = train_model(\n",
    "    wiki_loaders, model, criterion, optimizer, num_epochs=num_epochs, \n",
    "    model_name=model_name, save_model=SAVE_MODEL\n",
    ")\n",
    "result.update(metrics_dict)\n",
    "\n",
    "# results_df = results_df.append(result, ignore_index=True)\n",
    "#     results_df.to_csv(\"results/results_tuning_2_3_layers_maxlen_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.4914, Recall macro: 0.3466, F1 macro: 0.3929 \n",
      "Precision micro: 0.804, Recall micro: 0.6989, F1 micro: 0.7478 \n"
     ]
    }
   ],
   "source": [
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # take only pretrained params of layer_out\n",
    "# pretrained_params = ['layer_out.0.weight', 'layer_out.0.bias', 'layer_out.2.weight', 'layer_out.2.bias']\n",
    "# for param in pretrained_params:\n",
    "#     model.state_dict()[param] = pretrained_state_dict[param]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
