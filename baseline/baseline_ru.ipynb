{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import spacy\n",
    "import string\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import mwparserfromhell\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_EMBEDDINGS_FOLDER = \"/scratch/mz2476/wiki/embeddings/\"\n",
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/\"\n",
    "PATH_TO_MODELS_FOLDER = \"/scratch/mz2476/wiki/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocess import create_lookups_for_vocab, pad_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is: 376365\n"
     ]
    }
   ],
   "source": [
    "# LOAD vocab, tensor dataset, classes\n",
    "vocab = torch.load(PATH_TO_DATA_FOLDER + \"vocab_all_ru.pt\")\n",
    "print(\"Vocab size is:\", len(vocab))\n",
    "index_to_word, word_to_index = create_lookups_for_vocab(vocab)\n",
    "\n",
    "wiki_tensor_dataset = torch.load(PATH_TO_DATA_FOLDER + \"wiki_tensor_dataset_vocab_all_ru.pt\")\n",
    "\n",
    "classes = torch.load(PATH_TO_DATA_FOLDER + \"classes_list.pt\")\n",
    "mlb = MultiLabelBinarizer(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([23026,   838, 23027, 11820, 23028, 23029,   250,   557,  1281,  1137,\n",
       "         23030,   538, 23031, 23032, 23033,  6546,     5,  2135,     4,     4,\n",
       "            21,   107,  1514, 23034, 23035,  2059, 23036, 23037, 23038, 23039,\n",
       "         23026, 23040,   294, 23041,  5217, 23042, 23043,   993,   993, 23043,\n",
       "         19029,  1843,  5288, 23044, 23045, 23046, 17532, 17532, 23047, 23026,\n",
       "            99, 23026,   838, 23027,   106, 23048,     4,     4,    21,   114,\n",
       "           106, 23049, 23048,   106, 23048, 16270,   106,   271,  1996,   105]),\n",
       " tensor([70.]),\n",
       " tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load aligned Russian embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888423it [02:32, 12375.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2.5 million\n",
    "embeddings = utils.load_vectors(PATH_TO_EMBEDDINGS_FOLDER + \"wiki.ru.align.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 376365\n",
      "No. of words from vocab found in embeddings: 320854\n"
     ]
    }
   ],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "import utils\n",
    "\n",
    "weights_matrix_ve = utils.create_embeddings_matrix(word_to_index, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 3 models to compare: frozen, finetuned, trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model_names = {\n",
    "    \"frozen\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_frozen.pth\",\n",
    "    },\n",
    "    \"finetuned\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_init_pretrained.pth\",   \n",
    "    },\n",
    "    \"trained\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\",   \n",
    "    },\n",
    "}\n",
    "\n",
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU(),\n",
    "}\n",
    "\n",
    "for model_name in dict_model_names.keys():\n",
    "    model = FinalModel(options)\n",
    "    # load the state dict from file\n",
    "    file_name = dict_model_names[model_name][\"file_name\"]\n",
    "    model.load_state_dict(torch.load(\n",
    "        f\"{PATH_TO_MODELS_FOLDER}{file_name}\",\n",
    "        map_location=torch.device('cpu')\n",
    "    ))\n",
    "    model.to(device)\n",
    "    # save model to dict\n",
    "    dict_model_names[model_name][\"model\"] = model\n",
    "    # save per class results to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- frozen\n",
      "Precision macro: 0.3503, Recall macro: 0.1435, F1 macro: 0.1715 \n",
      "Precision micro: 0.7678, Recall micro: 0.2693, F1 micro: 0.3987 \n",
      "--- finetuned\n",
      "Precision macro: 0.6015, Recall macro: 0.4704, F1 macro: 0.516 \n",
      "Precision micro: 0.8187, Recall micro: 0.7468, F1 micro: 0.7811 \n",
      "--- trained\n",
      "Precision macro: 0.5225, Recall macro: 0.3148, F1 macro: 0.3643 \n",
      "Precision micro: 0.8348, Recall micro: 0.6714, F1 micro: 0.7443 \n"
     ]
    }
   ],
   "source": [
    "from utils import test_model\n",
    "\n",
    "for model_name in dict_model_names.keys():\n",
    "    model = dict_model_names[model_name][\"model\"]\n",
    "    # print aggregated metrics\n",
    "    metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "    metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "    print(\"---\", model_name)\n",
    "    print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "        metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "    ))\n",
    "    print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "        metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "    ))\n",
    "    \n",
    "    # save per class tables\n",
    "    df_per_class_metrics = utils.create_per_class_tables(\n",
    "        wiki_loaders[\"val\"], model, device, classes, threshold=0.5\n",
    "    )\n",
    "    dict_model_names[model_name][\"df_results\"] = df_per_class_metrics\n",
    "    # SAVE to file\n",
    "    df_per_class_metrics.to_csv(f\"results/ru_per_class_metrics_val_{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>count</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Culture.Arts</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1434</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Culture.Broadcasting</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1418</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Culture.Crafts and hobbies</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1437</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Culture.Entertainment</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1386</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.626506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Culture.Food and drink</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1433</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Culture.Games and toys</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1425</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Culture.Internet culture</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Culture.Language and literature</td>\n",
       "      <td>552.0</td>\n",
       "      <td>848</td>\n",
       "      <td>58</td>\n",
       "      <td>494</td>\n",
       "      <td>43</td>\n",
       "      <td>0.919926</td>\n",
       "      <td>0.894928</td>\n",
       "      <td>0.907254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Culture.Media</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Culture.Music</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.786885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Culture.Performing arts</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1425</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Culture.Philosophy and religion</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1372</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Culture.Plastic arts</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1423</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Culture.Sports</td>\n",
       "      <td>237.0</td>\n",
       "      <td>1189</td>\n",
       "      <td>17</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.928270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Culture.Visual arts</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1412</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Geography.Africa</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1406</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.425532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Geography.Americas</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1219</td>\n",
       "      <td>96</td>\n",
       "      <td>107</td>\n",
       "      <td>21</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.527094</td>\n",
       "      <td>0.646526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Geography.Antarctica</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Geography.Asia</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>84</td>\n",
       "      <td>164</td>\n",
       "      <td>44</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.719298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Geography.Bodies of water</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Geography.Europe</td>\n",
       "      <td>567.0</td>\n",
       "      <td>746</td>\n",
       "      <td>99</td>\n",
       "      <td>468</td>\n",
       "      <td>130</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.803433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Geography.Landforms</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1434</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Geography.Maps</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Geography.Oceania</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1417</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Geography.Parks</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1436</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>History_And_Society.Business and economics</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1410</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>History_And_Society.Education</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1430</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>History_And_Society.History and society</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1278</td>\n",
       "      <td>102</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.382353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>History_And_Society.Military and warfare</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1372</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.720721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>History_And_Society.Politics and government</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1383</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>History_And_Society.Transportation</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1368</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.728814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>STEM.Biology</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1365</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>STEM.Chemistry</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1436</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>STEM.Engineering</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>STEM.Geosciences</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1424</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>STEM.Information science</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>STEM.Mathematics</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1440</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>STEM.Medicine</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1414</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>STEM.Meteorology</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1441</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>STEM.Physics</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>STEM.Science</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1427</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>STEM.Space</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1413</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.983051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>STEM.Technology</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1379</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>STEM.Time</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1437</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class_name  count    TN   FN   TP   FP  \\\n",
       "0                                  Culture.Arts    9.0  1434    8    1    0   \n",
       "1                          Culture.Broadcasting   25.0  1418   22    3    0   \n",
       "2                    Culture.Crafts and hobbies    6.0  1437    6    0    0   \n",
       "3                         Culture.Entertainment   50.0  1386   24   26    7   \n",
       "4                        Culture.Food and drink    9.0  1433    4    5    1   \n",
       "5                        Culture.Games and toys   18.0  1425    5   13    0   \n",
       "6                      Culture.Internet culture    1.0  1442    1    0    0   \n",
       "7               Culture.Language and literature  552.0   848   58  494   43   \n",
       "8                                 Culture.Media    1.0  1442    1    0    0   \n",
       "9                                 Culture.Music   58.0  1369   10   48   16   \n",
       "10                      Culture.Performing arts   18.0  1425   17    1    0   \n",
       "11              Culture.Philosophy and religion   59.0  1372   34   25   12   \n",
       "12                         Culture.Plastic arts   18.0  1423   15    3    2   \n",
       "13                               Culture.Sports  237.0  1189   17  220   17   \n",
       "14                          Culture.Visual arts   30.0  1412   25    5    1   \n",
       "15                             Geography.Africa   31.0  1406   21   10    6   \n",
       "16                           Geography.Americas  203.0  1219   96  107   21   \n",
       "17                         Geography.Antarctica    1.0  1442    1    0    0   \n",
       "18                               Geography.Asia  248.0  1151   84  164   44   \n",
       "19                    Geography.Bodies of water    8.0  1435    8    0    0   \n",
       "20                             Geography.Europe  567.0   746   99  468  130   \n",
       "21                          Geography.Landforms    9.0  1434    9    0    0   \n",
       "22                               Geography.Maps    0.0  1443    0    0    0   \n",
       "23                            Geography.Oceania   25.0  1417   22    3    1   \n",
       "24                              Geography.Parks    7.0  1436    7    0    0   \n",
       "25   History_And_Society.Business and economics   31.0  1410   22    9    2   \n",
       "26                History_And_Society.Education   13.0  1430   13    0    0   \n",
       "27      History_And_Society.History and society  141.0  1278  102   39   24   \n",
       "28     History_And_Society.Military and warfare   66.0  1372   26   40    5   \n",
       "29  History_And_Society.Politics and government   47.0  1383   31   16   13   \n",
       "30           History_And_Society.Transportation   69.0  1368   26   43    6   \n",
       "31                                 STEM.Biology   72.0  1365   20   52    6   \n",
       "32                               STEM.Chemistry    5.0  1436    2    3    2   \n",
       "33                             STEM.Engineering    1.0  1442    1    0    0   \n",
       "34                             STEM.Geosciences   19.0  1424   13    6    0   \n",
       "35                     STEM.Information science    1.0  1442    1    0    0   \n",
       "36                             STEM.Mathematics    3.0  1440    3    0    0   \n",
       "37                                STEM.Medicine   27.0  1414   12   15    2   \n",
       "38                             STEM.Meteorology    2.0  1441    2    0    0   \n",
       "39                                 STEM.Physics    8.0  1435    8    0    0   \n",
       "40                                 STEM.Science   16.0  1427   16    0    0   \n",
       "41                                   STEM.Space   30.0  1413    1   29    0   \n",
       "42                              STEM.Technology   53.0  1379   21   32   11   \n",
       "43                                    STEM.Time    6.0  1437    6    0    0   \n",
       "\n",
       "    precision    recall        f1  \n",
       "0    1.000000  0.111111  0.200000  \n",
       "1    1.000000  0.120000  0.214286  \n",
       "2    0.000000  0.000000  0.000000  \n",
       "3    0.787879  0.520000  0.626506  \n",
       "4    0.833333  0.555556  0.666667  \n",
       "5    1.000000  0.722222  0.838710  \n",
       "6    0.000000  0.000000  0.000000  \n",
       "7    0.919926  0.894928  0.907254  \n",
       "8    0.000000  0.000000  0.000000  \n",
       "9    0.750000  0.827586  0.786885  \n",
       "10   1.000000  0.055556  0.105263  \n",
       "11   0.675676  0.423729  0.520833  \n",
       "12   0.600000  0.166667  0.260870  \n",
       "13   0.928270  0.928270  0.928270  \n",
       "14   0.833333  0.166667  0.277778  \n",
       "15   0.625000  0.322581  0.425532  \n",
       "16   0.835938  0.527094  0.646526  \n",
       "17   0.000000  0.000000  0.000000  \n",
       "18   0.788462  0.661290  0.719298  \n",
       "19   0.000000  0.000000  0.000000  \n",
       "20   0.782609  0.825397  0.803433  \n",
       "21   0.000000  0.000000  0.000000  \n",
       "22   0.000000  0.000000  0.000000  \n",
       "23   0.750000  0.120000  0.206897  \n",
       "24   0.000000  0.000000  0.000000  \n",
       "25   0.818182  0.290323  0.428571  \n",
       "26   0.000000  0.000000  0.000000  \n",
       "27   0.619048  0.276596  0.382353  \n",
       "28   0.888889  0.606061  0.720721  \n",
       "29   0.551724  0.340426  0.421053  \n",
       "30   0.877551  0.623188  0.728814  \n",
       "31   0.896552  0.722222  0.800000  \n",
       "32   0.600000  0.600000  0.600000  \n",
       "33   0.000000  0.000000  0.000000  \n",
       "34   1.000000  0.315789  0.480000  \n",
       "35   0.000000  0.000000  0.000000  \n",
       "36   0.000000  0.000000  0.000000  \n",
       "37   0.882353  0.555556  0.681818  \n",
       "38   0.000000  0.000000  0.000000  \n",
       "39   0.000000  0.000000  0.000000  \n",
       "40   0.000000  0.000000  0.000000  \n",
       "41   1.000000  0.966667  0.983051  \n",
       "42   0.744186  0.603774  0.666667  \n",
       "43   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_model_names[\"trained\"][\"df_results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model. Use pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = PATH_TO_MODELS_FOLDER + \"en_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\"\n",
    "\n",
    "best_params = {\n",
    "    'optimizer': 'SWA',\n",
    "    'num_hidden': 2,\n",
    "    'dim_hidden': 150,\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": best_params[\"num_hidden\"],\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": best_params[\"dim_hidden\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "pretrained_state_dict = torch.load(PRETRAINED_MODEL)\n",
    "\n",
    "# take pretrained params\n",
    "model.layer_out[0].weight.data = pretrained_state_dict['layer_out.0.weight']\n",
    "model.layer_out[0].bias.data = pretrained_state_dict['layer_out.0.bias']\n",
    "model.layer_out[2].weight.data = pretrained_state_dict['layer_out.2.weight']\n",
    "model.layer_out[2].bias.data = pretrained_state_dict['layer_out.2.bias']\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (layer_bag_of_words): BagOfWords(\n",
       "    (embed_e): Embedding(376365, 300)\n",
       "  )\n",
       "  (layer_out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained params:\n",
      "\n",
      "Precision macro: 0.3503, Recall macro: 0.1435, F1 macro: 0.1715 \n",
      "Precision micro: 0.7678, Recall micro: 0.2693, F1 micro: 0.3987 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Using pretrained params:\\n\")\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save frozen model\n",
    "# model_name = \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_frozen\"\n",
    "# torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune on Russian articles OR train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, \n",
    "                num_epochs=10, device=device, model_name=\"model\", save_model=False):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch, \"epoch\")\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 100 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "                ))\n",
    "                print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "                ))\n",
    "\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "                    if save_model:\n",
    "                        optimizer.swap_swa_sgd()\n",
    "                        torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}{model_name}.pth\")\n",
    "                        print('Model Saved')\n",
    "                        print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 150, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10\n",
      "0 epoch\n",
      "Epoch: [1/10], Step: [101/361], Train_loss: 0.16394229903817176\n",
      "Precision macro: 0.03774863222660023, Recall macro: 0.018300674097775547, F1 macro: 0.021573619594354748 \n",
      "Precision micro: 0.7364085667215815, Recall micro: 0.15964285714285714, F1 micro: 0.26240093924273555 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [201/361], Train_loss: 0.13773150239139795\n",
      "Precision macro: 0.10007624693922357, Recall macro: 0.051343324197594804, F1 macro: 0.058254934882816585 \n",
      "Precision micro: 0.8041958041958042, Recall micro: 0.32857142857142857, F1 micro: 0.4665314401622718 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [301/361], Train_loss: 0.12528121824065844\n",
      "Precision macro: 0.11422569054993983, Recall macro: 0.07616071949318902, F1 macro: 0.0814212545866872 \n",
      "Precision micro: 0.7639405204460966, Recall micro: 0.44035714285714284, F1 micro: 0.5586769370185772 \n",
      "Model Saved\n",
      "\n",
      "1 epoch\n",
      "Epoch: [2/10], Step: [101/361], Train_loss: 0.08439337681978941\n",
      "Precision macro: 0.2259704472226207, Recall macro: 0.12202399873145725, F1 macro: 0.13984515352159987 \n",
      "Precision micro: 0.8107951247823564, Recall micro: 0.49892857142857144, F1 micro: 0.6177315940747292 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [201/361], Train_loss: 0.08378258358687163\n",
      "Precision macro: 0.22118828289098233, Recall macro: 0.1281634524974851, F1 macro: 0.1469414566364466 \n",
      "Precision micro: 0.8135977337110482, Recall micro: 0.5128571428571429, F1 micro: 0.6291347207009859 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [301/361], Train_loss: 0.08123884287973245\n",
      "Precision macro: 0.27261289098023156, Recall macro: 0.17644959279690553, F1 macro: 0.20337303885016708 \n",
      "Precision micro: 0.8267246061922868, Recall micro: 0.5435714285714286, F1 micro: 0.6558931264813618 \n",
      "Model Saved\n",
      "\n",
      "2 epoch\n",
      "Epoch: [3/10], Step: [101/361], Train_loss: 0.07191145554184913\n",
      "Precision macro: 0.2895974070788268, Recall macro: 0.17686146914913883, F1 macro: 0.20885121877858814 \n",
      "Precision micro: 0.8395130049806309, Recall micro: 0.5417857142857143, F1 micro: 0.6585630562187974 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [201/361], Train_loss: 0.07064960964024067\n",
      "Precision macro: 0.29619340720331605, Recall macro: 0.20542084782192085, F1 macro: 0.22922444832589467 \n",
      "Precision micro: 0.798941798941799, Recall micro: 0.5932142857142857, F1 micro: 0.6808772289403566 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [301/361], Train_loss: 0.06952822438130776\n",
      "Precision macro: 0.3070634989462676, Recall macro: 0.20644742267524885, F1 macro: 0.23889940929966008 \n",
      "Precision micro: 0.837851929092805, Recall micro: 0.5739285714285715, F1 micro: 0.6812208562950403 \n",
      "Model Saved\n",
      "\n",
      "3 epoch\n",
      "Epoch: [4/10], Step: [101/361], Train_loss: 0.06629168134182692\n",
      "Precision macro: 0.3492394480898027, Recall macro: 0.22179094217784695, F1 macro: 0.2588039965868109 \n",
      "Precision micro: 0.833249623304872, Recall micro: 0.5925, F1 micro: 0.6925485284909206 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [201/361], Train_loss: 0.06485318504273892\n",
      "Precision macro: 0.3462230750912139, Recall macro: 0.24015617474623657, F1 macro: 0.26623782756821224 \n",
      "Precision micro: 0.8003605227579991, Recall micro: 0.6342857142857142, F1 micro: 0.7077106993424985 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [301/361], Train_loss: 0.06421152345836162\n",
      "Precision macro: 0.31794633521595944, Recall macro: 0.21844136175437623, F1 macro: 0.2532988159303398 \n",
      "Precision micro: 0.8557291666666667, Recall micro: 0.5867857142857142, F1 micro: 0.6961864406779661 \n",
      "4 epoch\n",
      "Epoch: [5/10], Step: [101/361], Train_loss: 0.060102666020393374\n",
      "Precision macro: 0.4070176861200382, Recall macro: 0.24270818026856134, F1 macro: 0.28334595071318947 \n",
      "Precision micro: 0.8317073170731707, Recall micro: 0.6089285714285714, F1 micro: 0.7030927835051547 \n",
      "Epoch: [5/10], Step: [201/361], Train_loss: 0.05926943069323897\n",
      "Precision macro: 0.4135324773107264, Recall macro: 0.2895450175422235, F1 macro: 0.32576959204145445 \n",
      "Precision micro: 0.8051724137931034, Recall micro: 0.6671428571428571, F1 micro: 0.7296874999999999 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [5/10], Step: [301/361], Train_loss: 0.06048339587946733\n",
      "Precision macro: 0.4034359985267267, Recall macro: 0.270323402247836, F1 macro: 0.30505130652825174 \n",
      "Precision micro: 0.8126410835214447, Recall micro: 0.6428571428571429, F1 micro: 0.7178464606181456 \n",
      "5 epoch\n",
      "Epoch: [6/10], Step: [101/361], Train_loss: 0.060187061801552776\n",
      "Precision macro: 0.43651624782180887, Recall macro: 0.2629348111940314, F1 macro: 0.3098926459443124 \n",
      "Precision micro: 0.8226851851851852, Recall micro: 0.6346428571428572, F1 micro: 0.7165322580645163 \n",
      "Epoch: [6/10], Step: [201/361], Train_loss: 0.05916553379967809\n",
      "Precision macro: 0.4126941476744955, Recall macro: 0.28875025895168543, F1 macro: 0.3283953782019932 \n",
      "Precision micro: 0.8266968325791855, Recall micro: 0.6525, F1 micro: 0.7293413173652693 \n",
      "Epoch: [6/10], Step: [301/361], Train_loss: 0.058904961807032426\n",
      "Precision macro: 0.4221681664130214, Recall macro: 0.2808827728641066, F1 macro: 0.320912943281777 \n",
      "Precision micro: 0.8444551128180509, Recall micro: 0.6282142857142857, F1 micro: 0.7204587343845995 \n",
      "6 epoch\n",
      "Epoch: [7/10], Step: [101/361], Train_loss: 0.05516995422542095\n",
      "Precision macro: 0.4479517735517136, Recall macro: 0.2982639774431381, F1 macro: 0.3440791961949927 \n",
      "Precision micro: 0.8461538461538461, Recall micro: 0.6364285714285715, F1 micro: 0.726457399103139 \n",
      "Epoch: [7/10], Step: [201/361], Train_loss: 0.05530497262254357\n",
      "Precision macro: 0.48861033392508485, Recall macro: 0.32991290047722815, F1 macro: 0.3728456485221045 \n",
      "Precision micro: 0.8097287989668532, Recall micro: 0.6717857142857143, F1 micro: 0.7343353503806364 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [7/10], Step: [301/361], Train_loss: 0.055703533900280794\n",
      "Precision macro: 0.4459257606977166, Recall macro: 0.2969567512728942, F1 macro: 0.3396355991051108 \n",
      "Precision micro: 0.8453510436432637, Recall micro: 0.6364285714285715, F1 micro: 0.726161369193154 \n",
      "7 epoch\n",
      "Epoch: [8/10], Step: [101/361], Train_loss: 0.05569138001650572\n",
      "Precision macro: 0.4503778572875683, Recall macro: 0.30340966614675097, F1 macro: 0.34804913750514305 \n",
      "Precision micro: 0.8425047438330171, Recall micro: 0.6342857142857142, F1 micro: 0.723716381418093 \n",
      "Epoch: [8/10], Step: [201/361], Train_loss: 0.05552267179824412\n",
      "Precision macro: 0.46426841415043046, Recall macro: 0.3031969241703691, F1 macro: 0.34682616872960276 \n",
      "Precision micro: 0.8372420262664165, Recall micro: 0.6375, F1 micro: 0.7238442822384428 \n",
      "Epoch: [8/10], Step: [301/361], Train_loss: 0.055627569252004225\n",
      "Precision macro: 0.4524664831247662, Recall macro: 0.30728966649502715, F1 macro: 0.3496334905751391 \n",
      "Precision micro: 0.8486590038314177, Recall micro: 0.6328571428571429, F1 micro: 0.7250409165302784 \n",
      "8 epoch\n",
      "Epoch: [9/10], Step: [101/361], Train_loss: 0.052995356991887094\n",
      "Precision macro: 0.4715299645030264, Recall macro: 0.3294818635677095, F1 macro: 0.3683402841638571 \n",
      "Precision micro: 0.8296263345195729, Recall micro: 0.6660714285714285, F1 micro: 0.738906497622821 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [9/10], Step: [201/361], Train_loss: 0.05272909205406904\n",
      "Precision macro: 0.4637896776755621, Recall macro: 0.34044027627034923, F1 macro: 0.37779569252020073 \n",
      "Precision micro: 0.8141135972461274, Recall micro: 0.6757142857142857, F1 micro: 0.7384855581576893 \n",
      "Epoch: [9/10], Step: [301/361], Train_loss: 0.05378380390504996\n",
      "Precision macro: 0.4631322923213206, Recall macro: 0.3191371797532066, F1 macro: 0.3618655039152929 \n",
      "Precision micro: 0.8155296229802513, Recall micro: 0.6489285714285714, F1 micro: 0.7227525855210819 \n",
      "9 epoch\n",
      "Epoch: [10/10], Step: [101/361], Train_loss: 0.05339182615280151\n",
      "Precision macro: 0.4592743071930398, Recall macro: 0.3048405479664032, F1 macro: 0.3516314961952886 \n",
      "Precision micro: 0.8420812414422638, Recall micro: 0.6589285714285714, F1 micro: 0.7393307954317772 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [201/361], Train_loss: 0.05300246635451913\n",
      "Precision macro: 0.5239493758717199, Recall macro: 0.31237480248666355, F1 macro: 0.3629296034626441 \n",
      "Precision micro: 0.8358608385370205, Recall micro: 0.6692857142857143, F1 micro: 0.7433558111860373 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [301/361], Train_loss: 0.052334477826952934\n",
      "Precision macro: 0.4913573921795875, Recall macro: 0.3465703832230578, F1 macro: 0.39287590565202724 \n",
      "Precision micro: 0.8040262941659819, Recall micro: 0.6989285714285715, F1 micro: 0.7478028276652656 \n",
      "Model Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAVE_MODEL = False\n",
    "\n",
    "num_epochs = 10\n",
    "    \n",
    "result = {\n",
    "    \"optimizer\": best_params[\"optimizer\"], \n",
    "    \"num_hidden\": best_params[\"num_hidden\"],\n",
    "    \"dim_hidden\": best_params[\"dim_hidden\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"learning_rate\": best_params[\"learning_rate\"],\n",
    "    \"num_epochs\": num_epochs\n",
    "}\n",
    "print(\"\\n\", result)\n",
    "\n",
    "# uncommen if train from scratch\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "optimizer = SWA(base_opt) \n",
    "\n",
    "# train the model\n",
    "model_name = \"ru_\" + \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "print(model_name)\n",
    "metrics_dict = train_model(\n",
    "    wiki_loaders, model, criterion, optimizer, num_epochs=num_epochs, \n",
    "    model_name=model_name, save_model=SAVE_MODEL\n",
    ")\n",
    "result.update(metrics_dict)\n",
    "\n",
    "# results_df = results_df.append(result, ignore_index=True)\n",
    "#     results_df.to_csv(\"results/results_tuning_2_3_layers_maxlen_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.4914, Recall macro: 0.3466, F1 macro: 0.3929 \n",
      "Precision micro: 0.804, Recall micro: 0.6989, F1 micro: 0.7478 \n"
     ]
    }
   ],
   "source": [
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # take only pretrained params of layer_out\n",
    "# pretrained_params = ['layer_out.0.weight', 'layer_out.0.bias', 'layer_out.2.weight', 'layer_out.2.bias']\n",
    "# for param in pretrained_params:\n",
    "#     model.state_dict()[param] = pretrained_state_dict[param]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
