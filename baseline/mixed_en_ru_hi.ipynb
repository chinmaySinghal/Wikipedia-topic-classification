{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import spacy\n",
    "import string\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import mwparserfromhell\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_EMBEDDINGS_FOLDER = \"/scratch/mz2476/wiki/embeddings/\"\n",
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/aligned_datasets/\"\n",
    "PATH_TO_MODELS_FOLDER = \"/scratch/mz2476/wiki/models/\"\n",
    "\n",
    "PATH_TO_SAVE_FOLDER = \"/scratch/mz2476/wiki/data/aligned_datasets/mix_en_hi_ru/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import preprocess\n",
    "import importlib\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from preprocess import tokenize_dataset, TensoredDataset, pad_collate_fn\n",
    "from preprocess import create_vocab_from_tokens, create_lookups_for_vocab\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES_LIST = [\"english\", \"russian\", \"hindi\"] # \n",
    "LANGUAGES_DICT = defaultdict(dict)\n",
    "\n",
    "test_size = 0.1\n",
    "train_size = 10000\n",
    "val_size = 1000\n",
    "\n",
    "# assuming the data is in PATH_TO_DATA_FOLDER\n",
    "for language in LANGUAGES_LIST:\n",
    "    language_code = language[:2]\n",
    "    LANGUAGES_DICT[language][\"language_code\"] = language_code\n",
    "    FILE_NAMES_DICT = {\n",
    "        \"vocab\": f\"vocab_all_{language_code}.pt\",\n",
    "        \"json\": f\"wikitext_topics_{language_code}_filtered.json\",\n",
    "        \"wiki_df\": f\"wikitext_tokenized_text_sections_outlinks_{language_code}.p\",\n",
    "        \"vocab\": f\"vocab_all_{language_code}.pt\",\n",
    "        \"train\": f\"df_wiki_train_{train_size}_{language_code}.pt\",\n",
    "        \"val\": f\"df_wiki_valid_{val_size}_{language_code}.pt\",\n",
    "        \"test\": f\"df_wiki_test_{test_size}_{language_code}.pt\",\n",
    "        \"not_test\": f\"df_wiki_not_test_{1 - test_size}_{language_code}.pt\",\n",
    "#         \"tensor_dataset\": f\"wiki_tensor_dataset_{language_code}.pt\",\n",
    "    }\n",
    "    # ADD check that these files exist\n",
    "    LANGUAGES_DICT[language][\"FILE_NAMES_DICT\"] = FILE_NAMES_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english vocab size is: 741334\n",
      "russian vocab size is: 858845\n",
      "hindi vocab size is: 441314\n"
     ]
    }
   ],
   "source": [
    "# LOAD vocab, tensor dataset, classes\n",
    "classes = torch.load(PATH_TO_DATA_FOLDER + \"45_classes_list.pt\")\n",
    "mlb = MultiLabelBinarizer(classes)\n",
    "\n",
    "for language, lang_dict in LANGUAGES_DICT.items():\n",
    "    vocab = torch.load(PATH_TO_DATA_FOLDER + lang_dict[\"FILE_NAMES_DICT\"][\"vocab\"])\n",
    "    print(f\"{language} vocab size is:\", len(vocab))\n",
    "#     LANGUAGES_DICT[language][\"vocab\"] = vocab\n",
    "    LANGUAGES_DICT[language][\"index_to_word\"], LANGUAGES_DICT[language][\"word_to_index\"] =\\\n",
    "        create_lookups_for_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order: dict_keys(['english', 'russian', 'hindi'])\n"
     ]
    }
   ],
   "source": [
    "# Create combined vocab, index_to_word, word_to_index\n",
    "# 0 - <pad>, 1 - <unk> \n",
    "vocab = [\"<pad>\", \"<unk>\"]\n",
    "print(\"Order:\", LANGUAGES_DICT.keys())\n",
    "for language, lang_dict in LANGUAGES_DICT.items(): # .keys() keep same order in Python version >= 3.7\n",
    "    assert lang_dict[\"index_to_word\"][0] != \"<pad>\"\n",
    "    vocab += lang_dict[\"index_to_word\"]\n",
    "    \n",
    "index_to_word, word_to_index = create_lookups_for_vocab(vocab)\n",
    "assert len(set(word_to_index)) == len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2041495"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train size: 30000 \n",
      "Combined val size: 3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sections_tokens</th>\n",
       "      <th>raw_outlinks</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q620946</td>\n",
       "      <td>[STEM.Information science, Geography.Americas]</td>\n",
       "      <td>[library, congress, control, number, lccn, ser...</td>\n",
       "      <td>[history, format, see, also, references, exter...</td>\n",
       "      <td>[[[serial number|serially]], [[Library of Cong...</td>\n",
       "      <td>[serial number, Library of Congress, Library o...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q163727</td>\n",
       "      <td>[History_And_Society.Education]</td>\n",
       "      <td>[бакала, вр, уч, ная, степень, академическая, ...</td>\n",
       "      <td>[этимология, диплом, бакалавра, российской, им...</td>\n",
       "      <td>[[[Учёная степень|академическая степень]], [[к...</td>\n",
       "      <td>[Учёная степень, квалификация (образование), В...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q2035301</td>\n",
       "      <td>[Geography.Asia]</td>\n",
       "      <td>[प्राचीन, काल, मध्यकाल, गौड़, प्रदेश, बंगाल, ब...</td>\n",
       "      <td>[स्थिति, विस्तार, गौड़, नगरी]</td>\n",
       "      <td>[[[बंगाल]], [[स्कंदपुराण]], [[भुवनेश्वर]], [[प...</td>\n",
       "      <td>[बंगाल, स्कंदपुराण, भुवनेश्वर, पद्मपुराण, हराह...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q161376</td>\n",
       "      <td>[Culture.Arts]</td>\n",
       "      <td>[ब्रह्माण्ड, सुन्दरी, मिस, यूनिवर्स, मिस, यूनि...</td>\n",
       "      <td>[इतिहास, खिताबधारी, सन्दर्भ]</td>\n",
       "      <td>[[[सौंदर्य प्रतियोगिता]], [[न्यूयॉर्क शहर]], [...</td>\n",
       "      <td>[सौंदर्य प्रतियोगिता, न्यूयॉर्क शहर, संयुक्त र...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q19860</td>\n",
       "      <td>[Culture.Language and literature, Geography.Eu...</td>\n",
       "      <td>[индоевропе, йские, языки, самая, распростран,...</td>\n",
       "      <td>[название, происхождение, история, гипотеза, д...</td>\n",
       "      <td>[[[Файл:Satem and kentum languages map in Eura...</td>\n",
       "      <td>[Файл:Satem and kentum languages map in Eurasi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                               mid_level_categories  \\\n",
       "0   Q620946     [STEM.Information science, Geography.Americas]   \n",
       "1   Q163727                    [History_And_Society.Education]   \n",
       "2  Q2035301                                   [Geography.Asia]   \n",
       "3   Q161376                                     [Culture.Arts]   \n",
       "4    Q19860  [Culture.Language and literature, Geography.Eu...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [library, congress, control, number, lccn, ser...   \n",
       "1  [бакала, вр, уч, ная, степень, академическая, ...   \n",
       "2  [प्राचीन, काल, मध्यकाल, गौड़, प्रदेश, बंगाल, ब...   \n",
       "3  [ब्रह्माण्ड, सुन्दरी, मिस, यूनिवर्स, मिस, यूनि...   \n",
       "4  [индоевропе, йские, языки, самая, распростран,...   \n",
       "\n",
       "                                     sections_tokens  \\\n",
       "0  [history, format, see, also, references, exter...   \n",
       "1  [этимология, диплом, бакалавра, российской, им...   \n",
       "2                      [स्थिति, विस्तार, गौड़, नगरी]   \n",
       "3                       [इतिहास, खिताबधारी, सन्दर्भ]   \n",
       "4  [название, происхождение, история, гипотеза, д...   \n",
       "\n",
       "                                        raw_outlinks  \\\n",
       "0  [[[serial number|serially]], [[Library of Cong...   \n",
       "1  [[[Учёная степень|академическая степень]], [[к...   \n",
       "2  [[[बंगाल]], [[स्कंदपुराण]], [[भुवनेश्वर]], [[प...   \n",
       "3  [[[सौंदर्य प्रतियोगिता]], [[न्यूयॉर्क शहर]], [...   \n",
       "4  [[[Файл:Satem and kentum languages map in Eura...   \n",
       "\n",
       "                                            outlinks  \\\n",
       "0  [serial number, Library of Congress, Library o...   \n",
       "1  [Учёная степень, квалификация (образование), В...   \n",
       "2  [बंगाल, स्कंदपुराण, भुवनेश्वर, पद्मपुराण, हराह...   \n",
       "3  [सौंदर्य प्रतियोगिता, न्यूयॉर्क शहर, संयुक्त र...   \n",
       "4  [Файл:Satem and kentum languages map in Eurasi...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "SEED = 57\n",
    "\n",
    "wiki_train, wiki_valid = [], []\n",
    "\n",
    "dict_of_dfs = defaultdict()\n",
    "\n",
    "for language, lang_dict in LANGUAGES_DICT.items():\n",
    "    language_code = lang_dict[\"language_code\"]\n",
    "    dict_of_dfs[f\"train_{language_code}\"], dict_of_dfs[f\"val_{language_code}\"] =\\\n",
    "            (torch.load(PATH_TO_DATA_FOLDER + lang_dict[\"FILE_NAMES_DICT\"][\"train\"]),\n",
    "             torch.load(PATH_TO_DATA_FOLDER + lang_dict[\"FILE_NAMES_DICT\"][\"val\"]))\n",
    "    wiki_train.append(dict_of_dfs[f\"train_{language_code}\"])\n",
    "    wiki_valid.append(dict_of_dfs[f\"val_{language_code}\"])\n",
    "\n",
    "wiki_train = pd.concat(wiki_train).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "wiki_valid = pd.concat(wiki_valid).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "dict_of_dfs[\"train\"] = wiki_train\n",
    "dict_of_dfs[\"val\"] = wiki_valid\n",
    "\n",
    "print(f\"Combined train size: {wiki_train.shape[0]} \\nCombined val size: {wiki_valid.shape[0]}\")\n",
    "wiki_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sections_tokens</th>\n",
       "      <th>raw_outlinks</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30263</td>\n",
       "      <td>Q100</td>\n",
       "      <td>[Geography.Americas]</td>\n",
       "      <td>[imagesize, three, zero, zeropx, image, flag, ...</td>\n",
       "      <td>[history, colonial, revolution, siege, boston,...</td>\n",
       "      <td>[[[City]], [[Financial District, Boston|Financ...</td>\n",
       "      <td>[City, Financial District, Boston, Fenway Park...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23482</td>\n",
       "      <td>Q1000219</td>\n",
       "      <td>[Culture.People, Geography.Asia]</td>\n",
       "      <td>[kapoor, family, prominent, indian, people, in...</td>\n",
       "      <td>[background, members, kapoor, family, previous...</td>\n",
       "      <td>[[[Randhir Kapoor|Randhir's]], [[Samundri]], [...</td>\n",
       "      <td>[Randhir Kapoor, Samundri, Samundri Tehsil, Fa...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26236</td>\n",
       "      <td>Q1000373</td>\n",
       "      <td>[Geography.Asia]</td>\n",
       "      <td>[mandalgovi, also, mandalgov, mandalgobi, capi...</td>\n",
       "      <td>[climate, transportation, references, external...</td>\n",
       "      <td>[[[Districts of Mongolia|District]], [[Countri...</td>\n",
       "      <td>[Districts of Mongolia, Countries of the world...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21925</td>\n",
       "      <td>Q1000485</td>\n",
       "      <td>[Geography.Asia]</td>\n",
       "      <td>[raebareli, southeast, lucknow, possesses, man...</td>\n",
       "      <td>[history, etymology, post, independence, geogr...</td>\n",
       "      <td>[[[States and territories of India|State]], [[...</td>\n",
       "      <td>[States and territories of India, List of dist...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17801</td>\n",
       "      <td>Q1000495</td>\n",
       "      <td>[Geography.Asia]</td>\n",
       "      <td>[rewa, south, city, allahabad, climate, rewa, ...</td>\n",
       "      <td>[climate, demographics, history, governance, t...</td>\n",
       "      <td>[[[India]], [[Madhya Pradesh]], [[List of dist...</td>\n",
       "      <td>[India, Madhya Pradesh, List of districts of I...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            QID              mid_level_categories  \\\n",
       "30263      Q100              [Geography.Americas]   \n",
       "23482  Q1000219  [Culture.People, Geography.Asia]   \n",
       "26236  Q1000373                  [Geography.Asia]   \n",
       "21925  Q1000485                  [Geography.Asia]   \n",
       "17801  Q1000495                  [Geography.Asia]   \n",
       "\n",
       "                                                  tokens  \\\n",
       "30263  [imagesize, three, zero, zeropx, image, flag, ...   \n",
       "23482  [kapoor, family, prominent, indian, people, in...   \n",
       "26236  [mandalgovi, also, mandalgov, mandalgobi, capi...   \n",
       "21925  [raebareli, southeast, lucknow, possesses, man...   \n",
       "17801  [rewa, south, city, allahabad, climate, rewa, ...   \n",
       "\n",
       "                                         sections_tokens  \\\n",
       "30263  [history, colonial, revolution, siege, boston,...   \n",
       "23482  [background, members, kapoor, family, previous...   \n",
       "26236  [climate, transportation, references, external...   \n",
       "21925  [history, etymology, post, independence, geogr...   \n",
       "17801  [climate, demographics, history, governance, t...   \n",
       "\n",
       "                                            raw_outlinks  \\\n",
       "30263  [[[City]], [[Financial District, Boston|Financ...   \n",
       "23482  [[[Randhir Kapoor|Randhir's]], [[Samundri]], [...   \n",
       "26236  [[[Districts of Mongolia|District]], [[Countri...   \n",
       "21925  [[[States and territories of India|State]], [[...   \n",
       "17801  [[[India]], [[Madhya Pradesh]], [[List of dist...   \n",
       "\n",
       "                                                outlinks  \\\n",
       "30263  [City, Financial District, Boston, Fenway Park...   \n",
       "23482  [Randhir Kapoor, Samundri, Samundri Tehsil, Fa...   \n",
       "26236  [Districts of Mongolia, Countries of the world...   \n",
       "21925  [States and territories of India, List of dist...   \n",
       "17801  [India, Madhya Pradesh, List of districts of I...   \n",
       "\n",
       "                                                  labels  \n",
       "30263  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "23482  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "26236  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "21925  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "17801  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict_of_dfs[\"train_en\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import create_dict_of_tensor_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2546.14it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2538.38it/s]\n",
      "100%|██████████| 10000/10000 [00:03<00:00, 3199.19it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2685.47it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 9616.85it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 8715.55it/s]\n",
      "100%|██████████| 30000/30000 [00:08<00:00, 3478.76it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 3182.94it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_tensor_dataset = create_dict_of_tensor_datasets(dict_of_dfs, word_to_index, max_num_tokens=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextData(tokens=tensor([1010031, 1448131, 1063534,  ..., 1063535, 1235361, 1374937]), len=tensor([1146.]), target=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_en', 'val_en', 'train_ru', 'val_ru', 'train_hi', 'val_hi', 'train', 'val'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_loaders.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "# SAVE = False\n",
    "# if SAVE:\n",
    "#     # SAVE tensor datasets\n",
    "#     torch.save(wiki_tensor_dataset, f'{PATH_TO_DATA_FOLDER}wiki_tensor_dataset_mixed_en_ru.pt')\n",
    "#     print(\"Saved.\")\n",
    "    \n",
    "# wiki_tensor_dataset = torch.load(f'{PATH_TO_DATA_FOLDER}wiki_tensor_dataset_mixed_en_ru.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load aligned en and ru embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = True\n",
    "LOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2519370it [02:56, 14287.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 741334\n",
      "No. of words from vocab found in embeddings: 554751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "665it [00:00, 6648.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888423it [02:16, 13850.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 858845\n",
      "No. of words from vocab found in embeddings: 607894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "502it [00:00, 5018.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158016it [00:11, 14150.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 441314\n",
      "No. of words from vocab found in embeddings: 104088\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "for language, lang_dict in LANGUAGES_DICT.items():\n",
    "    if LOAD:\n",
    "        embed_info_dict = torch.load(f'{PATH_TO_SAVE_FOLDER}embeddings_matrix_with_idx_to_word_{language_code}.pt')\n",
    "        LANGUAGES_DICT[language][\"weights_matrix_ve\"] = embed_info_dict[\"weights_matrix_ve\"]\n",
    "    if SAVE:\n",
    "        language_code = lang_dict[\"language_code\"]\n",
    "        # 2.5 million\n",
    "        embeddings = utils.load_vectors(PATH_TO_EMBEDDINGS_FOLDER + f\"wiki.{language_code}.align.vec\")\n",
    "        #Creating the weight matrix for pretrained word embeddings\n",
    "        weights_matrix_ve = utils.create_embeddings_matrix(lang_dict[\"index_to_word\"], embeddings)\n",
    "        LANGUAGES_DICT[language][\"weights_matrix_ve\"] = weights_matrix_ve\n",
    "        # SAVE embeddings matrix together with index_to_word\n",
    "        torch.save({\n",
    "            \"index_to_word\" : lang_dict[\"index_to_word\"],\n",
    "            \"weights_matrix_ve\" : weights_matrix_ve,\n",
    "        }, f'{PATH_TO_SAVE_FOLDER}embeddings_matrix_with_idx_to_word_{language_code}.pt')\n",
    "        print(\"Saved.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LANGUAGES_DICT[\"english\"][\"weights_matrix_ve\"].shape[0] + LANGUAGES_DICT[\"russian\"][\"weights_matrix_ve\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGUAGES_DICT[\"english\"][\"index_to_word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2041495"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings matrix shape: torch.Size([2041495, 300]), \n",
      "Vocab size: 2041495\n"
     ]
    }
   ],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "# 0 - <pad>, 1 - <unk> \n",
    "weights_matrix_ve = torch.zeros(len(index_to_word), LANGUAGES_DICT[\"english\"][\"weights_matrix_ve\"].shape[1])\n",
    "start_idx = 2\n",
    "for language, lang_dict in LANGUAGES_DICT.items():\n",
    "    end_idx = start_idx + len(lang_dict[\"index_to_word\"])\n",
    "    assert index_to_word[start_idx:end_idx] == lang_dict[\"index_to_word\"]\n",
    "    weights_matrix_ve[start_idx:end_idx] = lang_dict[\"weights_matrix_ve\"]\n",
    "    start_idx = end_idx\n",
    "#     weights_matrix_ve += lang_dict[\"weights_matrix_ve\"]\n",
    "\n",
    "print(f\"Embeddings matrix shape: {weights_matrix_ve.shape}, \\nVocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE = False\n",
    "# if SAVE:\n",
    "#     # SAVE embeddings matrix\n",
    "#     torch.save(weights_matrix_ve, f'{PATH_TO_SA}embedding_weights_matrix_mixed_en_ru.pt')\n",
    "#     print(\"Saved.\")\n",
    "    \n",
    "# weights_matrix_ve = torch.load(f'{PATH_TO_DATA_FOLDER}embedding_weights_matrix_mixed_en_ru.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model, evaluate on mix, en, ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model, print_results\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, options,\n",
    "                num_epochs=10, device=device, model_name=\"model\", save_model=False):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    plot_cache = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch, \"epoch\")\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train_ru\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 100 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print_results(metrics_dict)\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "                    if save_model:\n",
    "                        optimizer.swap_swa_sgd()\n",
    "#                         torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}{model_name}.pth\")\n",
    "                        torch.save({\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'options': options,\n",
    "                            'plot_cache': plot_cache,\n",
    "                        },\n",
    "                            f'{PATH_TO_MODELS_FOLDER}{model_name}.pth')\n",
    "     \n",
    "                        print('Model Saved')\n",
    "                        print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 150, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 15}\n",
      "mixed_en_hi_ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_15\n",
      "0 epoch\n",
      "Epoch: [1/15], Step: [101/938], Train_loss: 0.18508880265057087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mz2476/miniconda3/envs/my_base/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.0807, Recall macro: 0.086, F1 macro: 0.0767 \n",
      "Precision micro: 0.3037, Recall micro: 0.2314, F1 micro: 0.2627 \n",
      "Epoch: [1/15], Step: [201/938], Train_loss: 0.15673805095255375\n",
      "Precision macro: 0.1308, Recall macro: 0.1525, F1 macro: 0.1168 \n",
      "Precision micro: 0.2733, Recall micro: 0.3322, F1 micro: 0.2999 \n",
      "Epoch: [1/15], Step: [301/938], Train_loss: 0.14187694244086743\n",
      "Precision macro: 0.1882, Recall macro: 0.1853, F1 macro: 0.154 \n",
      "Precision micro: 0.3346, Recall micro: 0.377, F1 micro: 0.3545 \n",
      "1 epoch\n",
      "Epoch: [2/15], Step: [101/938], Train_loss: 0.10024497203528882\n",
      "Precision macro: 0.2281, Recall macro: 0.2167, F1 macro: 0.185 \n",
      "Precision micro: 0.3705, Recall micro: 0.3985, F1 micro: 0.384 \n",
      "Epoch: [2/15], Step: [201/938], Train_loss: 0.09584924245253205\n",
      "Precision macro: 0.2394, Recall macro: 0.2457, F1 macro: 0.2133 \n",
      "Precision micro: 0.4025, Recall micro: 0.4405, F1 micro: 0.4206 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-127db7cdf4e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m metrics_dict = train_model(\n\u001b[1;32m     44\u001b[0m     \u001b[0mwiki_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAVE_MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-7da72b53dbc7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(wiki_loaders, model, criterion, optimizer, options, num_epochs, device, model_name, save_model)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrunnin_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_ru\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/mz2476/miniconda3/envs/my_base/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/mz2476/miniconda3/envs/my_base/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/topic-modeling/topic-modeling/baseline/preprocess.py\u001b[0m in \u001b[0;36mpad_collate_fn\u001b[0;34m(batch, word_to_index)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     text_data = TextData(\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_list_of_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mlen\u001b[0m   \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/topic-modeling/topic-modeling/baseline/preprocess.py\u001b[0m in \u001b[0;36mpad_list_of_tensors\u001b[0;34m(list_of_tensors, pad_token)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m#print(torch.tensor([[pad_token]*(max_length - t.size(-1))])[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         padded_tensor = torch.cat([t.reshape(1, -1),\n\u001b[0;32m--> 132\u001b[0;31m                                    torch.LongTensor([[pad_token]*(max_length - t.size(-1))])], dim = -1)\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mpadded_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mpadded_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SAVE_MODEL = False\n",
    "\n",
    "lr = 0.01\n",
    "num_epochs = 15\n",
    "\n",
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU(),\n",
    "}\n",
    "    \n",
    "result = {\n",
    "    \"optimizer\": \"SWA\", \n",
    "    \"num_hidden\": options[\"num_layers\"],\n",
    "    \"dim_hidden\": options[\"mid_features\"],\n",
    "    \"dropout_rate\": options[\"dropout_rate\"],\n",
    "    \"learning_rate\": lr,\n",
    "    \"num_epochs\": num_epochs\n",
    "}\n",
    "\n",
    "print(\"\\n\", result)\n",
    "\n",
    "# uncommen if train from scratch\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = SWA(base_opt) \n",
    "\n",
    "# train the model\n",
    "model_name = \"mixed_en_hi_ru_\" + \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "print(model_name)\n",
    "metrics_dict = train_model(\n",
    "    wiki_loaders, model, criterion, optimizer, options=options, num_epochs=num_epochs, \n",
    "    model_name=model_name, save_model=SAVE_MODEL\n",
    ")\n",
    "result.update(metrics_dict)\n",
    "\n",
    "# results_df = results_df.append(result, ignore_index=True)\n",
    "#     results_df.to_csv(\"results/results_tuning_2_3_layers_maxlen_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.5497, Recall macro: 0.332, F1 macro: 0.3914 \n",
      "Precision micro: 0.8659, Recall micro: 0.6087, F1 micro: 0.7149 \n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_loaders[\"val_ru\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print_results(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.7587, Recall macro: 0.5095, F1 macro: 0.5842 \n",
      "Precision micro: 0.8372, Recall micro: 0.6806, F1 micro: 0.7508 \n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_loaders[\"val\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.749, Recall macro: 0.5404, F1 macro: 0.6103 \n",
      "Precision micro: 0.8413, Recall micro: 0.7104, F1 micro: 0.7703 \n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_loaders[\"val_en\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.7055, Recall macro: 0.4951, F1 macro: 0.5599 \n",
      "Precision micro: 0.8311, Recall micro: 0.6838, F1 micro: 0.7503 \n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_loaders[\"val_ru\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.7107, Recall macro: 0.4928, F1 macro: 0.5669 \n",
      "Precision micro: 0.8391, Recall micro: 0.6475, F1 micro: 0.731 \n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_loaders[\"val_hi\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model\n",
    "# torch.save({\n",
    "#         'state_dict': model.state_dict(),\n",
    "#         'opts': options,\n",
    "#         'plot_cache': plot_cache,\n",
    "#             }, \n",
    "#     f'{PATH_TO_MODELS_FOLDER}en_ru_mixed_model_train_10000.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model_names = {\n",
    "    \"frozen\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_frozen.pth\",\n",
    "    },\n",
    "    \"finetuned\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_init_pretrained.pth\",   \n",
    "    },\n",
    "    \"trained\": {\n",
    "        \"file_name\": \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\",   \n",
    "    },\n",
    "}\n",
    "\n",
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU(),\n",
    "}\n",
    "\n",
    "for model_name in dict_model_names.keys():\n",
    "    model = FinalModel(options)\n",
    "    # load the state dict from file\n",
    "    file_name = dict_model_names[model_name][\"file_name\"]\n",
    "    model.load_state_dict(torch.load(\n",
    "        f\"{PATH_TO_MODELS_FOLDER}{file_name}\",\n",
    "        map_location=torch.device('cpu')\n",
    "    ))\n",
    "    model.to(device)\n",
    "    # save model to dict\n",
    "    dict_model_names[model_name][\"model\"] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- frozen\n",
      "Precision macro: 0.3503, Recall macro: 0.1435, F1 macro: 0.1715 \n",
      "Precision micro: 0.7678, Recall micro: 0.2693, F1 micro: 0.3987 \n",
      "--- finetuned\n",
      "Precision macro: 0.6015, Recall macro: 0.4704, F1 macro: 0.516 \n",
      "Precision micro: 0.8187, Recall micro: 0.7468, F1 micro: 0.7811 \n",
      "--- trained\n",
      "Precision macro: 0.5225, Recall macro: 0.3148, F1 macro: 0.3643 \n",
      "Precision micro: 0.8348, Recall micro: 0.6714, F1 micro: 0.7443 \n"
     ]
    }
   ],
   "source": [
    "from utils import test_model\n",
    "\n",
    "for model_name in dict_model_names.keys():\n",
    "    model = dict_model_names[model_name][\"model\"]\n",
    "    # print aggregated metrics\n",
    "    metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "    metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "    print(\"---\", model_name)\n",
    "    print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "        metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "    ))\n",
    "    print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "        metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "    ))\n",
    "    \n",
    "    # save per class tables\n",
    "    df_per_class_metrics = utils.create_per_class_tables(\n",
    "        wiki_loaders[\"val\"], model, device, classes, threshold=0.5\n",
    "    )\n",
    "    dict_model_names[model_name][\"df_results\"] = df_per_class_metrics\n",
    "    # SAVE to file\n",
    "#     df_per_class_metrics.to_csv(f\"results/ru_per_class_metrics_val_{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>count</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Culture.Arts</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1434</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Culture.Broadcasting</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1418</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Culture.Crafts and hobbies</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1437</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Culture.Entertainment</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1386</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.626506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Culture.Food and drink</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1433</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Culture.Games and toys</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1425</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Culture.Internet culture</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Culture.Language and literature</td>\n",
       "      <td>552.0</td>\n",
       "      <td>848</td>\n",
       "      <td>58</td>\n",
       "      <td>494</td>\n",
       "      <td>43</td>\n",
       "      <td>0.919926</td>\n",
       "      <td>0.894928</td>\n",
       "      <td>0.907254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Culture.Media</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Culture.Music</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.786885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Culture.Performing arts</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1425</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Culture.Philosophy and religion</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1372</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Culture.Plastic arts</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1423</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Culture.Sports</td>\n",
       "      <td>237.0</td>\n",
       "      <td>1189</td>\n",
       "      <td>17</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.928270</td>\n",
       "      <td>0.928270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Culture.Visual arts</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1412</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Geography.Africa</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1406</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.425532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Geography.Americas</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1219</td>\n",
       "      <td>96</td>\n",
       "      <td>107</td>\n",
       "      <td>21</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.527094</td>\n",
       "      <td>0.646526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Geography.Antarctica</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Geography.Asia</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>84</td>\n",
       "      <td>164</td>\n",
       "      <td>44</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.719298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Geography.Bodies of water</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Geography.Europe</td>\n",
       "      <td>567.0</td>\n",
       "      <td>746</td>\n",
       "      <td>99</td>\n",
       "      <td>468</td>\n",
       "      <td>130</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.803433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Geography.Landforms</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1434</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Geography.Maps</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Geography.Oceania</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1417</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Geography.Parks</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1436</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>History_And_Society.Business and economics</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1410</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>History_And_Society.Education</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1430</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>History_And_Society.History and society</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1278</td>\n",
       "      <td>102</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.382353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>History_And_Society.Military and warfare</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1372</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.720721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>History_And_Society.Politics and government</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1383</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>History_And_Society.Transportation</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1368</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.728814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>STEM.Biology</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1365</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>STEM.Chemistry</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1436</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>STEM.Engineering</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>STEM.Geosciences</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1424</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>STEM.Information science</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>STEM.Mathematics</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1440</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>STEM.Medicine</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1414</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>STEM.Meteorology</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1441</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>STEM.Physics</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>STEM.Science</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1427</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>STEM.Space</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1413</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.983051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>STEM.Technology</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1379</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>STEM.Time</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1437</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class_name  count    TN   FN   TP   FP  \\\n",
       "0                                  Culture.Arts    9.0  1434    8    1    0   \n",
       "1                          Culture.Broadcasting   25.0  1418   22    3    0   \n",
       "2                    Culture.Crafts and hobbies    6.0  1437    6    0    0   \n",
       "3                         Culture.Entertainment   50.0  1386   24   26    7   \n",
       "4                        Culture.Food and drink    9.0  1433    4    5    1   \n",
       "5                        Culture.Games and toys   18.0  1425    5   13    0   \n",
       "6                      Culture.Internet culture    1.0  1442    1    0    0   \n",
       "7               Culture.Language and literature  552.0   848   58  494   43   \n",
       "8                                 Culture.Media    1.0  1442    1    0    0   \n",
       "9                                 Culture.Music   58.0  1369   10   48   16   \n",
       "10                      Culture.Performing arts   18.0  1425   17    1    0   \n",
       "11              Culture.Philosophy and religion   59.0  1372   34   25   12   \n",
       "12                         Culture.Plastic arts   18.0  1423   15    3    2   \n",
       "13                               Culture.Sports  237.0  1189   17  220   17   \n",
       "14                          Culture.Visual arts   30.0  1412   25    5    1   \n",
       "15                             Geography.Africa   31.0  1406   21   10    6   \n",
       "16                           Geography.Americas  203.0  1219   96  107   21   \n",
       "17                         Geography.Antarctica    1.0  1442    1    0    0   \n",
       "18                               Geography.Asia  248.0  1151   84  164   44   \n",
       "19                    Geography.Bodies of water    8.0  1435    8    0    0   \n",
       "20                             Geography.Europe  567.0   746   99  468  130   \n",
       "21                          Geography.Landforms    9.0  1434    9    0    0   \n",
       "22                               Geography.Maps    0.0  1443    0    0    0   \n",
       "23                            Geography.Oceania   25.0  1417   22    3    1   \n",
       "24                              Geography.Parks    7.0  1436    7    0    0   \n",
       "25   History_And_Society.Business and economics   31.0  1410   22    9    2   \n",
       "26                History_And_Society.Education   13.0  1430   13    0    0   \n",
       "27      History_And_Society.History and society  141.0  1278  102   39   24   \n",
       "28     History_And_Society.Military and warfare   66.0  1372   26   40    5   \n",
       "29  History_And_Society.Politics and government   47.0  1383   31   16   13   \n",
       "30           History_And_Society.Transportation   69.0  1368   26   43    6   \n",
       "31                                 STEM.Biology   72.0  1365   20   52    6   \n",
       "32                               STEM.Chemistry    5.0  1436    2    3    2   \n",
       "33                             STEM.Engineering    1.0  1442    1    0    0   \n",
       "34                             STEM.Geosciences   19.0  1424   13    6    0   \n",
       "35                     STEM.Information science    1.0  1442    1    0    0   \n",
       "36                             STEM.Mathematics    3.0  1440    3    0    0   \n",
       "37                                STEM.Medicine   27.0  1414   12   15    2   \n",
       "38                             STEM.Meteorology    2.0  1441    2    0    0   \n",
       "39                                 STEM.Physics    8.0  1435    8    0    0   \n",
       "40                                 STEM.Science   16.0  1427   16    0    0   \n",
       "41                                   STEM.Space   30.0  1413    1   29    0   \n",
       "42                              STEM.Technology   53.0  1379   21   32   11   \n",
       "43                                    STEM.Time    6.0  1437    6    0    0   \n",
       "\n",
       "    precision    recall        f1  \n",
       "0    1.000000  0.111111  0.200000  \n",
       "1    1.000000  0.120000  0.214286  \n",
       "2    0.000000  0.000000  0.000000  \n",
       "3    0.787879  0.520000  0.626506  \n",
       "4    0.833333  0.555556  0.666667  \n",
       "5    1.000000  0.722222  0.838710  \n",
       "6    0.000000  0.000000  0.000000  \n",
       "7    0.919926  0.894928  0.907254  \n",
       "8    0.000000  0.000000  0.000000  \n",
       "9    0.750000  0.827586  0.786885  \n",
       "10   1.000000  0.055556  0.105263  \n",
       "11   0.675676  0.423729  0.520833  \n",
       "12   0.600000  0.166667  0.260870  \n",
       "13   0.928270  0.928270  0.928270  \n",
       "14   0.833333  0.166667  0.277778  \n",
       "15   0.625000  0.322581  0.425532  \n",
       "16   0.835938  0.527094  0.646526  \n",
       "17   0.000000  0.000000  0.000000  \n",
       "18   0.788462  0.661290  0.719298  \n",
       "19   0.000000  0.000000  0.000000  \n",
       "20   0.782609  0.825397  0.803433  \n",
       "21   0.000000  0.000000  0.000000  \n",
       "22   0.000000  0.000000  0.000000  \n",
       "23   0.750000  0.120000  0.206897  \n",
       "24   0.000000  0.000000  0.000000  \n",
       "25   0.818182  0.290323  0.428571  \n",
       "26   0.000000  0.000000  0.000000  \n",
       "27   0.619048  0.276596  0.382353  \n",
       "28   0.888889  0.606061  0.720721  \n",
       "29   0.551724  0.340426  0.421053  \n",
       "30   0.877551  0.623188  0.728814  \n",
       "31   0.896552  0.722222  0.800000  \n",
       "32   0.600000  0.600000  0.600000  \n",
       "33   0.000000  0.000000  0.000000  \n",
       "34   1.000000  0.315789  0.480000  \n",
       "35   0.000000  0.000000  0.000000  \n",
       "36   0.000000  0.000000  0.000000  \n",
       "37   0.882353  0.555556  0.681818  \n",
       "38   0.000000  0.000000  0.000000  \n",
       "39   0.000000  0.000000  0.000000  \n",
       "40   0.000000  0.000000  0.000000  \n",
       "41   1.000000  0.966667  0.983051  \n",
       "42   0.744186  0.603774  0.666667  \n",
       "43   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_model_names[\"trained\"][\"df_results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model. Use pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PATH_TO_MODELS_FOLDER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1314704cba1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPRETRAINED_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH_TO_MODELS_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"en_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m best_params = {\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'SWA'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'num_hidden'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PATH_TO_MODELS_FOLDER' is not defined"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL = PATH_TO_MODELS_FOLDER + \"en_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\"\n",
    "\n",
    "best_params = {\n",
    "    'optimizer': 'SWA',\n",
    "    'num_hidden': 2,\n",
    "    'dim_hidden': 150,\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": best_params[\"num_hidden\"],\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": best_params[\"dim_hidden\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "pretrained_state_dict = torch.load(PRETRAINED_MODEL)\n",
    "\n",
    "# take pretrained params\n",
    "model.layer_out[0].weight.data = pretrained_state_dict['layer_out.0.weight']\n",
    "model.layer_out[0].bias.data = pretrained_state_dict['layer_out.0.bias']\n",
    "model.layer_out[2].weight.data = pretrained_state_dict['layer_out.2.weight']\n",
    "model.layer_out[2].bias.data = pretrained_state_dict['layer_out.2.bias']\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (layer_bag_of_words): BagOfWords(\n",
       "    (embed_e): Embedding(376365, 300)\n",
       "  )\n",
       "  (layer_out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained params:\n",
      "\n",
      "Precision macro: 0.3503, Recall macro: 0.1435, F1 macro: 0.1715 \n",
      "Precision micro: 0.7678, Recall micro: 0.2693, F1 micro: 0.3987 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Using pretrained params:\\n\")\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save frozen model\n",
    "# model_name = \"ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10_frozen\"\n",
    "# torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune on Russian articles OR train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, \n",
    "                num_epochs=10, device=device, model_name=\"model\", save_model=False):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch, \"epoch\")\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 100 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "                ))\n",
    "                print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "                ))\n",
    "\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "                    if save_model:\n",
    "                        optimizer.swap_swa_sgd()\n",
    "                        torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}{model_name}.pth\")\n",
    "                        print('Model Saved')\n",
    "                        print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 150, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "ru_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10\n",
      "0 epoch\n",
      "Epoch: [1/10], Step: [101/361], Train_loss: 0.16394229903817176\n",
      "Precision macro: 0.03774863222660023, Recall macro: 0.018300674097775547, F1 macro: 0.021573619594354748 \n",
      "Precision micro: 0.7364085667215815, Recall micro: 0.15964285714285714, F1 micro: 0.26240093924273555 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [201/361], Train_loss: 0.13773150239139795\n",
      "Precision macro: 0.10007624693922357, Recall macro: 0.051343324197594804, F1 macro: 0.058254934882816585 \n",
      "Precision micro: 0.8041958041958042, Recall micro: 0.32857142857142857, F1 micro: 0.4665314401622718 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [301/361], Train_loss: 0.12528121824065844\n",
      "Precision macro: 0.11422569054993983, Recall macro: 0.07616071949318902, F1 macro: 0.0814212545866872 \n",
      "Precision micro: 0.7639405204460966, Recall micro: 0.44035714285714284, F1 micro: 0.5586769370185772 \n",
      "Model Saved\n",
      "\n",
      "1 epoch\n",
      "Epoch: [2/10], Step: [101/361], Train_loss: 0.08439337681978941\n",
      "Precision macro: 0.2259704472226207, Recall macro: 0.12202399873145725, F1 macro: 0.13984515352159987 \n",
      "Precision micro: 0.8107951247823564, Recall micro: 0.49892857142857144, F1 micro: 0.6177315940747292 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [201/361], Train_loss: 0.08378258358687163\n",
      "Precision macro: 0.22118828289098233, Recall macro: 0.1281634524974851, F1 macro: 0.1469414566364466 \n",
      "Precision micro: 0.8135977337110482, Recall micro: 0.5128571428571429, F1 micro: 0.6291347207009859 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [301/361], Train_loss: 0.08123884287973245\n",
      "Precision macro: 0.27261289098023156, Recall macro: 0.17644959279690553, F1 macro: 0.20337303885016708 \n",
      "Precision micro: 0.8267246061922868, Recall micro: 0.5435714285714286, F1 micro: 0.6558931264813618 \n",
      "Model Saved\n",
      "\n",
      "2 epoch\n",
      "Epoch: [3/10], Step: [101/361], Train_loss: 0.07191145554184913\n",
      "Precision macro: 0.2895974070788268, Recall macro: 0.17686146914913883, F1 macro: 0.20885121877858814 \n",
      "Precision micro: 0.8395130049806309, Recall micro: 0.5417857142857143, F1 micro: 0.6585630562187974 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [201/361], Train_loss: 0.07064960964024067\n",
      "Precision macro: 0.29619340720331605, Recall macro: 0.20542084782192085, F1 macro: 0.22922444832589467 \n",
      "Precision micro: 0.798941798941799, Recall micro: 0.5932142857142857, F1 micro: 0.6808772289403566 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [301/361], Train_loss: 0.06952822438130776\n",
      "Precision macro: 0.3070634989462676, Recall macro: 0.20644742267524885, F1 macro: 0.23889940929966008 \n",
      "Precision micro: 0.837851929092805, Recall micro: 0.5739285714285715, F1 micro: 0.6812208562950403 \n",
      "Model Saved\n",
      "\n",
      "3 epoch\n",
      "Epoch: [4/10], Step: [101/361], Train_loss: 0.06629168134182692\n",
      "Precision macro: 0.3492394480898027, Recall macro: 0.22179094217784695, F1 macro: 0.2588039965868109 \n",
      "Precision micro: 0.833249623304872, Recall micro: 0.5925, F1 micro: 0.6925485284909206 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [201/361], Train_loss: 0.06485318504273892\n",
      "Precision macro: 0.3462230750912139, Recall macro: 0.24015617474623657, F1 macro: 0.26623782756821224 \n",
      "Precision micro: 0.8003605227579991, Recall micro: 0.6342857142857142, F1 micro: 0.7077106993424985 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [301/361], Train_loss: 0.06421152345836162\n",
      "Precision macro: 0.31794633521595944, Recall macro: 0.21844136175437623, F1 macro: 0.2532988159303398 \n",
      "Precision micro: 0.8557291666666667, Recall micro: 0.5867857142857142, F1 micro: 0.6961864406779661 \n",
      "4 epoch\n",
      "Epoch: [5/10], Step: [101/361], Train_loss: 0.060102666020393374\n",
      "Precision macro: 0.4070176861200382, Recall macro: 0.24270818026856134, F1 macro: 0.28334595071318947 \n",
      "Precision micro: 0.8317073170731707, Recall micro: 0.6089285714285714, F1 micro: 0.7030927835051547 \n",
      "Epoch: [5/10], Step: [201/361], Train_loss: 0.05926943069323897\n",
      "Precision macro: 0.4135324773107264, Recall macro: 0.2895450175422235, F1 macro: 0.32576959204145445 \n",
      "Precision micro: 0.8051724137931034, Recall micro: 0.6671428571428571, F1 micro: 0.7296874999999999 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [5/10], Step: [301/361], Train_loss: 0.06048339587946733\n",
      "Precision macro: 0.4034359985267267, Recall macro: 0.270323402247836, F1 macro: 0.30505130652825174 \n",
      "Precision micro: 0.8126410835214447, Recall micro: 0.6428571428571429, F1 micro: 0.7178464606181456 \n",
      "5 epoch\n",
      "Epoch: [6/10], Step: [101/361], Train_loss: 0.060187061801552776\n",
      "Precision macro: 0.43651624782180887, Recall macro: 0.2629348111940314, F1 macro: 0.3098926459443124 \n",
      "Precision micro: 0.8226851851851852, Recall micro: 0.6346428571428572, F1 micro: 0.7165322580645163 \n",
      "Epoch: [6/10], Step: [201/361], Train_loss: 0.05916553379967809\n",
      "Precision macro: 0.4126941476744955, Recall macro: 0.28875025895168543, F1 macro: 0.3283953782019932 \n",
      "Precision micro: 0.8266968325791855, Recall micro: 0.6525, F1 micro: 0.7293413173652693 \n",
      "Epoch: [6/10], Step: [301/361], Train_loss: 0.058904961807032426\n",
      "Precision macro: 0.4221681664130214, Recall macro: 0.2808827728641066, F1 macro: 0.320912943281777 \n",
      "Precision micro: 0.8444551128180509, Recall micro: 0.6282142857142857, F1 micro: 0.7204587343845995 \n",
      "6 epoch\n",
      "Epoch: [7/10], Step: [101/361], Train_loss: 0.05516995422542095\n",
      "Precision macro: 0.4479517735517136, Recall macro: 0.2982639774431381, F1 macro: 0.3440791961949927 \n",
      "Precision micro: 0.8461538461538461, Recall micro: 0.6364285714285715, F1 micro: 0.726457399103139 \n",
      "Epoch: [7/10], Step: [201/361], Train_loss: 0.05530497262254357\n",
      "Precision macro: 0.48861033392508485, Recall macro: 0.32991290047722815, F1 macro: 0.3728456485221045 \n",
      "Precision micro: 0.8097287989668532, Recall micro: 0.6717857142857143, F1 micro: 0.7343353503806364 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [7/10], Step: [301/361], Train_loss: 0.055703533900280794\n",
      "Precision macro: 0.4459257606977166, Recall macro: 0.2969567512728942, F1 macro: 0.3396355991051108 \n",
      "Precision micro: 0.8453510436432637, Recall micro: 0.6364285714285715, F1 micro: 0.726161369193154 \n",
      "7 epoch\n",
      "Epoch: [8/10], Step: [101/361], Train_loss: 0.05569138001650572\n",
      "Precision macro: 0.4503778572875683, Recall macro: 0.30340966614675097, F1 macro: 0.34804913750514305 \n",
      "Precision micro: 0.8425047438330171, Recall micro: 0.6342857142857142, F1 micro: 0.723716381418093 \n",
      "Epoch: [8/10], Step: [201/361], Train_loss: 0.05552267179824412\n",
      "Precision macro: 0.46426841415043046, Recall macro: 0.3031969241703691, F1 macro: 0.34682616872960276 \n",
      "Precision micro: 0.8372420262664165, Recall micro: 0.6375, F1 micro: 0.7238442822384428 \n",
      "Epoch: [8/10], Step: [301/361], Train_loss: 0.055627569252004225\n",
      "Precision macro: 0.4524664831247662, Recall macro: 0.30728966649502715, F1 macro: 0.3496334905751391 \n",
      "Precision micro: 0.8486590038314177, Recall micro: 0.6328571428571429, F1 micro: 0.7250409165302784 \n",
      "8 epoch\n",
      "Epoch: [9/10], Step: [101/361], Train_loss: 0.052995356991887094\n",
      "Precision macro: 0.4715299645030264, Recall macro: 0.3294818635677095, F1 macro: 0.3683402841638571 \n",
      "Precision micro: 0.8296263345195729, Recall micro: 0.6660714285714285, F1 micro: 0.738906497622821 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [9/10], Step: [201/361], Train_loss: 0.05272909205406904\n",
      "Precision macro: 0.4637896776755621, Recall macro: 0.34044027627034923, F1 macro: 0.37779569252020073 \n",
      "Precision micro: 0.8141135972461274, Recall micro: 0.6757142857142857, F1 micro: 0.7384855581576893 \n",
      "Epoch: [9/10], Step: [301/361], Train_loss: 0.05378380390504996\n",
      "Precision macro: 0.4631322923213206, Recall macro: 0.3191371797532066, F1 macro: 0.3618655039152929 \n",
      "Precision micro: 0.8155296229802513, Recall micro: 0.6489285714285714, F1 micro: 0.7227525855210819 \n",
      "9 epoch\n",
      "Epoch: [10/10], Step: [101/361], Train_loss: 0.05339182615280151\n",
      "Precision macro: 0.4592743071930398, Recall macro: 0.3048405479664032, F1 macro: 0.3516314961952886 \n",
      "Precision micro: 0.8420812414422638, Recall micro: 0.6589285714285714, F1 micro: 0.7393307954317772 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [201/361], Train_loss: 0.05300246635451913\n",
      "Precision macro: 0.5239493758717199, Recall macro: 0.31237480248666355, F1 macro: 0.3629296034626441 \n",
      "Precision micro: 0.8358608385370205, Recall micro: 0.6692857142857143, F1 micro: 0.7433558111860373 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [301/361], Train_loss: 0.052334477826952934\n",
      "Precision macro: 0.4913573921795875, Recall macro: 0.3465703832230578, F1 macro: 0.39287590565202724 \n",
      "Precision micro: 0.8040262941659819, Recall micro: 0.6989285714285715, F1 micro: 0.7478028276652656 \n",
      "Model Saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAVE_MODEL = False\n",
    "\n",
    "num_epochs = 10\n",
    "    \n",
    "result = {\n",
    "    \"optimizer\": best_params[\"optimizer\"], \n",
    "    \"num_hidden\": best_params[\"num_hidden\"],\n",
    "    \"dim_hidden\": best_params[\"dim_hidden\"],\n",
    "    \"dropout_rate\": best_params[\"dropout_rate\"],\n",
    "    \"learning_rate\": best_params[\"learning_rate\"],\n",
    "    \"num_epochs\": num_epochs\n",
    "}\n",
    "print(\"\\n\", result)\n",
    "\n",
    "# uncommen if train from scratch\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "optimizer = SWA(base_opt) \n",
    "\n",
    "# train the model\n",
    "model_name = \"ru_\" + \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "print(model_name)\n",
    "metrics_dict = train_model(\n",
    "    wiki_loaders, model, criterion, optimizer, num_epochs=num_epochs, \n",
    "    model_name=model_name, save_model=SAVE_MODEL\n",
    ")\n",
    "result.update(metrics_dict)\n",
    "\n",
    "# results_df = results_df.append(result, ignore_index=True)\n",
    "#     results_df.to_csv(\"results/results_tuning_2_3_layers_maxlen_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.4914, Recall macro: 0.3466, F1 macro: 0.3929 \n",
      "Precision micro: 0.804, Recall micro: 0.6989, F1 micro: 0.7478 \n"
     ]
    }
   ],
   "source": [
    "metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "))\n",
    "print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # take only pretrained params of layer_out\n",
    "# pretrained_params = ['layer_out.0.weight', 'layer_out.0.bias', 'layer_out.2.weight', 'layer_out.2.bias']\n",
    "# for param in pretrained_params:\n",
    "#     model.state_dict()[param] = pretrained_state_dict[param]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
