{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import nltk\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_EMBEDDINGS_FOLDER = \"/scratch/mz2476/wiki/embeddings/\"\n",
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/\"\n",
    "PATH_TO_MODELS_FOLDER = \"/scratch/mz2476/wiki/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocess import create_lookups_for_vocab, pad_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is: 682850\n"
     ]
    }
   ],
   "source": [
    "# LOAD vocab, tensor dataset, classes\n",
    "vocab = torch.load(PATH_TO_DATA_FOLDER + \"vocab_all_en.pt\")\n",
    "print(\"Vocab size is:\", len(vocab))\n",
    "index_to_word, word_to_index = create_lookups_for_vocab(vocab)\n",
    "\n",
    "wiki_tensor_dataset = torch.load(PATH_TO_DATA_FOLDER + \"wiki_tensor_dataset_vocab_all_en.pt\")\n",
    "\n",
    "classes = torch.load(PATH_TO_DATA_FOLDER + \"classes_list.pt\")\n",
    "mlb = MultiLabelBinarizer(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([13030,  8330,  3721,  8330,  3721,   132,  2496, 13031,  4719,  3982,\n",
       "         13031,  3178,   303,  5510, 13032,  8334,  2496, 13031,  4719,  1828,\n",
       "          2496,  1985, 13033, 10701, 13034,     7,  5299,  2338,  6948,     5,\n",
       "             9,     9,     8, 10510,   480, 13035, 13036, 11814, 13035, 13036,\n",
       "           965,   933,  2789,     5,   223,    10,   933, 13037,  6777,  1646,\n",
       "          3271, 13038,  2496, 13031,  4719,  1036, 13039,  1985,  2300,  1495,\n",
       "           601, 13040,  1495,     5,     9,   208,     6,     5,     9,     9,\n",
       "            11,   568,     5,     9,     9,   208, 13041,  1467,   403, 13042,\n",
       "          9309,  1065, 13043, 13044, 13043, 13044,  2300,  2189,  1880,  8330,\n",
       "          4719,   452,    10,     8,     8,     8, 13035, 13036,    21, 13045,\n",
       "          2300, 13045,  2641,  3721,  4340,  4251, 13043, 13044, 13046,  2496,\n",
       "         13031,  4719,  4340, 13045, 13047, 13048, 13049, 13050,  5496,  9571,\n",
       "           648,     5,     9,    10,     8,     5,    11, 13051,  6945,  9127,\n",
       "          2496,    53,  5458, 13051,  6945,  9127,  6945,  9127,  2025,   833,\n",
       "          6777,  3721, 13052,  3271, 13053, 13054,  3721, 11814, 13035, 13036,\n",
       "           689,  1261,    10,     8,     5,   253, 13055,  2496, 13031,  4719,\n",
       "         12197, 13052,  3271, 13053,  3721,  5251,   952,  3078,   167,  6674,\n",
       "            10,  8340,   439,  1366, 13056,   952,  1199,    10,     8,     5,\n",
       "             6,  3541,  1816,   794,    10,     8,     8,   611,   532,    10,\n",
       "             8,    10,     8,  1108, 13057,  2471,  5251,   952,   439,  8340,\n",
       "          1140,  2496, 13031,  4719,   132, 10436,  3078,  3271, 13053,   132,\n",
       "            58, 13058,   414, 13031,   132,   277,  2496,  2076, 13031,  1869,\n",
       "           525,    24, 13031,  4719,   526,  8330,  8340,  2496,    81,   397,\n",
       "           398, 13059,  3435,  2869,  2496, 13031,  8330,  8340,  2055,     5,\n",
       "             9,     9,     8,  8330,  8340,  2496, 13035, 13036]),\n",
       " tensor([248.]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings and make a pretrained embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2519370it [03:08, 13337.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Aligned fasstext. 2.5 million\n",
    "embeddings = utils.load_vectors(PATH_TO_EMBEDDINGS_FOLDER + \"wiki.en.align.vec\")\n",
    "\n",
    "# # CHANGE to googlenews vectors\n",
    "# import gensim\n",
    " \n",
    "# model = gensim.models.KeyedVectors.load(\"/scratch/mz2476/GoogleNews-vectors-negative300.bin\", binary=True)  \n",
    " \n",
    "# embeddings = model.vocab.keys()\n",
    "# wordsInVocab = len(embeddings)\n",
    "# print (wordsInVocab)\n",
    "\n",
    "# # embeddings = load_vectors(\"/scratch/mz2476/GoogleNews-vectors-negative300.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 682850\n",
      "No. of words from vocab found in embeddings: 528314\n"
     ]
    }
   ],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "weights_matrix_ve = utils.create_embeddings_matrix(word_to_index, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(mlb.classes_),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "    \n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = SWA(base_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (layer_bag_of_words): BagOfWords(\n",
       "    (embed_e): Embedding(595366, 300)\n",
       "  )\n",
       "  (layer_out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# from utils import test_model\n",
    "\n",
    "# best_val_f1_micro = 0\n",
    "# num_epochs = 20\n",
    "# for epoch in range(num_epochs):\n",
    "#     runnin_loss = 0.0\n",
    "#     for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "#         model.train()\n",
    "#         data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(data_batch, length_batch)\n",
    "#         loss = criterion(outputs, label_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         runnin_loss += loss.item()\n",
    "#         #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "#         if i>0 and i % 300 == 0:\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "#                 epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "#         # validate every 300 iterations\n",
    "#         if i > 0 and i % 300 == 0:\n",
    "#             metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "#             print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "#                 metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "#             ))\n",
    "#             print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "#                 metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "#             ))\n",
    "            \n",
    "#             if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "#                 best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "#                 optimizer.swap_swa_sgd()\n",
    "#                 torch.save(model.state_dict(), 'baseline.pth')\n",
    "#                 print('Model Saved')\n",
    "#                 print()\n",
    "# optimizer.swap_swa_sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (layer_bag_of_words): BagOfWords(\n",
       "    (embed_e): Embedding(595366, 300)\n",
       "  )\n",
       "  (layer_out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(mlb.classes_),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    \"../../baseline_models_params/en_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\",\n",
    "#     map_location=torch.device('cpu')\n",
    "))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_macro': 0.019017772983219675,\n",
       " 'recall_macro': 0.0034896592022761463,\n",
       " 'f1_macro': 0.0049014079184453605,\n",
       " 'precision_micro': 0.19329896907216496,\n",
       " 'recall_micro': 0.017530532343832176,\n",
       " 'f1_micro': 0.0321457272970801}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.test_model(wiki_loaders[\"val\"], model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>count</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Culture.Arts</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9977</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Culture.Broadcasting</td>\n",
       "      <td>217.0</td>\n",
       "      <td>9778</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Culture.Crafts and hobbies</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9982</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Culture.Entertainment</td>\n",
       "      <td>295.0</td>\n",
       "      <td>9676</td>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Culture.Food and drink</td>\n",
       "      <td>67.0</td>\n",
       "      <td>9929</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Culture.Games and toys</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9886</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Culture.Internet culture</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9990</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Culture.Language and literature</td>\n",
       "      <td>3631.0</td>\n",
       "      <td>6323</td>\n",
       "      <td>3621</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.005430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Culture.Media</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9993</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Culture.Music</td>\n",
       "      <td>435.0</td>\n",
       "      <td>9561</td>\n",
       "      <td>435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Culture.Performing arts</td>\n",
       "      <td>72.0</td>\n",
       "      <td>9923</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Culture.Philosophy and religion</td>\n",
       "      <td>274.0</td>\n",
       "      <td>9721</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Culture.Plastic arts</td>\n",
       "      <td>302.0</td>\n",
       "      <td>9694</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Culture.Sports</td>\n",
       "      <td>1567.0</td>\n",
       "      <td>8426</td>\n",
       "      <td>1567</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Culture.Visual arts</td>\n",
       "      <td>139.0</td>\n",
       "      <td>9856</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Geography.Africa</td>\n",
       "      <td>225.0</td>\n",
       "      <td>9766</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Geography.Americas</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>8098</td>\n",
       "      <td>1881</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.003156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Geography.Antarctica</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9969</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Geography.Asia</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>8546</td>\n",
       "      <td>1392</td>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.005004</td>\n",
       "      <td>0.009609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Geography.Bodies of water</td>\n",
       "      <td>81.0</td>\n",
       "      <td>9915</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Geography.Europe</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>6953</td>\n",
       "      <td>1906</td>\n",
       "      <td>262</td>\n",
       "      <td>875</td>\n",
       "      <td>0.230431</td>\n",
       "      <td>0.120849</td>\n",
       "      <td>0.158548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Geography.Landforms</td>\n",
       "      <td>57.0</td>\n",
       "      <td>9939</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Geography.Maps</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9995</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Geography.Oceania</td>\n",
       "      <td>468.0</td>\n",
       "      <td>9464</td>\n",
       "      <td>468</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Geography.Parks</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9935</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>History_And_Society.Business and economics</td>\n",
       "      <td>207.0</td>\n",
       "      <td>9782</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>History_And_Society.Education</td>\n",
       "      <td>181.0</td>\n",
       "      <td>9814</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>History_And_Society.History and society</td>\n",
       "      <td>449.0</td>\n",
       "      <td>9543</td>\n",
       "      <td>449</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>History_And_Society.Military and warfare</td>\n",
       "      <td>331.0</td>\n",
       "      <td>9665</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>History_And_Society.Politics and government</td>\n",
       "      <td>343.0</td>\n",
       "      <td>9652</td>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>History_And_Society.Transportation</td>\n",
       "      <td>551.0</td>\n",
       "      <td>9445</td>\n",
       "      <td>551</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>STEM.Biology</td>\n",
       "      <td>771.0</td>\n",
       "      <td>9089</td>\n",
       "      <td>753</td>\n",
       "      <td>18</td>\n",
       "      <td>136</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.023346</td>\n",
       "      <td>0.038919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>STEM.Chemistry</td>\n",
       "      <td>36.0</td>\n",
       "      <td>9960</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>STEM.Engineering</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9974</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>STEM.Geosciences</td>\n",
       "      <td>90.0</td>\n",
       "      <td>9902</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>STEM.Information science</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9986</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>STEM.Mathematics</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9985</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>STEM.Medicine</td>\n",
       "      <td>154.0</td>\n",
       "      <td>9837</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>STEM.Meteorology</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9991</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>STEM.Physics</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>STEM.Science</td>\n",
       "      <td>43.0</td>\n",
       "      <td>9953</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>STEM.Space</td>\n",
       "      <td>67.0</td>\n",
       "      <td>9929</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>STEM.Technology</td>\n",
       "      <td>275.0</td>\n",
       "      <td>9713</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>STEM.Time</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9966</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class_name   count    TN    FN   TP   FP  \\\n",
       "0                                  Culture.Arts    19.0  9977    19    0    0   \n",
       "1                          Culture.Broadcasting   217.0  9778   217    0    1   \n",
       "2                    Culture.Crafts and hobbies    14.0  9982    14    0    0   \n",
       "3                         Culture.Entertainment   295.0  9676   295    0   25   \n",
       "4                        Culture.Food and drink    67.0  9929    67    0    0   \n",
       "5                        Culture.Games and toys   109.0  9886   109    0    1   \n",
       "6                      Culture.Internet culture     6.0  9990     6    0    0   \n",
       "7               Culture.Language and literature  3631.0  6323  3621   10   42   \n",
       "8                                 Culture.Media     3.0  9993     3    0    0   \n",
       "9                                 Culture.Music   435.0  9561   435    0    0   \n",
       "10                      Culture.Performing arts    72.0  9923    72    0    1   \n",
       "11              Culture.Philosophy and religion   274.0  9721   274    0    1   \n",
       "12                         Culture.Plastic arts   302.0  9694   302    0    0   \n",
       "13                               Culture.Sports  1567.0  8426  1567    0    3   \n",
       "14                          Culture.Visual arts   139.0  9856   139    0    1   \n",
       "15                             Geography.Africa   225.0  9766   225    0    5   \n",
       "16                           Geography.Americas  1884.0  8098  1881    3   14   \n",
       "17                         Geography.Antarctica    27.0  9969    27    0    0   \n",
       "18                               Geography.Asia  1399.0  8546  1392    7   51   \n",
       "19                    Geography.Bodies of water    81.0  9915    81    0    0   \n",
       "20                             Geography.Europe  2168.0  6953  1906  262  875   \n",
       "21                          Geography.Landforms    57.0  9939    57    0    0   \n",
       "22                               Geography.Maps     1.0  9995     1    0    0   \n",
       "23                            Geography.Oceania   468.0  9464   468    0   64   \n",
       "24                              Geography.Parks    60.0  9935    60    0    1   \n",
       "25   History_And_Society.Business and economics   207.0  9782   207    0    7   \n",
       "26                History_And_Society.Education   181.0  9814   181    0    1   \n",
       "27      History_And_Society.History and society   449.0  9543   449    0    4   \n",
       "28     History_And_Society.Military and warfare   331.0  9665   331    0    0   \n",
       "29  History_And_Society.Politics and government   343.0  9652   343    0    1   \n",
       "30           History_And_Society.Transportation   551.0  9445   551    0    0   \n",
       "31                                 STEM.Biology   771.0  9089   753   18  136   \n",
       "32                               STEM.Chemistry    36.0  9960    36    0    0   \n",
       "33                             STEM.Engineering    22.0  9974    22    0    0   \n",
       "34                             STEM.Geosciences    90.0  9902    90    0    4   \n",
       "35                     STEM.Information science    10.0  9986    10    0    0   \n",
       "36                             STEM.Mathematics    11.0  9985    11    0    0   \n",
       "37                                STEM.Medicine   154.0  9837   154    0    5   \n",
       "38                             STEM.Meteorology     5.0  9991     5    0    0   \n",
       "39                                 STEM.Physics    18.0  9978    18    0    0   \n",
       "40                                 STEM.Science    43.0  9953    43    0    0   \n",
       "41                                   STEM.Space    67.0  9929    67    0    0   \n",
       "42                              STEM.Technology   275.0  9713   275    0    8   \n",
       "43                                    STEM.Time    29.0  9966    29    0    1   \n",
       "\n",
       "    precision    recall        f1  \n",
       "0    0.000000  0.000000  0.000000  \n",
       "1    0.000000  0.000000  0.000000  \n",
       "2    0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.000000  \n",
       "4    0.000000  0.000000  0.000000  \n",
       "5    0.000000  0.000000  0.000000  \n",
       "6    0.000000  0.000000  0.000000  \n",
       "7    0.192308  0.002754  0.005430  \n",
       "8    0.000000  0.000000  0.000000  \n",
       "9    0.000000  0.000000  0.000000  \n",
       "10   0.000000  0.000000  0.000000  \n",
       "11   0.000000  0.000000  0.000000  \n",
       "12   0.000000  0.000000  0.000000  \n",
       "13   0.000000  0.000000  0.000000  \n",
       "14   0.000000  0.000000  0.000000  \n",
       "15   0.000000  0.000000  0.000000  \n",
       "16   0.176471  0.001592  0.003156  \n",
       "17   0.000000  0.000000  0.000000  \n",
       "18   0.120690  0.005004  0.009609  \n",
       "19   0.000000  0.000000  0.000000  \n",
       "20   0.230431  0.120849  0.158548  \n",
       "21   0.000000  0.000000  0.000000  \n",
       "22   0.000000  0.000000  0.000000  \n",
       "23   0.000000  0.000000  0.000000  \n",
       "24   0.000000  0.000000  0.000000  \n",
       "25   0.000000  0.000000  0.000000  \n",
       "26   0.000000  0.000000  0.000000  \n",
       "27   0.000000  0.000000  0.000000  \n",
       "28   0.000000  0.000000  0.000000  \n",
       "29   0.000000  0.000000  0.000000  \n",
       "30   0.000000  0.000000  0.000000  \n",
       "31   0.116883  0.023346  0.038919  \n",
       "32   0.000000  0.000000  0.000000  \n",
       "33   0.000000  0.000000  0.000000  \n",
       "34   0.000000  0.000000  0.000000  \n",
       "35   0.000000  0.000000  0.000000  \n",
       "36   0.000000  0.000000  0.000000  \n",
       "37   0.000000  0.000000  0.000000  \n",
       "38   0.000000  0.000000  0.000000  \n",
       "39   0.000000  0.000000  0.000000  \n",
       "40   0.000000  0.000000  0.000000  \n",
       "41   0.000000  0.000000  0.000000  \n",
       "42   0.000000  0.000000  0.000000  \n",
       "43   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.create_per_class_tables(wiki_loaders[\"val\"], model, device, class_names=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = torch.zeros_like(wiki_loaders[\"val\"].dataset[0][-1])\n",
    "# for idx in range(len(wiki_loaders[\"val\"].dataset)):\n",
    "#     counts += wiki_loaders[\"val\"].dataset[idx][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_per_class = pd.DataFrame(per_class_metrics_dict)\n",
    "# df_per_class[\"class_name\"] = mlb.classes_\n",
    "# # change columns order\n",
    "# df_per_class = df_per_class[[df_per_class.columns[-1]] + list(df_per_class.columns[:-1])]\n",
    "# df_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, multilabel_confusion_matrix\n",
    "\n",
    "from utils import create_per_class_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search vs. Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> dropout </li>\n",
    "    <li> learning rate </li>\n",
    "    <li> optimizer </li>\n",
    "    <li> num of hidden layers </li>\n",
    "    <li> dim of hidden layers </li>\n",
    "    <li> take only first 500 words from the article </li>\n",
    "    <li> TODO threshold </li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I focused on SWA optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one layer\n",
    "# range_dropout = [0]\n",
    "# range_num_hidden = [1]\n",
    "# range_dim_hidden = [80, 120, 150]\n",
    "# range_lr = [0.01]\n",
    "\n",
    "# many layers\n",
    "range_dropout = [0, 0.1, 0.2]\n",
    "range_num_hidden = [2, 3]\n",
    "range_dim_hidden = [40, 80, 120]\n",
    "range_lr = [0.001, 0.01]\n",
    "\n",
    "# # best hyperparams\n",
    "# range_dropout = [0.2]\n",
    "# range_num_hidden = [2]\n",
    "# range_dim_hidden = [120, 150, 200]\n",
    "# range_lr = [0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, num_epochs=10, device=device, model_name=\"model\"):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 1000 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "                ))\n",
    "                print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "                ))\n",
    "\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "                    optimizer.swap_swa_sgd()\n",
    "                    torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}en_{model_name}.pth\")\n",
    "                    print('Model Saved')\n",
    "                    print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_without_best = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 40, 'dropout_rate': 0, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.1613335421010852\n",
      "Precision macro: 0.07763095820863863, Recall macro: 0.022133927127800327, F1 macro: 0.02803677062124604 \n",
      "Precision micro: 0.6717482173592328, Recall micro: 0.15964471454449833, F1 micro: 0.2579792256846081 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.13147279946506024\n",
      "Precision macro: 0.11809104702657391, Recall macro: 0.061221524474661226, F1 macro: 0.0676405812609879 \n",
      "Precision micro: 0.7536959954069183, Recall micro: 0.3068427511248758, F1 micro: 0.43612956810631226 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.08414565202966333\n",
      "Precision macro: 0.15466359242020145, Recall macro: 0.09444943925158747, F1 macro: 0.10662958689069836 \n",
      "Precision micro: 0.7951121879279469, Recall micro: 0.4410681937708175, F1 micro: 0.5673908141020823 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.0829362342171371\n",
      "Precision macro: 0.2003969834251638, Recall macro: 0.10030206781573629, F1 macro: 0.11315574429679455 \n",
      "Precision micro: 0.8110995850622407, Recall micro: 0.45690410798807923, F1 micro: 0.5845325757858931 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.07102981609851122\n",
      "Precision macro: 0.30059340191407036, Recall macro: 0.13502718795778146, F1 macro: 0.15685563121917334 \n",
      "Precision micro: 0.8170898170898171, Recall micro: 0.524688833050897, F1 micro: 0.6390292505871469 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.07137939257733524\n",
      "Precision macro: 0.27866896659111257, Recall macro: 0.12831827379083338, F1 macro: 0.14980718275922222 \n",
      "Precision micro: 0.8232227488151659, Recall micro: 0.5075089113539415, F1 micro: 0.6279145428912265 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.06365261610969901\n",
      "Precision macro: 0.36272392782588775, Recall macro: 0.1668015884493493, F1 macro: 0.19817612170677223 \n",
      "Precision micro: 0.829625797551302, Recall micro: 0.5622626073745106, F1 micro: 0.6702657518024451 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.0641889440510422\n",
      "Precision macro: 0.36393171020288195, Recall macro: 0.16590300871165223, F1 macro: 0.19672826605874197 \n",
      "Precision micro: 0.8224643899090441, Recall micro: 0.560100508385438, F1 micro: 0.6663885702367296 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.05933181545883417\n",
      "Precision macro: 0.432218646984138, Recall macro: 0.1983965216228907, F1 macro: 0.23738769779207586 \n",
      "Precision micro: 0.8369366372858806, Recall micro: 0.5881493601355694, F1 micro: 0.6908267270668177 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.059824428640305995\n",
      "Precision macro: 0.4199456973424733, Recall macro: 0.1988261294247742, F1 macro: 0.2360876684857556 \n",
      "Precision micro: 0.8374487318992215, Recall micro: 0.584643253666803, F1 micro: 0.688575361321404 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.05665300438553095\n",
      "Precision macro: 0.44309613952688776, Recall macro: 0.2186398434719199, F1 macro: 0.2614985553048511 \n",
      "Precision micro: 0.8318030710780434, Recall micro: 0.6140945480044411, F1 micro: 0.7065586445692003 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.056971511159092186\n",
      "Precision macro: 0.4462593886019939, Recall macro: 0.21972494145455573, F1 macro: 0.26002531501169557 \n",
      "Precision micro: 0.8305592805868897, Recall micro: 0.6152632501606965, F1 micro: 0.7068815038603558 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.05440715150535107\n",
      "Precision macro: 0.49912000832755116, Recall macro: 0.2531101386731522, F1 macro: 0.29890006151305704 \n",
      "Precision micro: 0.8222023676569131, Recall micro: 0.6452988955764624, F1 micro: 0.7230880041906756 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.05484144066646695\n",
      "Precision macro: 0.4664014126215013, Recall macro: 0.24516556719258056, F1 macro: 0.29240918496710205 \n",
      "Precision micro: 0.8301814268142681, Recall micro: 0.6310407292701455, F1 micro: 0.7170412668902095 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.053180619910359384\n",
      "Precision macro: 0.5163594067611879, Recall macro: 0.2692891171814398, F1 macro: 0.31965102205461327 \n",
      "Precision micro: 0.8338216512658704, Recall micro: 0.6485712616139777, F1 micro: 0.7296213515645544 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.05305031879525632\n",
      "Precision macro: 0.5283511965083475, Recall macro: 0.26414482950465784, F1 macro: 0.313878477494937 \n",
      "Precision micro: 0.8336609708150613, Recall micro: 0.6443054987436452, F1 micro: 0.726853225221662 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.05155713684670627\n",
      "Precision macro: 0.5329218043888294, Recall macro: 0.2946615042576652, F1 macro: 0.35077385578326187 \n",
      "Precision micro: 0.8341926929876252, Recall micro: 0.6617775959796646, F1 micro: 0.7380494639773209 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.051850678117014465\n",
      "Precision macro: 0.540059315411565, Recall macro: 0.27517132940989075, F1 macro: 0.33388903050917645 \n",
      "Precision micro: 0.8474339035769829, Recall micro: 0.6368258049436101, F1 micro: 0.7271878023554531 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.050932021727785465\n"
     ]
    }
   ],
   "source": [
    "# results_df = pd.DataFrame(columns=[\n",
    "#     \"optimizer\", \"num_hidden\", \"dim_hidden\", \"dropout_rate\", \"learning_rate\", \"num_epochs\", \n",
    "#     'precision_macro', 'recall_macro', 'f1_macro', \n",
    "#     'precision_micro', 'recall_micro', 'f1_micro'\n",
    "# ])\n",
    "\n",
    "\n",
    "for num_hidden, dim_hidden, dropout_rate, lr in itertools.product(range_num_hidden, range_dim_hidden, range_dropout, range_lr):\n",
    "    # model\n",
    "    options = {\n",
    "        \"VOCAB_SIZE\": len(index_to_word),\n",
    "        \"dim_e\": weights_matrix_ve.shape[1],\n",
    "        \"pretrained_embeddings\": weights_matrix_ve,\n",
    "        \"num_layers\": num_hidden,\n",
    "        \"num_classes\": len(classes),\n",
    "        \"mid_features\": dim_hidden,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"activation\": nn.ReLU()\n",
    "    }\n",
    "    num_epochs = 10\n",
    "    \n",
    "    result = {\n",
    "        \"optimizer\": \"SWA\", \n",
    "        \"num_hidden\": num_hidden,\n",
    "        \"dim_hidden\": dim_hidden,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    print(\"\\n\", result)\n",
    "    \n",
    "    model = FinalModel(options)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    base_opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = SWA(base_opt) \n",
    "    \n",
    "    # train the model\n",
    "    model_name = \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "    metrics_dict = train_model(wiki_loaders, model, criterion, optimizer, num_epochs=num_epochs, model_name=model_name)\n",
    "    result.update(metrics_dict)\n",
    "    \n",
    "    results_df = results_df.append(result, ignore_index=True)\n",
    "    results_df.to_csv(\"results/results_tuning_2_3_layers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv(\"results_tuning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
