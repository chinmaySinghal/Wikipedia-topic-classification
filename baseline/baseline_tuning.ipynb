{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import nltk\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "# import importlib\n",
    "# importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_FOLDER = \"../../\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data (the functions are in `preprocess.py`):\n",
    "<ol>\n",
    "    <li> Remove stopwords. </li>\n",
    "    <li> Remove rows with missing labels. </li>\n",
    "    <li> Remove rows with no tokens. </li>\n",
    "    <li> Create a set of all categories. Binarize the labels. </li>\n",
    "    <li> Split in train/val/test. </li>\n",
    "    <li> Build vocabulary for train. </li>\n",
    "</ol>\n",
    "\n",
    "Make DataLoader:\n",
    "<ol>\n",
    "    <li> Tokenize train/val/test. </li>\n",
    "    <li> Create batches using collate function that pads the short sentences. </li>\n",
    "</ol>\n",
    "\n",
    "Use pretrained embeddings:\n",
    "<ol>\n",
    "    <li> Load pretrained embeddings. </li>\n",
    "    <li> Create embedding matrix for given vocabulary. Words that are in given vocabualry but not in pretrained embeddings have zero embedding vector. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the dataframe from pickle file\n",
    "import pickle as pkl\n",
    "\n",
    "wiki_df =  pkl.load(open(PATH_TO_FOLDER + \"wikitext_tokenized.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q2000864</td>\n",
       "      <td>[Culture.Philosophy and religion]</td>\n",
       "      <td>[affirming, the, consequent, sometimes, called...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q1064113</td>\n",
       "      <td>[History_And_Society.Business and economics]</td>\n",
       "      <td>[growth, two, six, two, zero, one, six, zero, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q6941060</td>\n",
       "      <td>[Geography.Europe]</td>\n",
       "      <td>[the, museum, of, work, or, arbetets, museum, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q843920</td>\n",
       "      <td>[History_And_Society.History and society, STEM...</td>\n",
       "      <td>[like, this, one, in, dorset, england, arable,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q178999</td>\n",
       "      <td>[STEM.Biology, STEM.Medicine]</td>\n",
       "      <td>[an, axon, from, greek, axis, or, nerve, fiber...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                               mid_level_categories  \\\n",
       "0  Q2000864                  [Culture.Philosophy and religion]   \n",
       "1  Q1064113       [History_And_Society.Business and economics]   \n",
       "2  Q6941060                                 [Geography.Europe]   \n",
       "3   Q843920  [History_And_Society.History and society, STEM...   \n",
       "4   Q178999                      [STEM.Biology, STEM.Medicine]   \n",
       "\n",
       "                                              tokens  \n",
       "0  [affirming, the, consequent, sometimes, called...  \n",
       "1  [growth, two, six, two, zero, one, six, zero, ...  \n",
       "2  [the, museum, of, work, or, arbetets, museum, ...  \n",
       "3  [like, this, one, in, dorset, england, arable,...  \n",
       "4  [an, axon, from, greek, axis, or, nerve, fiber...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import preprocess\n",
    "# import importlib\n",
    "# importlib.reload(preprocess)\n",
    "\n",
    "from preprocess import remove_stop_words, train_validate_test_split\n",
    "from preprocess import tokenize_dataset, TensoredDataset, pad_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q2000864</td>\n",
       "      <td>[Culture.Philosophy and religion]</td>\n",
       "      <td>[affirming, consequent, sometimes, called, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q1064113</td>\n",
       "      <td>[History_And_Society.Business and economics]</td>\n",
       "      <td>[growth, two, six, two, zero, one, six, zero, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q6941060</td>\n",
       "      <td>[Geography.Europe]</td>\n",
       "      <td>[museum, work, arbetets, museum, swedish, muse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q843920</td>\n",
       "      <td>[History_And_Society.History and society, STEM...</td>\n",
       "      <td>[like, one, dorset, england, arable, land, lat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q178999</td>\n",
       "      <td>[STEM.Biology, STEM.Medicine]</td>\n",
       "      <td>[axon, greek, axis, nerve, fiber, long, slende...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                               mid_level_categories  \\\n",
       "0  Q2000864                  [Culture.Philosophy and religion]   \n",
       "1  Q1064113       [History_And_Society.Business and economics]   \n",
       "2  Q6941060                                 [Geography.Europe]   \n",
       "3   Q843920  [History_And_Society.History and society, STEM...   \n",
       "4   Q178999                      [STEM.Biology, STEM.Medicine]   \n",
       "\n",
       "                                              tokens  \n",
       "0  [affirming, consequent, sometimes, called, con...  \n",
       "1  [growth, two, six, two, zero, one, six, zero, ...  \n",
       "2  [museum, work, arbetets, museum, swedish, muse...  \n",
       "3  [like, one, dorset, england, arable, land, lat...  \n",
       "4  [axon, greek, axis, nerve, fiber, long, slende...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing stop words\n",
    "wiki_df['tokens'] = wiki_df[\"tokens\"].apply(remove_stop_words)\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99969, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing rows with missing labels\n",
    "mask = wiki_df.mid_level_categories.apply(lambda x: len(x) > 0)\n",
    "wiki_df = wiki_df[mask]\n",
    "wiki_df = wiki_df.reset_index(drop=True)\n",
    "wiki_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99960, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing rows with no tokens\n",
    "mask = wiki_df.tokens.apply(lambda x: len(x) > 0)\n",
    "wiki_df = wiki_df[mask]\n",
    "wiki_df = wiki_df.reset_index(drop=True)\n",
    "wiki_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q2000864</td>\n",
       "      <td>[Culture.Philosophy and religion]</td>\n",
       "      <td>[affirming, consequent, sometimes, called, con...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q1064113</td>\n",
       "      <td>[History_And_Society.Business and economics]</td>\n",
       "      <td>[growth, two, six, two, zero, one, six, zero, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q6941060</td>\n",
       "      <td>[Geography.Europe]</td>\n",
       "      <td>[museum, work, arbetets, museum, swedish, muse...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q843920</td>\n",
       "      <td>[History_And_Society.History and society, STEM...</td>\n",
       "      <td>[like, one, dorset, england, arable, land, lat...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q178999</td>\n",
       "      <td>[STEM.Biology, STEM.Medicine]</td>\n",
       "      <td>[axon, greek, axis, nerve, fiber, long, slende...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                               mid_level_categories  \\\n",
       "0  Q2000864                  [Culture.Philosophy and religion]   \n",
       "1  Q1064113       [History_And_Society.Business and economics]   \n",
       "2  Q6941060                                 [Geography.Europe]   \n",
       "3   Q843920  [History_And_Society.History and society, STEM...   \n",
       "4   Q178999                      [STEM.Biology, STEM.Medicine]   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [affirming, consequent, sometimes, called, con...   \n",
       "1  [growth, two, six, two, zero, one, six, zero, ...   \n",
       "2  [museum, work, arbetets, museum, swedish, muse...   \n",
       "3  [like, one, dorset, england, arable, land, lat...   \n",
       "4  [axon, greek, axis, nerve, fiber, long, slende...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize the labels\n",
    "# labels list: mlb.classes_\n",
    "mlb = MultiLabelBinarizer()\n",
    "wiki_df[\"labels\"] = list(mlb.fit_transform(wiki_df.mid_level_categories))\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val/test split\n",
    "wiki_train, wiki_valid, wiki_test = train_validate_test_split(wiki_df, seed=1)\n",
    "\n",
    "wiki_train = wiki_train.reset_index(drop=True)\n",
    "wiki_valid = wiki_valid.reset_index(drop=True)\n",
    "wiki_test = wiki_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is: 595364\n"
     ]
    }
   ],
   "source": [
    "# Building vocabulary\n",
    "vocab = list(set([y for x in list(wiki_train['tokens']) for y in x]))\n",
    "\n",
    "print(\"Vocab size is: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {\"<pad>\":0, \"<unk>\":1}\n",
    "for word in vocab:\n",
    "    if word not in word_to_index:\n",
    "        word_to_index[word] = len(word_to_index)\n",
    "index_to_word = {v:k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79968/79968 [00:05<00:00, 14360.07it/s]\n",
      "100%|██████████| 9996/9996 [00:00<00:00, 12697.08it/s]\n",
      "100%|██████████| 9996/9996 [00:00<00:00, 12833.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# CHANGE max number of tokens \n",
    "max_num_tokens = 500\n",
    "wiki_tokenized_train = tokenize_dataset(wiki_train, word_to_index, max_num_tokens=max_num_tokens)\n",
    "wiki_tokenized_val = tokenize_dataset(wiki_valid, word_to_index, max_num_tokens=max_num_tokens)\n",
    "wiki_tokenized_test = tokenize_dataset(wiki_test, word_to_index, max_num_tokens=max_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tokenized_datasets = {}\n",
    "wiki_tokenized_datasets['X_train'] = wiki_tokenized_train\n",
    "wiki_tokenized_datasets['X_val'] = wiki_tokenized_val\n",
    "wiki_tokenized_datasets['X_test'] = wiki_tokenized_test\n",
    "\n",
    "wiki_tokenized_datasets['y_train'] = list(wiki_train.labels)\n",
    "wiki_tokenized_datasets['y_val'] = list(wiki_valid.labels)\n",
    "wiki_tokenized_datasets['y_test'] = list(wiki_test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tensor_dataset = {}\n",
    "wiki_tensor_dataset['train'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_train'], wiki_tokenized_datasets['y_train']\n",
    ")\n",
    "wiki_tensor_dataset['val'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_val'], wiki_tokenized_datasets['y_val']\n",
    ")\n",
    "wiki_tensor_dataset['test'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_test'], wiki_tokenized_datasets['y_test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([164870, 443205, 388602, 361458, 491134, 429789, 164870, 573021, 497131,\n",
       "         524145, 377983,  74744, 377983, 327308, 292454, 245377, 198652, 226397,\n",
       "         310381,   2640,  23873,  23873]),\n",
       " tensor([22.]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings and make a pretrained embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2519370it [03:34, 11760.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2.5 million\n",
    "embeddings = utils.load_vectors(PATH_TO_FOLDER + \"wiki.en.align.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "vocab_size = len(index_to_word)\n",
    "embed_dim = len(embeddings[\"apple\"])\n",
    "weights_matrix = np.zeros((vocab_size,embed_dim))\n",
    "\n",
    "words_found = 0\n",
    "for i, word in enumerate(word_to_index):\n",
    "    if word in embeddings.keys():\n",
    "        weights_matrix[i] = embeddings[word]\n",
    "        words_found += 1\n",
    "    else:\n",
    "        weights_matrix[i] = np.zeros(embed_dim)\n",
    "weights_matrix = torch.FloatTensor(weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 595364\n",
      "No. of words from vocab found in fastText: 470346\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words in vocab: {}\".format(len(vocab)))\n",
    "print(\"No. of words from vocab found in fastText: {}\".format(words_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(mlb.classes_),\n",
    "    \"mid_features\": 100,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "    \n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = SWA(base_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (layer_bag_of_words): BagOfWords(\n",
       "    (embed_e): Embedding(595366, 300)\n",
       "  )\n",
       "  (layer_out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# from utils import test_model\n",
    "\n",
    "# best_val_f1_micro = 0\n",
    "# num_epochs = 20\n",
    "# for epoch in range(num_epochs):\n",
    "#     runnin_loss = 0.0\n",
    "#     for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "#         model.train()\n",
    "#         data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(data_batch, length_batch)\n",
    "#         loss = criterion(outputs, label_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         runnin_loss += loss.item()\n",
    "#         #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "#         if i>0 and i % 300 == 0:\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "#                 epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "#         # validate every 300 iterations\n",
    "#         if i > 0 and i % 300 == 0:\n",
    "#             metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "#             print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "#                 metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "#             ))\n",
    "#             print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "#                 metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "#             ))\n",
    "            \n",
    "#             if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "#                 best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "#                 optimizer.swap_swa_sgd()\n",
    "#                 torch.save(model.state_dict(), 'baseline.pth')\n",
    "#                 print('Model Saved')\n",
    "#                 print()\n",
    "# optimizer.swap_swa_sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"../../baseline.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search vs. Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> dropout </li>\n",
    "    <li> ?? learning rate </li>\n",
    "    <li> optimizer </li>\n",
    "    <li> num of hidden layers </li>\n",
    "    <li> dim of hidden layers </li>\n",
    "    <li> * take only first 500/1000 words from the article </li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I focused on SWA optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one layer\n",
    "# range_dropout = [0]\n",
    "# range_num_hidden = [1]\n",
    "# range_dim_hidden = [40, 80, 120]\n",
    "# range_lr = [0.001, 0.01]\n",
    "\n",
    "# many layers\n",
    "range_dropout = [0, 0.1, 0.2]\n",
    "range_num_hidden = [2, 3]\n",
    "range_dim_hidden = [40, 80, 120]\n",
    "range_lr = [0.001, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, num_epochs=10, device=device):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 1000 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "                ))\n",
    "                print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "                ))\n",
    "\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "#                     optimizer.swap_swa_sgd()\n",
    "#                     torch.save(model.state_dict(), f\"{PATH_TO_FOLDER}baseline_models_params/{model_name}.pth\")\n",
    "#                     print('Model Saved')\n",
    "#                     print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 40, 'dropout_rate': 0, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.15921992294490336\n",
      "Precision macro: 0.057917949610036136, Recall macro: 0.02070349329910915, F1 macro: 0.028041222781815298 \n",
      "Precision micro: 0.7162206805417906, Recall micro: 0.12668731373809386, F1 micro: 0.21529294935451843 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.13101045458763838\n",
      "Precision macro: 0.11735284777738592, Recall macro: 0.06062856167597581, F1 macro: 0.06742932498089074 \n",
      "Precision micro: 0.7609964412811387, Recall micro: 0.3123940863670894, F1 micro: 0.4429530201342282 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.08174411727488042\n",
      "Precision macro: 0.187709587514583, Recall macro: 0.0940581292541711, F1 macro: 0.10804515746230227 \n",
      "Precision micro: 0.8213893163332601, Recall micro: 0.4366855606848595, F1 micro: 0.570218610507039 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.07741684293001891\n",
      "Precision macro: 0.26156668116744525, Recall macro: 0.11703989853736009, F1 macro: 0.1341215538523816 \n",
      "Precision micro: 0.8212678536489924, Recall micro: 0.49056273008823703, F1 micro: 0.6142308395829523 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.06501522216200828\n",
      "Precision macro: 0.3216892901388468, Recall macro: 0.16026039529456962, F1 macro: 0.19098234874026299 \n",
      "Precision micro: 0.8300123435020279, Recall micro: 0.5501081049494536, F1 micro: 0.6616763310490248 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.06385463719442487\n",
      "Precision macro: 0.3541831582787581, Recall macro: 0.1803513075119165, F1 macro: 0.2165005223007122 \n",
      "Precision micro: 0.8377273122770382, Recall micro: 0.5626132180213872, F1 micro: 0.6731454939523176 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.058398597855120894\n",
      "Precision macro: 0.4005460262733794, Recall macro: 0.20441920443110675, F1 macro: 0.2420850719067573 \n",
      "Precision micro: 0.8323826597131682, Recall micro: 0.5969146263074855, F1 micro: 0.6952526799387442 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.057712107565253974\n",
      "Precision macro: 0.4394135168150175, Recall macro: 0.2144462597062126, F1 macro: 0.2543098261983374 \n",
      "Precision micro: 0.8346482047979391, Recall micro: 0.6058551978028399, F1 micro: 0.7020822752666328 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.05536366346105933\n",
      "Precision macro: 0.5168700511518046, Recall macro: 0.24240635389720763, F1 macro: 0.28639240393830656 \n",
      "Precision micro: 0.8279297922894152, Recall micro: 0.6312160345935838, F1 micro: 0.7163129973474801 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.05429098291974515\n",
      "Precision macro: 0.5377201064742401, Recall macro: 0.25342902895811353, F1 macro: 0.30222407776706744 \n",
      "Precision micro: 0.831004200076365, Recall micro: 0.6358908432186058, F1 micro: 0.7204713983050847 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.052111896596848964\n",
      "Precision macro: 0.5491273375090592, Recall macro: 0.2787010101371857, F1 macro: 0.3312463610189382 \n",
      "Precision micro: 0.831357839459865, Recall micro: 0.6475778647811605, F1 micro: 0.7280491410176395 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.051782177100889386\n",
      "Precision macro: 0.5512897701784533, Recall macro: 0.2892256057461591, F1 macro: 0.3472956709981702 \n",
      "Precision micro: 0.8329243809948695, Recall micro: 0.6545900777186934, F1 micro: 0.7330672076434789 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04995626080967486\n",
      "Precision macro: 0.5369978092124185, Recall macro: 0.3169992793183804, F1 macro: 0.37417548780416676 \n",
      "Precision micro: 0.8282900884065263, Recall micro: 0.6734061824344066, F1 micro: 0.7428608264036616 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.05000879331026226\n",
      "Precision macro: 0.5389464078464773, Recall macro: 0.3156760683530166, F1 macro: 0.37407321138535304 \n",
      "Precision micro: 0.8374594753905099, Recall micro: 0.6641734353999883, F1 micro: 0.7408179892455597 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04861273599974811\n",
      "Precision macro: 0.5471046498638767, Recall macro: 0.32600741361031854, F1 macro: 0.3868043904234426 \n",
      "Precision micro: 0.8371804236669101, Recall micro: 0.6697247706422018, F1 micro: 0.7441482972437748 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04844998440425843\n",
      "Precision macro: 0.5655727333771361, Recall macro: 0.34572981596849345, F1 macro: 0.40254380575885745 \n",
      "Precision micro: 0.8339009507591884, Recall micro: 0.6867878221235318, F1 micro: 0.7532284423366553 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04754207475669682\n",
      "Precision macro: 0.5781182396330118, Recall macro: 0.35820557757369625, F1 macro: 0.4148326113250907 \n",
      "Precision micro: 0.8313681713528221, Recall micro: 0.696312744697014, F1 micro: 0.7578706353749284 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04725845618359745\n",
      "Precision macro: 0.5890051102195429, Recall macro: 0.3509442209947567, F1 macro: 0.41172359283243143 \n",
      "Precision micro: 0.8442503639010189, Recall micro: 0.6778472506281774, F1 micro: 0.7519528084789161 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04662592886574567\n",
      "Precision macro: 0.5796977892280585, Recall macro: 0.3647791156451623, F1 macro: 0.4225298751403827 \n",
      "Precision micro: 0.8314086162782657, Recall micro: 0.7025653012329808, F1 micro: 0.7615759802369036 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04635121727176011\n",
      "Precision macro: 0.5712318735516331, Recall macro: 0.3711589788465705, F1 macro: 0.4282466302351853 \n",
      "Precision micro: 0.8363382250174703, Recall micro: 0.6993513703032782, F1 micro: 0.7617350348470866 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 40, 'dropout_rate': 0, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.08916987020522356\n",
      "Precision macro: 0.3480525235236109, Recall macro: 0.17251638933584698, F1 macro: 0.20442236740086983 \n",
      "Precision micro: 0.8386785839467228, Recall micro: 0.5592824168760592, F1 micro: 0.6710604732690624 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.07283070002123714\n",
      "Precision macro: 0.5356085232666734, Recall macro: 0.29240745631581927, F1 macro: 0.3364718731434335 \n",
      "Precision micro: 0.8114817688130334, Recall micro: 0.6723543504937767, F1 micro: 0.7353956282755977 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.049416129995137455\n",
      "Precision macro: 0.5442546977444377, Recall macro: 0.3421067581652388, F1 macro: 0.3966942169999527 \n",
      "Precision micro: 0.8320594479830149, Recall micro: 0.6870215625547829, F1 micro: 0.7526165861152899 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.04852870473172516\n",
      "Precision macro: 0.5560406245732383, Recall macro: 0.38668104945794646, F1 macro: 0.4356343370377672 \n",
      "Precision micro: 0.8251999457774163, Recall micro: 0.7114474376205224, F1 micro: 0.7641133460947063 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.04545710920728743\n",
      "Precision macro: 0.6097312918405274, Recall macro: 0.381370964521192, F1 macro: 0.44299391659359094 \n",
      "Precision micro: 0.8579116525898797, Recall micro: 0.670718167475019, F1 micro: 0.7528532073986619 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.045365075478330255\n",
      "Precision macro: 0.614847415967117, Recall macro: 0.40250873285959676, F1 macro: 0.4639062840227698 \n",
      "Precision micro: 0.8346548989864166, Recall micro: 0.7073569800736282, F1 micro: 0.7657515182186235 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04443521058931947\n",
      "Precision macro: 0.6141680874686012, Recall macro: 0.416524284416745, F1 macro: 0.4687446539654237 \n",
      "Precision micro: 0.8300758949560078, Recall micro: 0.7221994974580728, F1 micro: 0.7723892256733953 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04409290775284171\n",
      "Precision macro: 0.6281917976557739, Recall macro: 0.4215751918921967, F1 macro: 0.47454629166562884 \n",
      "Precision micro: 0.8334337954591119, Recall micro: 0.7271664816221586, F1 micro: 0.7766820621645238 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04346967448107898\n",
      "Precision macro: 0.6298899057576431, Recall macro: 0.4304365348518459, F1 macro: 0.48912309555824507 \n",
      "Precision micro: 0.8242534171147243, Recall micro: 0.7435283118097353, F1 micro: 0.7818125960061445 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.043373334121890365\n",
      "Precision macro: 0.6129762935629857, Recall macro: 0.4313942056937913, F1 macro: 0.49030656557000757 \n",
      "Precision micro: 0.843562305833621, Recall micro: 0.7140185823642845, F1 micro: 0.773403379960757 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04311453340016305\n",
      "Precision macro: 0.6160361108426119, Recall macro: 0.42958265547793834, F1 macro: 0.48855031273813676 \n",
      "Precision micro: 0.8429968323922324, Recall micro: 0.7153625898439783, F1 micro: 0.7739529002686898 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04303271537646651\n",
      "Precision macro: 0.6596358140113641, Recall macro: 0.4658841155763817, F1 macro: 0.5213805078121794 \n",
      "Precision micro: 0.8411728772144166, Recall micro: 0.7241862911237071, F1 micro: 0.7783081077686366 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04245577777735889\n",
      "Precision macro: 0.6734738189403374, Recall macro: 0.46312987314568926, F1 macro: 0.5236036516070004 \n",
      "Precision micro: 0.817143217824907, Recall micro: 0.7565009057441711, F1 micro: 0.7856535987377109 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04262932132277638\n",
      "Precision macro: 0.6657080089562707, Recall macro: 0.44780102193823423, F1 macro: 0.5149590063754437 \n",
      "Precision micro: 0.8469536286130173, Recall micro: 0.7140185823642845, F1 micro: 0.7748256182625237 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.042248909981921316\n",
      "Precision macro: 0.6872756102357583, Recall macro: 0.44404953372271483, F1 macro: 0.5125503632626476 \n",
      "Precision micro: 0.843603186491455, Recall micro: 0.7240109858002688, F1 micro: 0.7792452830188679 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.042498554101213816\n",
      "Precision macro: 0.6494477918763552, Recall macro: 0.5056577680847768, F1 macro: 0.5503490661231972 \n",
      "Precision micro: 0.8160890622051702, Recall micro: 0.7581955238707415, F1 micro: 0.7860777898945837 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04205398740246892\n",
      "Precision macro: 0.6907905347500637, Recall macro: 0.46975597095873484, F1 macro: 0.5276498214175649 \n",
      "Precision micro: 0.8358542680520847, Recall micro: 0.7427102203003565, F1 micro: 0.7865342368266346 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04210121946688741\n",
      "Precision macro: 0.6696734063286502, Recall macro: 0.44320207093907826, F1 macro: 0.5105045899956816 \n",
      "Precision micro: 0.8422604759618681, Recall micro: 0.7176415590486764, F1 micro: 0.7749731810437307 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04167783260904252\n",
      "Precision macro: 0.6573604364688778, Recall macro: 0.4876899150922008, F1 macro: 0.5438086546830087 \n",
      "Precision micro: 0.8229477020602218, Recall micro: 0.758604569625431, F1 micro: 0.7894672828995378 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04190000388212502\n",
      "Precision macro: 0.6595163434938124, Recall macro: 0.4516517827050366, F1 macro: 0.5112866067811154 \n",
      "Precision micro: 0.8452891886368281, Recall micro: 0.7250628177408988, F1 micro: 0.7805737292400604 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 40, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.15970440240949393\n",
      "Precision macro: 0.07687513313333563, Recall macro: 0.034323191744375744, F1 macro: 0.04304175949727334 \n",
      "Precision micro: 0.7201602699283003, Recall micro: 0.19955589318062292, F1 micro: 0.31251429878746284 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.12828035385534167\n",
      "Precision macro: 0.11146631652812816, Recall macro: 0.07011964908409755, F1 macro: 0.07958072751556808 \n",
      "Precision micro: 0.7650534255185418, Recall micro: 0.35563606614854204, F1 micro: 0.48555927876176797 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.08146339373290538\n",
      "Precision macro: 0.20162371289385855, Recall macro: 0.0884504367442662, F1 macro: 0.09937974864228244 \n",
      "Precision micro: 0.7973478772323815, Recall micro: 0.4356921638520423, F1 micro: 0.5634824667472793 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.07827480612136424\n",
      "Precision macro: 0.20823916517783433, Recall macro: 0.11180196118151606, F1 macro: 0.1290012817932376 \n",
      "Precision micro: 0.8185063969056828, Recall micro: 0.4822649447788231, F1 micro: 0.6069274893366672 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.06717850595340133\n",
      "Precision macro: 0.2843986240997452, Recall macro: 0.14631819085156017, F1 macro: 0.1725775229847018 \n",
      "Precision micro: 0.8248607867792348, Recall micro: 0.5366680301525156, F1 micro: 0.650263744822459 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.0656389356199652\n",
      "Precision macro: 0.33405778244599604, Recall macro: 0.1667589233250675, F1 macro: 0.19696646010922247 \n",
      "Precision micro: 0.8312049344105638, Recall micro: 0.5591071115526208, F1 micro: 0.6685299049748463 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.05966322180256248\n",
      "Precision macro: 0.4706084776313467, Recall macro: 0.19440724464117434, F1 macro: 0.22998608635793055 \n",
      "Precision micro: 0.8238434163701067, Recall micro: 0.5952200081809151, F1 micro: 0.6911151066933542 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.05889829684328288\n",
      "Precision macro: 0.4480885668538267, Recall macro: 0.21448223113837972, F1 macro: 0.2525464074000792 \n",
      "Precision micro: 0.8257343097881723, Recall micro: 0.6127505405247473, F1 micro: 0.7034751106936805 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.055202558487653736\n",
      "Precision macro: 0.4507105004685645, Recall macro: 0.23780621609270572, F1 macro: 0.278849578105035 \n",
      "Precision micro: 0.828806647687928, Recall micro: 0.6294629813592006, F1 micro: 0.7155097974094985 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.05471898176614195\n",
      "Precision macro: 0.5094241555444573, Recall macro: 0.24728603196898494, F1 macro: 0.29651788783213096 \n",
      "Precision micro: 0.8305241283768183, Recall micro: 0.6305732484076433, F1 micro: 0.7168670696871056 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.05242333895526827\n",
      "Precision macro: 0.5445846633574541, Recall macro: 0.27716380967762233, F1 macro: 0.3322821628309662 \n",
      "Precision micro: 0.8319687523473297, Recall micro: 0.6472272541342838, F1 micro: 0.7280615263261685 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.052116368267685176\n",
      "Precision macro: 0.5429604440946633, Recall macro: 0.2898822019171406, F1 macro: 0.34653313857787166 \n",
      "Precision micro: 0.837265577737447, Recall micro: 0.6469935137030328, F1 micro: 0.7299337442726703 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.050621765077114106\n",
      "Precision macro: 0.5392833383607957, Recall macro: 0.30539784516603086, F1 macro: 0.3614173947157933 \n",
      "Precision micro: 0.8351559036500665, Recall micro: 0.6604920236077836, F1 micro: 0.7376252161712403 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.05028997243009508\n",
      "Precision macro: 0.5610948222257138, Recall macro: 0.3219337417103636, F1 macro: 0.3787557974196927 \n",
      "Precision micro: 0.8368216200337814, Recall micro: 0.6658680535265588, F1 micro: 0.7416205662219331 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.049150639075785874\n",
      "Precision macro: 0.5608862674433237, Recall macro: 0.338228772229446, F1 macro: 0.393557469205203 \n",
      "Precision micro: 0.8333333333333334, Recall micro: 0.6760941973937942, F1 micro: 0.7465238571474658 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04875977356825024\n",
      "Precision macro: 0.5720620175262385, Recall macro: 0.33579614356606946, F1 macro: 0.3959931862292246 \n",
      "Precision micro: 0.8352534562211982, Recall micro: 0.6778472506281774, F1 micro: 0.7483629560336764 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.0479483928643167\n",
      "Precision macro: 0.5609558174350445, Recall macro: 0.347212582112159, F1 macro: 0.40209286088032015 \n",
      "Precision micro: 0.8310734463276837, Recall micro: 0.6876643487407235, F1 micro: 0.7525980878073738 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.047695255031809214\n",
      "Precision macro: 0.5641769845505545, Recall macro: 0.3473042729920623, F1 macro: 0.40379099657398093 \n",
      "Precision micro: 0.8357377517497501, Recall micro: 0.6838076316250804, F1 micro: 0.7521774063956291 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04697764629684389\n",
      "Precision macro: 0.5584355815852091, Recall macro: 0.3612082265289193, F1 macro: 0.41890619102951443 \n",
      "Precision micro: 0.8400257216347528, Recall micro: 0.6870215625547829, F1 micro: 0.7558584332508277 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04692243207525462\n",
      "Precision macro: 0.5476203167563474, Recall macro: 0.3690879489585587, F1 macro: 0.42310737497126644 \n",
      "Precision micro: 0.8334962301033231, Recall micro: 0.6976567521767078, F1 micro: 0.7595508477272004 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 40, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.09096962071955204\n",
      "Precision macro: 0.36782331407501767, Recall macro: 0.18261082820297944, F1 macro: 0.21193317756278854 \n",
      "Precision micro: 0.8106152970820125, Recall micro: 0.5827733302167942, F1 micro: 0.678066358444384 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.07416215118579567\n",
      "Precision macro: 0.516728570457947, Recall macro: 0.27348649523610413, F1 macro: 0.3238467476150298 \n",
      "Precision micro: 0.8313627406396847, Recall micro: 0.640974697598317, F1 micro: 0.7238591744481472 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.049673843979835514\n",
      "Precision macro: 0.5421520429744597, Recall macro: 0.3581487800159572, F1 macro: 0.4114335694903086 \n",
      "Precision micro: 0.8348050829205256, Recall micro: 0.6794834336469351, F1 micro: 0.7491785323110625 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.04847733356058598\n",
      "Precision macro: 0.5555052086436185, Recall macro: 0.37195207369814176, F1 macro: 0.4276564338605296 \n",
      "Precision micro: 0.8346186203028604, Recall micro: 0.6956699585110735, F1 micro: 0.758836090129713 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.045894103014841675\n",
      "Precision macro: 0.5406181434205931, Recall macro: 0.42918232977652554, F1 macro: 0.4632576771759158 \n",
      "Precision micro: 0.8085212098627415, Recall micro: 0.7607082335066908, F1 micro: 0.783886313000542 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.045366941343992946\n",
      "Precision macro: 0.6124078055513119, Recall macro: 0.4274212278711737, F1 macro: 0.473816853310253 \n",
      "Precision micro: 0.8287653006918574, Recall micro: 0.7279845731315374, F1 micro: 0.7751127702597604 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04431054967083037\n",
      "Precision macro: 0.6184559639190561, Recall macro: 0.444836424326598, F1 macro: 0.49791705617271304 \n",
      "Precision micro: 0.8319718403400411, Recall micro: 0.7320165955706188, F1 micro: 0.7788001243394467 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04398718979116529\n",
      "Precision macro: 0.634111613433834, Recall macro: 0.42924735632899047, F1 macro: 0.49002681592664477 \n",
      "Precision micro: 0.8389566395663957, Recall micro: 0.7236019400455794, F1 micro: 0.7770213032974619 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04341162375360727\n",
      "Precision macro: 0.6377299398539066, Recall macro: 0.4359488804477747, F1 macro: 0.49597743426597296 \n",
      "Precision micro: 0.8368257401649317, Recall micro: 0.7234266347221411, F1 micro: 0.7760052653022848 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04326272436324507\n",
      "Precision macro: 0.6427944814397152, Recall macro: 0.4542161949427298, F1 macro: 0.5132116521266347 \n",
      "Precision micro: 0.8350398179749715, Recall micro: 0.729153275287793, F1 micro: 0.7785126029448465 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04282854941114783\n",
      "Precision macro: 0.6476924525657872, Recall macro: 0.44767129115082654, F1 macro: 0.5113920059387659 \n",
      "Precision micro: 0.8422864084221893, Recall micro: 0.7293285806112312, F1 micro: 0.7817481444364411 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04266011537518352\n",
      "Precision macro: 0.6481628975027297, Recall macro: 0.45098682611149826, F1 macro: 0.5121629626875787 \n",
      "Precision micro: 0.835234474017744, Recall micro: 0.7316659849237421, F1 micro: 0.7800274109145278 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04229159418307245\n",
      "Precision macro: 0.6661329875481047, Recall macro: 0.44675512045317256, F1 macro: 0.5089857729769693 \n",
      "Precision micro: 0.850768156424581, Recall micro: 0.7119149184830246, F1 micro: 0.7751725893169599 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04251895695645362\n",
      "Precision macro: 0.6624755643097262, Recall macro: 0.4501698293059327, F1 macro: 0.5107259993204146 \n",
      "Precision micro: 0.8328712871287128, Recall micro: 0.7373341903815812, F1 micro: 0.7821963239624338 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04216411723382771\n",
      "Precision macro: 0.6993895924727839, Recall macro: 0.4726635375660538, F1 macro: 0.5368920561363364 \n",
      "Precision micro: 0.8542893187552565, Recall micro: 0.7122655291299013, F1 micro: 0.7768394888626877 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04204529907274991\n",
      "Precision macro: 0.6713111089811142, Recall macro: 0.46029721498197396, F1 macro: 0.5214742546536137 \n",
      "Precision micro: 0.8409106037409305, Recall micro: 0.7382107169987728, F1 micro: 0.7862210604929051 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.042169213056564334\n",
      "Precision macro: 0.7073766180917074, Recall macro: 0.4652855615918707, F1 macro: 0.5277814949105057 \n",
      "Precision micro: 0.8305315765561109, Recall micro: 0.747735639572255, F1 micro: 0.7869618696186962 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04182667444087565\n",
      "Precision macro: 0.6659210420093803, Recall macro: 0.4623272842035673, F1 macro: 0.5249271954339225 \n",
      "Precision micro: 0.8285321461306796, Recall micro: 0.7394962893706539, F1 micro: 0.7814863988637416 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04168593579903245\n",
      "Precision macro: 0.6918886635193183, Recall macro: 0.44943155699067217, F1 macro: 0.5125078133297447 \n",
      "Precision micro: 0.8406954887218046, Recall micro: 0.7317828551393677, F1 micro: 0.7824674310350214 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.0414589924281463\n",
      "Precision macro: 0.6927294825360001, Recall macro: 0.4610075456881171, F1 macro: 0.5303607065337597 \n",
      "Precision micro: 0.8350549378248568, Recall micro: 0.7416583883597265, F1 micro: 0.7855904926962118 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 40, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.15922832523286343\n",
      "Precision macro: 0.039279103249691485, Recall macro: 0.00817684684130883, F1 macro: 0.013226373288873288 \n",
      "Precision micro: 0.7675765095119934, Recall micro: 0.05422778005025419, F1 micro: 0.10129898482698395 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.13069599244743585\n",
      "Precision macro: 0.11282589163989747, Recall macro: 0.06007258488198533, F1 macro: 0.06939016250348184 \n",
      "Precision micro: 0.7849709213345577, Recall micro: 0.2997136679717174, F1 micro: 0.4337970989977587 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.08196875473856927\n",
      "Precision macro: 0.15546145250795138, Recall macro: 0.09308479287451944, F1 macro: 0.10635488740423887 \n",
      "Precision micro: 0.8196558570483123, Recall micro: 0.43423128615672296, F1 micro: 0.5677069406776424 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.07774918906763197\n",
      "Precision macro: 0.2402920215317574, Recall macro: 0.11851754092729276, F1 macro: 0.13453546526964796 \n",
      "Precision micro: 0.8121937429325292, Recall micro: 0.5036521942382983, F1 micro: 0.6217493237150586 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.06580227144062518\n",
      "Precision macro: 0.3226570049758895, Recall macro: 0.15796627262580176, F1 macro: 0.1863916954471355 \n",
      "Precision micro: 0.825826355222565, Recall micro: 0.5474785250978788, F1 micro: 0.6584440227703985 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.06403618822619318\n",
      "Precision macro: 0.3868500756980204, Recall macro: 0.17756234538902363, F1 macro: 0.21164694110652393 \n",
      "Precision micro: 0.8265905657210005, Recall micro: 0.5754689417401975, F1 micro: 0.6785406690322803 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.05845052839815617\n",
      "Precision macro: 0.43152616506344793, Recall macro: 0.19781991612556685, F1 macro: 0.2344197546242052 \n",
      "Precision micro: 0.8300887839048627, Recall micro: 0.5955121837199789, F1 micro: 0.6935011908812521 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.05771001993119717\n",
      "Precision macro: 0.4527958566562603, Recall macro: 0.2107842016801422, F1 macro: 0.252332759214462 \n",
      "Precision micro: 0.8398952279610379, Recall micro: 0.5996026412668731, F1 micro: 0.6996931469485168 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.054448902770876884\n",
      "Precision macro: 0.4586358418044247, Recall macro: 0.23386918076737126, F1 macro: 0.2782898050088329 \n",
      "Precision micro: 0.8353748927206055, Recall micro: 0.6256646993513703, F1 micro: 0.7154694286668893 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.05392048077471554\n",
      "Precision macro: 0.5240747810525441, Recall macro: 0.2572785289037164, F1 macro: 0.308361833797964 \n",
      "Precision micro: 0.8361250482439213, Recall micro: 0.632969087827967, F1 micro: 0.720500199547692 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.05170941969566047\n",
      "Precision macro: 0.5209956418901522, Recall macro: 0.2787227023554815, F1 macro: 0.3341416463767046 \n",
      "Precision micro: 0.8377498485766203, Recall micro: 0.6465844679483433, F1 micro: 0.7298571946835527 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.05147047310974449\n",
      "Precision macro: 0.5232681644045903, Recall macro: 0.2990066025207265, F1 macro: 0.35373262345442275 \n",
      "Precision micro: 0.8376119402985075, Recall micro: 0.6558756500905745, F1 micro: 0.7356864287352932 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04976532462798059\n",
      "Precision macro: 0.5119511427447475, Recall macro: 0.32165572378557544, F1 macro: 0.3730647527703929 \n",
      "Precision micro: 0.8350104973575617, Recall micro: 0.6739905335125344, F1 micro: 0.745909590635711 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04962912438903004\n",
      "Precision macro: 0.5216849859873479, Recall macro: 0.32276437632634986, F1 macro: 0.37455304400831685 \n",
      "Precision micro: 0.830783528738919, Recall micro: 0.6790743878922456, F1 micro: 0.7473071605414617 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.048788920922204854\n",
      "Precision macro: 0.553724442909025, Recall macro: 0.3223953196374096, F1 macro: 0.3819304619151715 \n",
      "Precision micro: 0.8450609937518596, Recall micro: 0.6638812598609244, F1 micro: 0.7435939391956017 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04840481237601489\n",
      "Precision macro: 0.523839348766848, Recall macro: 0.3376224624735628, F1 macro: 0.3920982309640785 \n",
      "Precision micro: 0.8401473562554175, Recall micro: 0.6796587389703734, F1 micro: 0.7514294020738443 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04774077181704342\n",
      "Precision macro: 0.5470268336707631, Recall macro: 0.34601573588162854, F1 macro: 0.4007303845341173 \n",
      "Precision micro: 0.8385026737967914, Recall micro: 0.6871968678782212, F1 micro: 0.755347164236624 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.047288913515396415\n",
      "Precision macro: 0.5449789193577117, Recall macro: 0.3485267890240993, F1 macro: 0.4047210992997755 \n",
      "Precision micro: 0.8450560652395515, Recall micro: 0.6781978612750541, F1 micro: 0.7524880863617209 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.046507118290290236\n",
      "Precision macro: 0.5410625047461199, Recall macro: 0.360307494299147, F1 macro: 0.41576675394164264 \n",
      "Precision micro: 0.8422849825709611, Recall micro: 0.6918716765032431, F1 micro: 0.7597048444016682 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04654965913388878\n",
      "Precision macro: 0.5468432662610411, Recall macro: 0.36026299193050854, F1 macro: 0.41665851346186056 \n",
      "Precision micro: 0.8376882661996498, Recall micro: 0.6987670192251505, F1 micro: 0.7619472409838155 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 40, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.08657455552741886\n",
      "Precision macro: 0.3816291885220531, Recall macro: 0.2011505538904431, F1 macro: 0.23787893710951247 \n",
      "Precision micro: 0.8225572396001289, Recall micro: 0.5962134050137322, F1 micro: 0.6913304197581054 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.07058666454069316\n",
      "Precision macro: 0.542921513400907, Recall macro: 0.3037469285178522, F1 macro: 0.35793716331474923 \n",
      "Precision micro: 0.8234376108391857, Recall micro: 0.6783147314906796, F1 micro: 0.743864146107017 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.047800629997625944\n",
      "Precision macro: 0.5723320074993024, Recall macro: 0.387377066396169, F1 macro: 0.43974255295871045 \n",
      "Precision micro: 0.8270990312163616, Recall micro: 0.7184012154502425, F1 micro: 0.7689276667604841 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.046707198874093594\n",
      "Precision macro: 0.5939200119045577, Recall macro: 0.4193557503386811, F1 macro: 0.4649757047492077 \n",
      "Precision micro: 0.8232934325461015, Recall micro: 0.7435283118097353, F1 micro: 0.7813804961925815 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.04409324466064572\n",
      "Precision macro: 0.5977300775093203, Recall macro: 0.3917792757209225, F1 macro: 0.4473180041218628 \n",
      "Precision micro: 0.8431359170305677, Recall micro: 0.7220826272424473, F1 micro: 0.7779281689697505 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.04387161206174642\n",
      "Precision macro: 0.6308453215746167, Recall macro: 0.4213183778304696, F1 macro: 0.4791480146896252 \n",
      "Precision micro: 0.8463360267483979, Recall micro: 0.7099865599252031, F1 micro: 0.772188502971178 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.042261524660512806\n",
      "Precision macro: 0.628999261103385, Recall macro: 0.4335937961006868, F1 macro: 0.48802870081993716 \n",
      "Precision micro: 0.828810698519865, Recall micro: 0.7460410214456845, F1 micro: 0.785250791893471 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04243712961766869\n",
      "Precision macro: 0.6536414189801738, Recall macro: 0.46262817656371413, F1 macro: 0.5232246448008867 \n",
      "Precision micro: 0.8370258365656433, Recall micro: 0.7439957926722375, F1 micro: 0.7877737903724787 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04151571765355766\n",
      "Precision macro: 0.6639335448721139, Recall macro: 0.4528997732151401, F1 macro: 0.5124836228039201 \n",
      "Precision micro: 0.8222138222138222, Recall micro: 0.7626365920645124, F1 micro: 0.7913054022918814 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04146458537131548\n",
      "Precision macro: 0.7036150192572106, Recall macro: 0.48494777973610526, F1 macro: 0.5456458792326349 \n",
      "Precision micro: 0.8289943848902501, Recall micro: 0.7591889207035587, F1 micro: 0.7925575720603935 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.041221391512081026\n",
      "Precision macro: 0.6798675820646177, Recall macro: 0.4778278752478899, F1 macro: 0.5260960796721041 \n",
      "Precision micro: 0.8240085880272796, Recall micro: 0.7625197218488868, F1 micro: 0.7920725970439163 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04126640528254211\n",
      "Precision macro: 0.7209497652909632, Recall macro: 0.4580225400076124, F1 macro: 0.5239140079866471 \n",
      "Precision micro: 0.8447663112720445, Recall micro: 0.7361654882253258, F1 micro: 0.7867357771810405 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04057096377946436\n",
      "Precision macro: 0.7156123853960814, Recall macro: 0.47767814019385924, F1 macro: 0.5474081191391806 \n",
      "Precision micro: 0.8429216969042962, Recall micro: 0.7303219774440484, F1 micro: 0.782592360676268 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.040813202655874195\n",
      "Precision macro: 0.7424871596696715, Recall macro: 0.4796718557643199, F1 macro: 0.5443754331476514 \n",
      "Precision micro: 0.849942023054362, Recall micro: 0.7281598784549758, F1 micro: 0.7843519859004218 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.040124972535297274\n",
      "Precision macro: 0.7228500059579264, Recall macro: 0.4938649592282658, F1 macro: 0.5530051137188258 \n",
      "Precision micro: 0.83924473873992, Recall micro: 0.7480278151113189, F1 micro: 0.7910152629302354 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04028762597776949\n",
      "Precision macro: 0.7361376336703196, Recall macro: 0.48866017391369443, F1 macro: 0.5496134151418315 \n",
      "Precision micro: 0.8284975400529835, Recall micro: 0.7675451411207853, F1 micro: 0.7968574635241302 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.040446768645197154\n",
      "Precision macro: 0.752936085086657, Recall macro: 0.4835673494277834, F1 macro: 0.5439077234697025 \n",
      "Precision micro: 0.8215246075917704, Recall micro: 0.7676620113364109, F1 micro: 0.7936805219913001 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04020739588234574\n",
      "Precision macro: 0.7478712451558615, Recall macro: 0.5069700542330288, F1 macro: 0.5658801342812144 \n",
      "Precision micro: 0.8228021124572849, Recall micro: 0.773856132764565, F1 micro: 0.7975788966514092 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.039671270253136756\n",
      "Precision macro: 0.7050123180516737, Recall macro: 0.5131111716117737, F1 macro: 0.569468751086941 \n",
      "Precision micro: 0.8261581067472307, Recall micro: 0.7670192251504704, F1 micro: 0.795491045725887 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.03978270624671131\n",
      "Precision macro: 0.7315631269551646, Recall macro: 0.5137119369952883, F1 macro: 0.5667381358695077 \n",
      "Precision micro: 0.8158782747407817, Recall micro: 0.7770700636942676, F1 micro: 0.7960014366096014 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 80, 'dropout_rate': 0, 'learning_rate': 0.001, 'num_epochs': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.144156963840127\n",
      "Precision macro: 0.12103301941443148, Recall macro: 0.04751702898736054, F1 macro: 0.054537181664419225 \n",
      "Precision micro: 0.7455923545888944, Recall micro: 0.264418862852802, F1 micro: 0.3903890949874903 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.11628943858481944\n",
      "Precision macro: 0.13280337522435579, Recall macro: 0.08160651927362478, F1 macro: 0.0921777449436474 \n",
      "Precision micro: 0.8097161320101546, Recall micro: 0.4100391515222346, F1 micro: 0.5443966018852554 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.07034102689474821\n",
      "Precision macro: 0.29497333845347556, Recall macro: 0.14021160034977528, F1 macro: 0.16558655692080274 \n",
      "Precision micro: 0.8174690007293947, Recall micro: 0.5239291766493309, F1 micro: 0.6385812471065846 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.06720424460060895\n",
      "Precision macro: 0.42247449222842093, Recall macro: 0.17070872621809083, F1 macro: 0.2015239037908872 \n",
      "Precision micro: 0.8222942983953567, Recall micro: 0.5629638286682639, F1 micro: 0.6683547816434839 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.05834048568829894\n",
      "Precision macro: 0.4533923053455631, Recall macro: 0.21384274939376177, F1 macro: 0.25409404507936234 \n",
      "Precision micro: 0.8340262492867042, Recall micro: 0.5978495880324899, F1 micro: 0.6964601769911504 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.05682474680338055\n",
      "Precision macro: 0.4878021347723045, Recall macro: 0.2345748953744753, F1 macro: 0.2839456219499378 \n",
      "Precision micro: 0.8430227843995783, Recall micro: 0.6075498159294104, F1 micro: 0.7061740134483462 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.05187303237989545\n",
      "Precision macro: 0.5469961302860878, Recall macro: 0.2708620580625101, F1 macro: 0.3301933146223374 \n",
      "Precision micro: 0.8479854923913901, Recall micro: 0.6284695845263835, F1 micro: 0.7219089810712849 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.051700385365635156\n",
      "Precision macro: 0.5289415771031308, Recall macro: 0.30355602154146777, F1 macro: 0.3636267112506055 \n",
      "Precision micro: 0.8425525457166705, Recall micro: 0.6488634371530415, F1 micro: 0.7331308596329064 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04911597719229758\n",
      "Precision macro: 0.5307039141906524, Recall macro: 0.32816461624923726, F1 macro: 0.3880646376894197 \n",
      "Precision micro: 0.8393714535137494, Recall micro: 0.6742242739437855, F1 micro: 0.7477883275543602 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.048649318678304555\n",
      "Precision macro: 0.5661566915208899, Recall macro: 0.34672206667734967, F1 macro: 0.4058197865184993 \n",
      "Precision micro: 0.8417836618898669, Recall micro: 0.6762110676094197, F1 micro: 0.7499675955930006 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04678652300313115\n",
      "Precision macro: 0.5788291893521252, Recall macro: 0.36854040056967996, F1 macro: 0.42581214628630254 \n",
      "Precision micro: 0.8359320618126131, Recall micro: 0.7017472097236019, F1 micro: 0.7629848470408844 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04647655860241502\n",
      "Precision macro: 0.577176034742945, Recall macro: 0.3749933861291717, F1 macro: 0.4317671606330257 \n",
      "Precision micro: 0.837289084926136, Recall micro: 0.6988254543329633, F1 micro: 0.761816791948019 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.045482538925483824\n",
      "Precision macro: 0.5642652356105413, Recall macro: 0.39054006060919266, F1 macro: 0.4438283171205763 \n",
      "Precision micro: 0.8341677525546944, Recall micro: 0.7107462163267692, F1 micro: 0.7675269767148356 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04536243558768183\n",
      "Precision macro: 0.5736587085504655, Recall macro: 0.3959236688887762, F1 macro: 0.45064288938275593 \n",
      "Precision micro: 0.8369557717518802, Recall micro: 0.7088178577689476, F1 micro: 0.7675757767512497 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.044362543780356645\n",
      "Precision macro: 0.5697807818454821, Recall macro: 0.39659296269455646, F1 macro: 0.45161471216292526 \n",
      "Precision micro: 0.8432094477249045, Recall micro: 0.7092853386314498, F1 micro: 0.7704709914942237 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.0442417785609141\n",
      "Precision macro: 0.5736378049822093, Recall macro: 0.40169958710153497, F1 macro: 0.4552130040558694 \n",
      "Precision micro: 0.8447842480100545, Recall micro: 0.7070063694267515, F1 micro: 0.7697789088595514 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04348493629135192\n",
      "Precision macro: 0.5980768592630942, Recall macro: 0.4040741002498953, F1 macro: 0.45988654153355096 \n",
      "Precision micro: 0.8397348277747403, Recall micro: 0.7179921696955531, F1 micro: 0.7741061584501497 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04344339133892208\n",
      "Precision macro: 0.574169888061447, Recall macro: 0.41834099734394703, F1 macro: 0.47019999718241734 \n",
      "Precision micro: 0.8391925760346813, Recall micro: 0.723952550692456, F1 micro: 0.7773246329526917 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04264749507419765\n",
      "Precision macro: 0.568738415183441, Recall macro: 0.4291973850052415, F1 macro: 0.47671047918593173 \n",
      "Precision micro: 0.8365956309706727, Recall micro: 0.7317828551393677, F1 micro: 0.7806869895891778 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04248518111370504\n",
      "Precision macro: 0.5945166503691548, Recall macro: 0.4254466917198598, F1 macro: 0.47825861143684195 \n",
      "Precision micro: 0.8404133180252583, Recall micro: 0.7271664816221586, F1 micro: 0.7796992481203007 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 80, 'dropout_rate': 0, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.08098524937406182\n",
      "Precision macro: 0.44298636311877887, Recall macro: 0.22403742709189967, F1 macro: 0.26127949523225974 \n",
      "Precision micro: 0.8200153964588145, Recall micro: 0.6224507684216677, F1 micro: 0.7077035511410823 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.06684390869643539\n",
      "Precision macro: 0.5501176959470628, Recall macro: 0.359722867288498, F1 macro: 0.4065524096507607 \n",
      "Precision micro: 0.8149126371393742, Recall micro: 0.7031496523111085, F1 micro: 0.7549170300197622 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.04608607903309166\n",
      "Precision macro: 0.5708516247816102, Recall macro: 0.3792788507117306, F1 macro: 0.4303804684761332 \n",
      "Precision micro: 0.8342012080816497, Recall micro: 0.7020978203704786, F1 micro: 0.7624698565807844 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.04517766229435802\n",
      "Precision macro: 0.6011511978835808, Recall macro: 0.42259991204941544, F1 macro: 0.47447332953520344 \n",
      "Precision micro: 0.8290620698695681, Recall micro: 0.7391456787237772, F1 micro: 0.7815261044176708 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.04268950051255524\n",
      "Precision macro: 0.6542799833431994, Recall macro: 0.43502503258964526, F1 macro: 0.49432206311872096 \n",
      "Precision micro: 0.8371278458844134, Recall micro: 0.7262315198971542, F1 micro: 0.7777464876873494 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.042277433948591354\n",
      "Precision macro: 0.627820003964941, Recall macro: 0.4721839184900026, F1 macro: 0.5219401376501167 \n",
      "Precision micro: 0.8422080107707842, Recall micro: 0.7310816338456144, F1 micro: 0.7827202202202203 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.041005644008517264\n",
      "Precision macro: 0.653539132047569, Recall macro: 0.48577330047376965, F1 macro: 0.5307299688990014 \n",
      "Precision micro: 0.8053234662210551, Recall micro: 0.7885233448255712, F1 micro: 0.7968348637397029 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.040901092533953486\n",
      "Precision macro: 0.6398275821672725, Recall macro: 0.48483657566650257, F1 macro: 0.5360320712077236 \n",
      "Precision micro: 0.843646482045846, Recall micro: 0.744112662887863, F1 micro: 0.7907597727202161 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.0400239555798471\n",
      "Precision macro: 0.6789850004842116, Recall macro: 0.46305130162008423, F1 macro: 0.5233348991599197 \n",
      "Precision micro: 0.8393724563476435, Recall micro: 0.74720972360194, F1 micro: 0.7906142764398553 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04001838730089367\n",
      "Precision macro: 0.6923852722228475, Recall macro: 0.4667826723471137, F1 macro: 0.5319376734067182 \n",
      "Precision micro: 0.8412792224848963, Recall micro: 0.7486121661894466, F1 micro: 0.7922451377508426 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.0395274851154536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.7322754482324055, Recall macro: 0.4887740873715481, F1 macro: 0.5443834620179124 \n",
      "Precision micro: 0.8266042273129983, Recall micro: 0.7632793782504529, F1 micro: 0.7936806926933009 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.0393534677317366\n",
      "Precision macro: 0.6929251486273018, Recall macro: 0.4881061376051475, F1 macro: 0.554875681249072 \n",
      "Precision micro: 0.8385409857497712, Recall micro: 0.7496055630222638, F1 micro: 0.791583104501558 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.038996341672725976\n",
      "Precision macro: 0.7200653312566581, Recall macro: 0.5036295971237902, F1 macro: 0.5738907319059935 \n",
      "Precision micro: 0.8494731897188108, Recall micro: 0.7396715946940922, F1 micro: 0.7907790341725496 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.03896323175588623\n",
      "Precision macro: 0.7363518788484772, Recall macro: 0.4981759465887389, F1 macro: 0.5571364925886854 \n",
      "Precision micro: 0.8204919061362781, Recall micro: 0.7641559048676445, F1 micro: 0.7913225015884542 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.038601447835564615\n",
      "Precision macro: 0.719943985346503, Recall macro: 0.5539715186199471, F1 macro: 0.6059974663130131 \n",
      "Precision micro: 0.8231585328887542, Recall micro: 0.7934318938818442, F1 micro: 0.8080218995477267 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.03852548573818058\n",
      "Precision macro: 0.6890673462149578, Recall macro: 0.5357484284989191, F1 macro: 0.5813145446812933 \n",
      "Precision micro: 0.8196661608497724, Recall micro: 0.7891076959036989, F1 micro: 0.8040967012028105 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.03782847626693547\n",
      "Precision macro: 0.7215132810383813, Recall macro: 0.5309337431634541, F1 macro: 0.5883277157695164 \n",
      "Precision micro: 0.8375969737770084, Recall micro: 0.7633962484660785, F1 micro: 0.7987771323754203 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.03809456705953926\n",
      "Precision macro: 0.7279359769828933, Recall macro: 0.5190637699122734, F1 macro: 0.5836545105853636 \n",
      "Precision micro: 0.842969819438107, Recall micro: 0.7556828142347922, F1 micro: 0.7969433659949466 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.03820592720620334\n",
      "Precision macro: 0.6966819236824071, Recall macro: 0.4851818198064453, F1 macro: 0.5480995912071877 \n",
      "Precision micro: 0.8449668874172186, Recall micro: 0.7455735405831824, F1 micro: 0.7921646540216682 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.03809079759940505\n",
      "Precision macro: 0.6958770381128226, Recall macro: 0.5469913326981283, F1 macro: 0.598755348735242 \n",
      "Precision micro: 0.8300031318509239, Recall micro: 0.7743236136270671, F1 micro: 0.8011971703246871 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 80, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.14452567730098964\n",
      "Precision macro: 0.09874777356627068, Recall macro: 0.03808932026450746, F1 macro: 0.045924694717848394 \n",
      "Precision micro: 0.7150958154086821, Recall micro: 0.2136971892713142, F1 micro: 0.32905925226076393 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.1174716276768595\n",
      "Precision macro: 0.15480265090219428, Recall macro: 0.08668499148155277, F1 macro: 0.09666635867012673 \n",
      "Precision micro: 0.806733857570052, Recall micro: 0.42564132530824517, F1 micro: 0.5572641725958228 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.06968896752223372\n",
      "Precision macro: 0.2551647493124191, Recall macro: 0.1433695445649177, F1 macro: 0.16818426627659522 \n",
      "Precision micro: 0.8306667899750301, Recall micro: 0.5248641383743353, F1 micro: 0.6432715032586122 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.06688433522731066\n",
      "Precision macro: 0.37705128192786835, Recall macro: 0.17543537087209507, F1 macro: 0.20648855267274707 \n",
      "Precision micro: 0.8317517312131316, Recall micro: 0.5685151639104774, F1 micro: 0.6753904894133982 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.05752218883670866\n",
      "Precision macro: 0.4218091790502841, Recall macro: 0.20635953392219267, F1 macro: 0.24832639999432676 \n",
      "Precision micro: 0.8440343992652584, Recall micro: 0.5907205048793315, F1 micro: 0.6950154692334135 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.05656087299156934\n",
      "Precision macro: 0.43704705646791475, Recall macro: 0.23516222293687186, F1 macro: 0.27606280060888744 \n",
      "Precision micro: 0.8287115060517849, Recall micro: 0.6321509963185882, F1 micro: 0.7172075446680147 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.05233869696594775\n",
      "Precision macro: 0.534062138834882, Recall macro: 0.2731611379100024, F1 macro: 0.32653465207358273 \n",
      "Precision micro: 0.8367810866105934, Recall micro: 0.6452988955764624, F1 micro: 0.7286704058066644 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.0517669211551547\n",
      "Precision macro: 0.5235516231526848, Recall macro: 0.3080345239395094, F1 macro: 0.36377789663723237 \n",
      "Precision micro: 0.833211625529429, Recall micro: 0.6667445801437504, F1 micro: 0.7407407407407407 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04916564919613302\n",
      "Precision macro: 0.5499675730487333, Recall macro: 0.328401000103402, F1 macro: 0.3851803384838823 \n",
      "Precision micro: 0.8362876495172215, Recall micro: 0.6781978612750541, F1 micro: 0.7489916427349875 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.048626621993258595\n",
      "Precision macro: 0.5407442316796482, Recall macro: 0.34045207916535314, F1 macro: 0.39785366599033584 \n",
      "Precision micro: 0.8443468715697037, Recall micro: 0.6742242739437855, F1 micro: 0.7497563194489572 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.047022821942344306\n",
      "Precision macro: 0.5449932101394958, Recall macro: 0.3574309947157371, F1 macro: 0.41591378130133805 \n",
      "Precision micro: 0.8425513575635685, Recall micro: 0.685443814643838, F1 micro: 0.7559207346544224 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04672089820541441\n",
      "Precision macro: 0.5655516260703656, Recall macro: 0.3630924167393015, F1 macro: 0.4198829273285579 \n",
      "Precision micro: 0.8456896551724138, Recall micro: 0.6878980891719745, F1 micro: 0.7586762478651758 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.0456521153151989\n",
      "Precision macro: 0.583868746199826, Recall macro: 0.37497055087828507, F1 macro: 0.43105524722825783 \n",
      "Precision micro: 0.8403007096184922, Recall micro: 0.6988838894407761, F1 micro: 0.7630957697951891 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04520139419846237\n",
      "Precision macro: 0.5834821896791493, Recall macro: 0.37953973774843147, F1 macro: 0.43570354464016425 \n",
      "Precision micro: 0.84310138605502, Recall micro: 0.7002278969204698, F1 micro: 0.7650513950073421 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04445587998814881\n",
      "Precision macro: 0.5753435087959744, Recall macro: 0.38703014867529023, F1 macro: 0.44427714068349966 \n",
      "Precision micro: 0.8412808910546468, Recall micro: 0.7061882779173727, F1 micro: 0.7678378550098481 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04422083360794932\n",
      "Precision macro: 0.5702787821758352, Recall macro: 0.4025999448465841, F1 macro: 0.45405994213198464 \n",
      "Precision micro: 0.8379209070418687, Recall micro: 0.7168819026471104, F1 micro: 0.7726900547962462 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04337105113826692\n",
      "Precision macro: 0.6040679512983768, Recall macro: 0.39909943321169644, F1 macro: 0.45699603538754413 \n",
      "Precision micro: 0.8469046291132181, Recall micro: 0.7098696897095775, F1 micro: 0.772355914422863 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04329952836874872\n",
      "Precision macro: 0.6012924496541617, Recall macro: 0.40691752367848016, F1 macro: 0.46150238908640673 \n",
      "Precision micro: 0.8414965986394558, Recall micro: 0.7228422836440134, F1 micro: 0.7776695061767203 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04295539273507893\n",
      "Precision macro: 0.6016645513835619, Recall macro: 0.41288475058410906, F1 macro: 0.467226846719775 \n",
      "Precision micro: 0.8389612150800109, Recall micro: 0.7230175889674516, F1 micro: 0.776686230815103 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04259823947492987\n",
      "Precision macro: 0.6013003248893232, Recall macro: 0.40570371310639397, F1 macro: 0.46197563564364413 \n",
      "Precision micro: 0.8481455563331001, Recall micro: 0.7082335066908199, F1 micro: 0.7719007738114192 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 80, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'num_epochs': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.08017346029356122\n",
      "Precision macro: 0.46171422905849213, Recall macro: 0.2199982145333129, F1 macro: 0.2587360107921002 \n",
      "Precision micro: 0.8313567362428842, Recall micro: 0.6144451586513177, F1 micro: 0.7066294815362386 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.06594208792410791\n",
      "Precision macro: 0.5247374202203763, Recall macro: 0.36967767013753206, F1 macro: 0.42182864456505925 \n",
      "Precision micro: 0.8339115044247788, Recall micro: 0.688307134926664, F1 micro: 0.7541455919072925 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.045515495229512454\n",
      "Precision macro: 0.5981393245901212, Recall macro: 0.414824869084603, F1 macro: 0.46906092073662137 \n",
      "Precision micro: 0.8402164235326347, Recall micro: 0.7168819026471104, F1 micro: 0.7736646276092578 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.044698548722080886\n",
      "Precision macro: 0.6832327400395676, Recall macro: 0.4428677340133809, F1 macro: 0.49832055121204805 \n",
      "Precision micro: 0.8112617961377414, Recall micro: 0.7585461345176182, F1 micro: 0.7840188439934772 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.042238298006355764\n",
      "Precision macro: 0.6246581961525858, Recall macro: 0.4388360507771662, F1 macro: 0.4936901729791966 \n",
      "Precision micro: 0.858499929108181, Recall micro: 0.7076491556126921, F1 micro: 0.7758096031263013 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.04180497462302447\n",
      "Precision macro: 0.6889952762215951, Recall macro: 0.465699840312137, F1 macro: 0.5270706483888156 \n",
      "Precision micro: 0.8295100718052996, Recall micro: 0.7628118973879506, F1 micro: 0.7947640791476407 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04053369781374931\n",
      "Precision macro: 0.6715914124145198, Recall macro: 0.4726066964402285, F1 macro: 0.5291434352191429 \n",
      "Precision micro: 0.8350845134764733, Recall micro: 0.747735639572255, F1 micro: 0.7889998766802317 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04057293689437211\n",
      "Precision macro: 0.694079892785386, Recall macro: 0.49789120395390346, F1 macro: 0.5450772714918323 \n",
      "Precision micro: 0.8454924267478008, Recall micro: 0.7469759831706889, F1 micro: 0.7931868950111691 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.03931745133176446\n",
      "Precision macro: 0.7200948793440959, Recall macro: 0.4816285245196061, F1 macro: 0.5442596800831213 \n",
      "Precision micro: 0.8442355398877138, Recall micro: 0.7556828142347922, F1 micro: 0.7975085566279176 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.039546718589961526\n",
      "Precision macro: 0.7073357827094662, Recall macro: 0.48599862595747284, F1 macro: 0.5505899160474031 \n",
      "Precision micro: 0.8325182597650047, Recall micro: 0.7659673932098404, F1 micro: 0.7978574471970296 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.03904157109744847\n",
      "Precision macro: 0.7048319998006067, Recall macro: 0.5011504339921342, F1 macro: 0.5630770237630857 \n",
      "Precision micro: 0.8448091806778756, Recall micro: 0.7399053351253433, F1 micro: 0.7888850814616367 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.039138596051372584\n",
      "Precision macro: 0.7196566009897233, Recall macro: 0.5279952722612725, F1 macro: 0.5838101751183695 \n",
      "Precision micro: 0.8291846029072306, Recall micro: 0.7766610179395781, F1 micro: 0.8020638464787884 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.038213809167966246\n",
      "Precision macro: 0.7073041517988502, Recall macro: 0.5258506974851321, F1 macro: 0.5816471391781267 \n",
      "Precision micro: 0.8173157579388856, Recall micro: 0.7971133056740489, F1 micro: 0.8070881282726385 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.03833187216706574\n",
      "Precision macro: 0.7147047242054402, Recall macro: 0.5185409954129178, F1 macro: 0.5775540112832162 \n",
      "Precision micro: 0.8362244897959183, Recall micro: 0.7662011336410915, F1 micro: 0.7996828591467692 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.037773303452879194\n",
      "Precision macro: 0.7219985565414796, Recall macro: 0.49091532143353867, F1 macro: 0.5533186624481877 \n",
      "Precision micro: 0.8316237177883412, Recall micro: 0.776953193478642, F1 micro: 0.8033594151234102 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.03805047269212082\n",
      "Precision macro: 0.7263309244583162, Recall macro: 0.5101038657900164, F1 macro: 0.5764537369626063 \n",
      "Precision micro: 0.8453986123838199, Recall micro: 0.7547478525097879, F1 micro: 0.797505479917261 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.03744318982027471\n",
      "Precision macro: 0.7428256340027072, Recall macro: 0.5301280405145636, F1 macro: 0.5957047742488657 \n",
      "Precision micro: 0.8483617300131061, Recall micro: 0.7565009057441711, F1 micro: 0.7998023043894602 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.037782500900328156\n",
      "Precision macro: 0.7335981006480394, Recall macro: 0.5283468773462202, F1 macro: 0.5897583493989801 \n",
      "Precision micro: 0.8536913655023891, Recall micro: 0.741249342605037, F1 micro: 0.7935068184661578 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.03746222483646124\n",
      "Precision macro: 0.7356160310523886, Recall macro: 0.5316222606507327, F1 macro: 0.5950305345992191 \n",
      "Precision micro: 0.8352030947775628, Recall micro: 0.7569683866066733, F1 micro: 0.7941636268890048 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.037595169989857825\n",
      "Precision macro: 0.6948380684272973, Recall macro: 0.5433747159290369, F1 macro: 0.5854520542467264 \n",
      "Precision micro: 0.8199097744360903, Recall micro: 0.7965289545959212, F1 micro: 0.8080502697255321 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 80, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.13902023539692163\n",
      "Precision macro: 0.11756419602553207, Recall macro: 0.05133428867573326, F1 macro: 0.05797651449623969 \n",
      "Precision micro: 0.7451390004820826, Recall micro: 0.2709635949278326, F1 micro: 0.3974117243743572 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.11310851918533445\n",
      "Precision macro: 0.22177848236698097, Recall macro: 0.09003503836005346, F1 macro: 0.10184696876991593 \n",
      "Precision micro: 0.7894185437401781, Recall micro: 0.44030853736925146, F1 micro: 0.5653087253357342 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.068869873739779\n",
      "Precision macro: 0.30279089451790436, Recall macro: 0.14732521109397997, F1 macro: 0.1748371053877213 \n",
      "Precision micro: 0.8350051560888723, Recall micro: 0.5204815052883772, F1 micro: 0.6412526997840173 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.06597702421993017\n",
      "Precision macro: 0.39897790141506045, Recall macro: 0.18276488729404264, F1 macro: 0.21742985160714562 \n",
      "Precision micro: 0.8362223550508069, Recall micro: 0.5722550108104949, F1 micro: 0.6795031917846239 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.05682202848047018\n",
      "Precision macro: 0.45905004872438293, Recall macro: 0.22576049881460672, F1 macro: 0.26894922596273435 \n",
      "Precision micro: 0.8348682121308352, Recall micro: 0.6145035937591304, F1 micro: 0.7079336228079033 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.055612665604799986\n",
      "Precision macro: 0.5112531915356349, Recall macro: 0.25344249532188967, F1 macro: 0.2994088057215113 \n",
      "Precision micro: 0.8291887793783169, Recall micro: 0.6391047741483084, F1 micro: 0.7218427218427218 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.05113331354223192\n",
      "Precision macro: 0.5453225653512721, Recall macro: 0.29793002964829396, F1 macro: 0.3567472540173424 \n",
      "Precision micro: 0.8440582789497647, Recall micro: 0.6499737042014843, F1 micro: 0.7344095605955564 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.05033333298005164\n",
      "Precision macro: 0.5416473768211583, Recall macro: 0.309864517179364, F1 macro: 0.3690575799932647 \n",
      "Precision micro: 0.8402432693020842, Recall micro: 0.6620113364109157, F1 micro: 0.7405543208262517 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04820048291422427\n",
      "Precision macro: 0.5824234989544785, Recall macro: 0.34498979950510544, F1 macro: 0.4036869158336298 \n",
      "Precision micro: 0.8385517886705535, Recall micro: 0.6807690060188161, F1 micro: 0.751467457911372 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.047723843207582835\n",
      "Precision macro: 0.5642901347261563, Recall macro: 0.3542096027962392, F1 macro: 0.41097539935937727 \n",
      "Precision micro: 0.8418018018018018, Recall micro: 0.6825220592531993, F1 micro: 0.7538401962049825 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04596295372210443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.5569612770017284, Recall macro: 0.3621710437742363, F1 macro: 0.41943613277740854 \n",
      "Precision micro: 0.8435190449118818, Recall micro: 0.6936247297376263, F1 micro: 0.7612634279300947 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04584634846355766\n",
      "Precision macro: 0.5880772250996915, Recall macro: 0.37445368156257647, F1 macro: 0.4293867398832013 \n",
      "Precision micro: 0.8401913202504044, Recall micro: 0.6980073628235844, F1 micro: 0.7625279285030322 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04440433394163847\n",
      "Precision macro: 0.5763327799728839, Recall macro: 0.39208206308858456, F1 macro: 0.4442076142891214 \n",
      "Precision micro: 0.8375421454620519, Recall micro: 0.7112721322970841, F1 micro: 0.7692599380648424 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.044549899446778\n",
      "Precision macro: 0.577494278688312, Recall macro: 0.39963941951715554, F1 macro: 0.45277631903971843 \n",
      "Precision micro: 0.8378378378378378, Recall micro: 0.7173493835096125, F1 micro: 0.7729261766094758 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04345716625452042\n",
      "Precision macro: 0.5824430310206282, Recall macro: 0.3997476413888508, F1 macro: 0.4532369811720494 \n",
      "Precision micro: 0.8389870998566651, Recall micro: 0.7182843452346169, F1 micro: 0.7739579398060696 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04344442193675786\n",
      "Precision macro: 0.6061960469894679, Recall macro: 0.4014632511227927, F1 macro: 0.45654970098498543 \n",
      "Precision micro: 0.8444891447596587, Recall micro: 0.7114474376205224, F1 micro: 0.7722803679035839 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04284777792729438\n",
      "Precision macro: 0.6014147834826497, Recall macro: 0.40783840608807265, F1 macro: 0.46276729862047533 \n",
      "Precision micro: 0.84130792432136, Recall micro: 0.7171740781861743, F1 micro: 0.7742973407778935 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.042706869829446076\n",
      "Precision macro: 0.5944207830588897, Recall macro: 0.42036873944655573, F1 macro: 0.4727273280165813 \n",
      "Precision micro: 0.8406519396551724, Recall micro: 0.729387015719044, F1 micro: 0.7810769375175995 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04186633775942027\n",
      "Precision macro: 0.6073530713888421, Recall macro: 0.4174014801316704, F1 macro: 0.47619297651443804 \n",
      "Precision micro: 0.8434741273937254, Recall micro: 0.7258224741424648, F1 micro: 0.7802380728037941 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.041893124162219464\n",
      "Precision macro: 0.6252498879014797, Recall macro: 0.43037039786752235, F1 macro: 0.482737038765987 \n",
      "Precision micro: 0.8351367302773952, Recall micro: 0.7441710979956758, F1 micro: 0.7870341758852976 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 80, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.08432545262947679\n",
      "Precision macro: 0.4129539571995828, Recall macro: 0.19528547741705848, F1 macro: 0.23388734178866777 \n",
      "Precision micro: 0.8179014786115284, Recall micro: 0.6044527552153334, F1 micro: 0.6951612903225806 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.06881570839043706\n",
      "Precision macro: 0.5192568531745454, Recall macro: 0.3346292590939682, F1 macro: 0.3860504651610306 \n",
      "Precision micro: 0.8203098106712564, Recall micro: 0.6962543095892012, F1 micro: 0.7532081673936405 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.04691067609377205\n",
      "Precision macro: 0.5717642253326946, Recall macro: 0.386609373066986, F1 macro: 0.4348633125398776 \n",
      "Precision micro: 0.8282084206258026, Recall micro: 0.7161222462455443, F1 micro: 0.7680977749921655 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.04613007847592235\n",
      "Precision macro: 0.5831501041517965, Recall macro: 0.4064287368054696, F1 macro: 0.4613090697305264 \n",
      "Precision micro: 0.8443519367812037, Recall micro: 0.6992929351954654, F1 micro: 0.7650067122674679 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.043593875920400024\n",
      "Precision macro: 0.6302590697143824, Recall macro: 0.4573277586791249, F1 macro: 0.5065912461958253 \n",
      "Precision micro: 0.8297747306562194, Recall micro: 0.7425933500847309, F1 micro: 0.7837671148390281 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.043284936239011584\n",
      "Precision macro: 0.683708574648091, Recall macro: 0.4329445958265745, F1 macro: 0.4965394413273805 \n",
      "Precision micro: 0.8321949912950315, Recall micro: 0.7262315198971542, F1 micro: 0.7756108216057666 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04193815854005516\n",
      "Precision macro: 0.619600827959761, Recall macro: 0.4685334035494186, F1 macro: 0.5207676486501153 \n",
      "Precision micro: 0.8362603643011034, Recall micro: 0.748495295973821, F1 micro: 0.7899475794017884 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.041645501270890235\n",
      "Precision macro: 0.6941532839850253, Recall macro: 0.4564171560506876, F1 macro: 0.5189662984477955 \n",
      "Precision micro: 0.8369558122837935, Recall micro: 0.7493133874831999, F1 micro: 0.7907134488499722 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04051710685715079\n",
      "Precision macro: 0.7105205129986231, Recall macro: 0.47616286370834043, F1 macro: 0.5392742068805179 \n",
      "Precision micro: 0.8330751355538343, Recall micro: 0.7541635014316601, F1 micro: 0.7916577212084037 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.040727850382216275\n",
      "Precision macro: 0.6978613017395261, Recall macro: 0.4718487577982705, F1 macro: 0.5388481242732847 \n",
      "Precision micro: 0.8351511169513798, Recall micro: 0.7427686554081693, F1 micro: 0.7862555284075093 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.0403225115109235\n",
      "Precision macro: 0.6940173655338377, Recall macro: 0.49763021804915153, F1 macro: 0.5588058676427584 \n",
      "Precision micro: 0.8359822824387702, Recall micro: 0.7499561736691405, F1 micro: 0.7906360696134298 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.03999971919413656\n",
      "Precision macro: 0.6786068740916374, Recall macro: 0.4843352361688405, F1 macro: 0.5397429098217149 \n",
      "Precision micro: 0.8327599388379205, Recall micro: 0.7638052942207678, F1 micro: 0.7967935627419305 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.03946828165464103\n",
      "Precision macro: 0.7068456950512687, Recall macro: 0.49989765746919823, F1 macro: 0.560856641715173 \n",
      "Precision micro: 0.8418895178606285, Recall micro: 0.7560918599894817, F1 micro: 0.7966873960962995 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.03950881071574986\n",
      "Precision macro: 0.6924232161386473, Recall macro: 0.5011171174568398, F1 macro: 0.5547004671221375 \n",
      "Precision micro: 0.8330139261530599, Recall micro: 0.7619938058785718, F1 micro: 0.7959227271340068 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.03900069057568908\n",
      "Precision macro: 0.6924144004010228, Recall macro: 0.5082182064029299, F1 macro: 0.562041037814212 \n",
      "Precision micro: 0.8261247825006214, Recall micro: 0.7768363232630164, F1 micro: 0.8007227827134467 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.03915798813849688\n",
      "Precision macro: 0.728398156244707, Recall macro: 0.5179379178208683, F1 macro: 0.5743385200874355 \n",
      "Precision micro: 0.8317201239956981, Recall micro: 0.7682463624145387, F1 micro: 0.7987241798298907 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.03887373106367886\n",
      "Precision macro: 0.7186293441637068, Recall macro: 0.5103866611757965, F1 macro: 0.5710927457667899 \n",
      "Precision micro: 0.8448399504530935, Recall micro: 0.7572605621457371, F1 micro: 0.7986564772587206 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.038982282469980416\n",
      "Precision macro: 0.7391716146926249, Recall macro: 0.4895686993372021, F1 macro: 0.5561049093615923 \n",
      "Precision micro: 0.8500067141130656, Recall micro: 0.7397884649097177, F1 micro: 0.7910769519167681 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.03865896460786462\n",
      "Precision macro: 0.6843359193529172, Recall macro: 0.5318131149482895, F1 macro: 0.5792106544075718 \n",
      "Precision micro: 0.8327528338927237, Recall micro: 0.7684216677379769, F1 micro: 0.799294918550936 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.03871943440288305\n",
      "Precision macro: 0.7101027377914314, Recall macro: 0.5311291997957034, F1 macro: 0.5812913406626922 \n",
      "Precision micro: 0.8330904251948545, Recall micro: 0.7682463624145387, F1 micro: 0.7993555055633247 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 120, 'dropout_rate': 0, 'learning_rate': 0.001, 'num_epochs': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.13636404048651457\n",
      "Precision macro: 0.11362074486778663, Recall macro: 0.052624252026950334, F1 macro: 0.060978297017297095 \n",
      "Precision micro: 0.7754964658364187, Recall micro: 0.2692689768012622, F1 micro: 0.39973975276512697 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.11002953597158194\n",
      "Precision macro: 0.2208234759629599, Recall macro: 0.10475912221240848, F1 macro: 0.11935994833176222 \n",
      "Precision micro: 0.7949245541838135, Recall micro: 0.47408402968503477, F1 micro: 0.5939456056224606 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.06532959870994091\n",
      "Precision macro: 0.3855325475707885, Recall macro: 0.16910302442908895, F1 macro: 0.20153033999853856 \n",
      "Precision micro: 0.8147836336083516, Recall micro: 0.565534973412026, F1 micro: 0.6676554792866752 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.0629808770827949\n",
      "Precision macro: 0.42406580052374027, Recall macro: 0.20002614857065246, F1 macro: 0.23961055470617906 \n",
      "Precision micro: 0.841297786720322, Recall micro: 0.5863963069011863, F1 micro: 0.6910919045487416 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.0548047181032598\n",
      "Precision macro: 0.5017713389869325, Recall macro: 0.23547028760760982, F1 macro: 0.28418485494093687 \n",
      "Precision micro: 0.8424493701164089, Recall micro: 0.6174253491497692, F1 micro: 0.7125948406676784 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.053571844620630145\n",
      "Precision macro: 0.5449760333337448, Recall macro: 0.2759265439829854, F1 macro: 0.32885103560665174 \n",
      "Precision micro: 0.8271268766558728, Recall micro: 0.656752176707766, F1 micro: 0.732158561610371 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04889166871644556\n",
      "Precision macro: 0.5412180474488409, Recall macro: 0.32527721318816605, F1 macro: 0.3826591857417927 \n",
      "Precision micro: 0.8442786437172195, Recall micro: 0.6678548471921931, F1 micro: 0.7457748776508973 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04844219020754099\n",
      "Precision macro: 0.5551908464508452, Recall macro: 0.3467765931594263, F1 macro: 0.40492922822407706 \n",
      "Precision micro: 0.8427641099855282, Recall micro: 0.6805937006953778, F1 micro: 0.7530469078330586 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04654436895996332\n",
      "Precision macro: 0.5718086203023076, Recall macro: 0.35793091605672867, F1 macro: 0.4171627063600148 \n",
      "Precision micro: 0.8464649382537733, Recall micro: 0.684917898673523, F1 micro: 0.7571705426356589 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04603309092950076\n",
      "Precision macro: 0.576050118477593, Recall macro: 0.38433076928256965, F1 macro: 0.440532721525703 \n",
      "Precision micro: 0.8387208898157803, Recall micro: 0.7050195757611173, F1 micro: 0.7660803860562575 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04492759351432323\n",
      "Precision macro: 0.5713227373132674, Recall macro: 0.4023659534797613, F1 macro: 0.45209961769414325 \n",
      "Precision micro: 0.833513952858304, Recall micro: 0.7191024367439958, F1 micro: 0.7720927314364588 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.044286837458610534\n",
      "Precision macro: 0.568919641885094, Recall macro: 0.3992592998787084, F1 macro: 0.4525283776028159 \n",
      "Precision micro: 0.8425311203319502, Recall micro: 0.7119149184830246, F1 micro: 0.7717353434896905 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.043183644190430644\n",
      "Precision macro: 0.5738206885357957, Recall macro: 0.40678105502284806, F1 macro: 0.45866555228547307 \n",
      "Precision micro: 0.842552026286966, Recall micro: 0.7192193069596213, F1 micro: 0.7760158885281043 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.043130893675610425\n",
      "Precision macro: 0.5792543288181381, Recall macro: 0.40058273039591213, F1 macro: 0.45635726536975363 \n",
      "Precision micro: 0.8449820788530465, Recall micro: 0.7163559866767955, F1 micro: 0.7753707978874799 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04204147365130484\n",
      "Precision macro: 0.5947569878047984, Recall macro: 0.41660447270113105, F1 macro: 0.46891135416164326 \n",
      "Precision micro: 0.838614984321836, Recall micro: 0.7345293052065681, F1 micro: 0.7831287770232386 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04193438628688455\n",
      "Precision macro: 0.591901422840142, Recall macro: 0.42206660148811975, F1 macro: 0.47589504174113095 \n",
      "Precision micro: 0.8412901488115279, Recall micro: 0.7300882370127972, F1 micro: 0.7817544737830059 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04127826366387308\n",
      "Precision macro: 0.6038277409557858, Recall macro: 0.421205192502365, F1 macro: 0.4770029309703734 \n",
      "Precision micro: 0.8511505770161012, Recall micro: 0.7197452229299363, F1 micro: 0.7799518743667679 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04112723331712186\n",
      "Precision macro: 0.6177936732954877, Recall macro: 0.44001302672099124, F1 macro: 0.4942526159637136 \n",
      "Precision micro: 0.832254724307533, Recall micro: 0.7514754864722726, F1 micro: 0.7898050053738676 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04106088973768055\n",
      "Precision macro: 0.6230319916460326, Recall macro: 0.42946594095118157, F1 macro: 0.4861601276083393 \n",
      "Precision micro: 0.8433485010706638, Recall micro: 0.7364576637643896, F1 micro: 0.786286926412328 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04061260339803994\n",
      "Precision macro: 0.6217911705021261, Recall macro: 0.4366826424163524, F1 macro: 0.4919039052223153 \n",
      "Precision micro: 0.8453925371138157, Recall micro: 0.7387366329690879, F1 micro: 0.7884741322855272 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 120, 'dropout_rate': 0, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.07634331284463405\n",
      "Precision macro: 0.4921602902668541, Recall macro: 0.24374181022416738, F1 macro: 0.2917734029002404 \n",
      "Precision micro: 0.8280677119900148, Recall micro: 0.6202886694325951, F1 micro: 0.7092743552051316 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.06349706527218223\n",
      "Precision macro: 0.5729996329707701, Recall macro: 0.33645483609747623, F1 macro: 0.3947567769657253 \n",
      "Precision micro: 0.8356994971901804, Recall micro: 0.6604335884999708, F1 micro: 0.7378006985018115 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.04511537765339017\n",
      "Precision macro: 0.6170540086610106, Recall macro: 0.434770472600777, F1 macro: 0.486715009970807 \n",
      "Precision micro: 0.831297812374178, Recall micro: 0.7238941155846432, F1 micro: 0.7738872403560831 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.04422039754409343\n",
      "Precision macro: 0.6347213931675446, Recall macro: 0.4351521826597667, F1 macro: 0.4939911988793407 \n",
      "Precision micro: 0.8186956245634089, Recall micro: 0.7533454099222813, F1 micro: 0.7846622032866707 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.04153960403613746\n",
      "Precision macro: 0.6785560562014389, Recall macro: 0.4540178390603434, F1 macro: 0.5204253635121667 \n",
      "Precision micro: 0.8335512552301255, Recall micro: 0.7450476246128674, F1 micro: 0.7868184763491621 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.04132269269973039\n",
      "Precision macro: 0.6998290380979526, Recall macro: 0.4533872490793801, F1 macro: 0.5214840144337265 \n",
      "Precision micro: 0.8365753692017436, Recall micro: 0.7514170513644598, F1 micro: 0.7917128432459056 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.03976169095002115\n",
      "Precision macro: 0.7050587809598242, Recall macro: 0.4806995456014949, F1 macro: 0.5457036074474552 \n",
      "Precision micro: 0.848119090726212, Recall micro: 0.7390872436159645, F1 micro: 0.789858240179854 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.039767522484995425\n",
      "Precision macro: 0.6784922110948937, Recall macro: 0.5196954960575074, F1 macro: 0.5672366285588812 \n",
      "Precision micro: 0.825863880818446, Recall micro: 0.7806930403786595, F1 micro: 0.8026434364674075 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.03930538454651832\n",
      "Precision macro: 0.6840175474017065, Recall macro: 0.49914444313363954, F1 macro: 0.5528228151360786 \n",
      "Precision micro: 0.8292652239557121, Recall micro: 0.7702915911879857, F1 micro: 0.7986912660183586 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.03904182854667306\n",
      "Precision macro: 0.7224257142155274, Recall macro: 0.5164918521623753, F1 macro: 0.5751700182138416 \n",
      "Precision micro: 0.8263164431197219, Recall micro: 0.7775959796645825, F1 micro: 0.801216244693982 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.03845276564825326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.7160748107506628, Recall macro: 0.5053990495818438, F1 macro: 0.5678479574793066 \n",
      "Precision micro: 0.8308614704954245, Recall micro: 0.7692981943551686, F1 micro: 0.7988955640512169 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.03830531266564503\n",
      "Precision macro: 0.7142920514145265, Recall macro: 0.5459377925794846, F1 macro: 0.5965834352666968 \n",
      "Precision micro: 0.8194326585040611, Recall micro: 0.7899842225208905, F1 micro: 0.8044390229389188 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.0380645357882604\n",
      "Precision macro: 0.732128112076614, Recall macro: 0.5123516688742034, F1 macro: 0.5805723755373593 \n",
      "Precision micro: 0.8412667573343695, Recall micro: 0.7590720504879331, F1 micro: 0.798058610309025 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.03789351520081982\n",
      "Precision macro: 0.7367653385711985, Recall macro: 0.5178284615899948, F1 macro: 0.5772714087651294 \n",
      "Precision micro: 0.8262151144559423, Recall micro: 0.7698241103254836, F1 micro: 0.7970234133946397 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.03742755460552871\n",
      "Precision macro: 0.7271303594307116, Recall macro: 0.4951312799751049, F1 macro: 0.564771631384953 \n",
      "Precision micro: 0.8600708157428844, Recall micro: 0.7380938467831473, F1 micro: 0.7944274977200542 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.03745059555210173\n",
      "Precision macro: 0.7224789526908113, Recall macro: 0.5495465655134318, F1 macro: 0.6062067066527269 \n",
      "Precision micro: 0.8201896531028688, Recall micro: 0.7985741833693684, F1 micro: 0.8092376017764619 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.03710494330525398\n",
      "Precision macro: 0.6918875223613331, Recall macro: 0.5723576477267209, F1 macro: 0.6124568621196675 \n",
      "Precision micro: 0.8252846964253091, Recall micro: 0.7919125810787121, F1 micro: 0.808254309059462 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.03718039326136932\n",
      "Precision macro: 0.70078859543549, Recall macro: 0.5288431742921245, F1 macro: 0.5792348087062054 \n",
      "Precision micro: 0.8412872436476203, Recall micro: 0.7622859814176357, F1 micro: 0.7998405837088813 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.0368896729145199\n",
      "Precision macro: 0.7120524126989572, Recall macro: 0.5338497843162023, F1 macro: 0.5910376521278123 \n",
      "Precision micro: 0.8306782825140012, Recall micro: 0.780050254192719, F1 micro: 0.8045686044058704 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.03690097568742931\n",
      "Precision macro: 0.7209793852639877, Recall macro: 0.5177327752457317, F1 macro: 0.5798926055607606 \n",
      "Precision micro: 0.8587450116967111, Recall micro: 0.7293285806112312, F1 micro: 0.7887635478876354 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 120, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.13530538115650415\n",
      "Precision macro: 0.11713176016528447, Recall macro: 0.06151006498526861, F1 macro: 0.07119026671688446 \n",
      "Precision micro: 0.767273244242252, Recall micro: 0.3153742768655408, F1 micro: 0.4470120511864828 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.10918729434721172\n",
      "Precision macro: 0.1938980027176397, Recall macro: 0.09609467037168176, F1 macro: 0.10796584779279961 \n",
      "Precision micro: 0.7954176804541768, Recall micro: 0.4584818558990241, F1 micro: 0.5816806909589649 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.06547627967968583\n",
      "Precision macro: 0.32777032294849456, Recall macro: 0.16624887481285272, F1 macro: 0.19716705274637372 \n",
      "Precision micro: 0.8270345066159301, Recall micro: 0.5588149360135569, F1 micro: 0.6669688938485144 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.06271902902051807\n",
      "Precision macro: 0.41774163243860074, Recall macro: 0.20132460171683295, F1 macro: 0.2396610207556935 \n",
      "Precision micro: 0.8312101910828026, Recall micro: 0.5948109624262257, F1 micro: 0.6934159882829797 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.05427464537322521\n",
      "Precision macro: 0.5165251352161181, Recall macro: 0.2413401608284212, F1 macro: 0.28945721251336726 \n",
      "Precision micro: 0.830567081604426, Recall micro: 0.6316250803482732, F1 micro: 0.7175623195140571 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.053140557751990856\n",
      "Precision macro: 0.559168508433051, Recall macro: 0.2825674246416589, F1 macro: 0.3375883680579424 \n",
      "Precision micro: 0.8374831258437078, Recall micro: 0.6525448489452463, F1 micro: 0.7335369658751274 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04948078616522253\n",
      "Precision macro: 0.552713639863286, Recall macro: 0.34199691642720587, F1 macro: 0.3963931228909805 \n",
      "Precision micro: 0.8337124982226646, Recall micro: 0.6852685093203997, F1 micro: 0.7522370826517849 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04883655618131161\n",
      "Precision macro: 0.5756590414619978, Recall macro: 0.35153822660152595, F1 macro: 0.4091167854719332 \n",
      "Precision micro: 0.8412329165455075, Recall micro: 0.6762110676094197, F1 micro: 0.7497489390650814 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04639604789949953\n",
      "Precision macro: 0.5949929529238696, Recall macro: 0.3685268072666304, F1 macro: 0.42333673617581674 \n",
      "Precision micro: 0.8392605136889641, Recall micro: 0.6950271723251329, F1 micro: 0.7603643918810932 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.0461408663848415\n",
      "Precision macro: 0.5850053442100012, Recall macro: 0.3784298836082776, F1 macro: 0.4338710158204808 \n",
      "Precision micro: 0.8399495904221802, Recall micro: 0.7010459884298487, F1 micro: 0.7642374824818448 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04483429230004549\n",
      "Precision macro: 0.5830863496241139, Recall macro: 0.3879134420320954, F1 macro: 0.4426961403415458 \n",
      "Precision micro: 0.8418133924857896, Recall micro: 0.7096359492783264, F1 micro: 0.7700941691239419 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04453503858018666\n",
      "Precision macro: 0.5793931055230399, Recall macro: 0.39895795989085714, F1 macro: 0.4525459000190283 \n",
      "Precision micro: 0.8420507866409054, Recall micro: 0.7130836206392801, F1 micro: 0.7722195855086222 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.0437178786303848\n",
      "Precision macro: 0.5739591170693314, Recall macro: 0.40742897516087867, F1 macro: 0.4605823288925606 \n",
      "Precision micro: 0.8395681987914998, Recall micro: 0.7226085432127622, F1 micro: 0.7767100056529113 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04336121354904026\n",
      "Precision macro: 0.5698644577278009, Recall macro: 0.4243480630582019, F1 macro: 0.47270165184625146 \n",
      "Precision micro: 0.8365051903114187, Recall micro: 0.7345877403143809, F1 micro: 0.7822407516878752 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04212588457390666\n",
      "Precision macro: 0.5830016201656524, Recall macro: 0.40939569668122594, F1 macro: 0.4633494698623112 \n",
      "Precision micro: 0.8411605937921728, Recall micro: 0.7284520539940397, F1 micro: 0.780759715654652 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.042296707516536115\n",
      "Precision macro: 0.6063965836128342, Recall macro: 0.4264361318478207, F1 macro: 0.480019811511912 \n",
      "Precision micro: 0.8413811707580253, Recall micro: 0.7290364050721674, F1 micro: 0.781190319651858 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.041488170590251686\n",
      "Precision macro: 0.6007732749027379, Recall macro: 0.41686991683277985, F1 macro: 0.4740347016256592 \n",
      "Precision micro: 0.8418219740294691, Recall micro: 0.7311400689534272, F1 micro: 0.7825869402051538 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04145115004852414\n",
      "Precision macro: 0.6228305460005561, Recall macro: 0.43428028249378614, F1 macro: 0.4905874534644138 \n",
      "Precision micro: 0.8433880958793428, Recall micro: 0.731958160462806, F1 micro: 0.7837322071015171 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04092824744805694\n",
      "Precision macro: 0.6241774756654314, Recall macro: 0.4364422065044169, F1 macro: 0.4938230357091919 \n",
      "Precision micro: 0.8500068615342391, Recall micro: 0.7238941155846432, F1 micro: 0.7818979392179759 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04094965689722448\n",
      "Precision macro: 0.6440704537541005, Recall macro: 0.4485132345120029, F1 macro: 0.5040195981398845 \n",
      "Precision micro: 0.8409840390449809, Recall micro: 0.7451060597206802, F1 micro: 0.7901471727343146 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 120, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'num_epochs': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.07760655555129051\n",
      "Precision macro: 0.47024990399045286, Recall macro: 0.2521273786323328, F1 macro: 0.2951176053544268 \n",
      "Precision micro: 0.8365810107910876, Recall micro: 0.6296967217904517, F1 micro: 0.7185437087417483 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.06394812887348235\n",
      "Precision macro: 0.5197638475719214, Recall macro: 0.35770992040325544, F1 macro: 0.4126813360919713 \n",
      "Precision micro: 0.8375503162737206, Recall micro: 0.6808858762344416, F1 micro: 0.7511361804995971 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.04439932489395142\n",
      "Precision macro: 0.6073519283663632, Recall macro: 0.4093186269891568, F1 macro: 0.46625711945742976 \n",
      "Precision micro: 0.8217894177080564, Recall micro: 0.7224332378893239, F1 micro: 0.7689150107286126 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.04381175547093153\n",
      "Precision macro: 0.6453381887264276, Recall macro: 0.44165622394499415, F1 macro: 0.49244624224691264 \n",
      "Precision micro: 0.8407469156385462, Recall micro: 0.7366914041956407, F1 micro: 0.7852871558490094 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.041016162507236005\n",
      "Precision macro: 0.6757451221669148, Recall macro: 0.47063996624752397, F1 macro: 0.5338106401463683 \n",
      "Precision micro: 0.8439564840152173, Recall micro: 0.7389119382925261, F1 micro: 0.7879486540378864 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.040905510436743495\n",
      "Precision macro: 0.6781698780079207, Recall macro: 0.4970168386871788, F1 macro: 0.5468211530220782 \n",
      "Precision micro: 0.8536341567899471, Recall micro: 0.6986501490095249, F1 micro: 0.7684051544072752 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.03942690058425069\n",
      "Precision macro: 0.7092878954134821, Recall macro: 0.5033153087569594, F1 macro: 0.5624570024315884 \n",
      "Precision micro: 0.8300961599694326, Recall micro: 0.761701630339508, F1 micro: 0.7944295465626524 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.039410270522348585\n",
      "Precision macro: 0.7429584859827935, Recall macro: 0.5111616624253155, F1 macro: 0.5724533004681439 \n",
      "Precision micro: 0.838358458961474, Recall micro: 0.7604160579676269, F1 micro: 0.7974873601961083 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.03850578074529767\n",
      "Precision macro: 0.7051994085238279, Recall macro: 0.4964378744004611, F1 macro: 0.5618657588870948 \n",
      "Precision micro: 0.8414737528529508, Recall micro: 0.7540466312160345, F1 micro: 0.7953648915187377 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.03857327120937407\n",
      "Precision macro: 0.7104049129543818, Recall macro: 0.54210288866018, F1 macro: 0.6008788907345485 \n",
      "Precision micro: 0.8400025487447432, Recall micro: 0.7703500262957985, F1 micro: 0.8036699484866034 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.03796565384045243\n",
      "Precision macro: 0.7329201371797396, Recall macro: 0.5181660535469348, F1 macro: 0.5821779844968314 \n",
      "Precision micro: 0.8356501958801972, Recall micro: 0.772804300823935, F1 micro: 0.8029994838944717 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.037723790764808654\n",
      "Precision macro: 0.7145627965363985, Recall macro: 0.5244960987528944, F1 macro: 0.5742914587697269 \n",
      "Precision micro: 0.8348024605238126, Recall micro: 0.7692397592473558, F1 micro: 0.8006812237698437 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.03760338908806443\n",
      "Precision macro: 0.7052885426454346, Recall macro: 0.5061463640252053, F1 macro: 0.5648745022232382 \n",
      "Precision micro: 0.8411912308505018, Recall micro: 0.744404838426927, F1 micro: 0.7898440648541402 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.036850472684949634\n",
      "Precision macro: 0.7182633030446378, Recall macro: 0.5392348562548254, F1 macro: 0.5986956004156115 \n",
      "Precision micro: 0.8358829084041549, Recall micro: 0.775901361538012, F1 micro: 0.8047760470331536 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.03712659423146397\n",
      "Precision macro: 0.7303091111830162, Recall macro: 0.5373681792332603, F1 macro: 0.5907334697195245 \n",
      "Precision micro: 0.8379746835443038, Recall micro: 0.7736808274411267, F1 micro: 0.8045453164403124 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.03681114786863327\n",
      "Precision macro: 0.7197660602830988, Recall macro: 0.5378999442867782, F1 macro: 0.5928627470768502 \n",
      "Precision micro: 0.8374030635134269, Recall micro: 0.763513118681704, F1 micro: 0.798752903777968 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.036965538204647604\n",
      "Precision macro: 0.708469500513401, Recall macro: 0.5751807897573642, F1 macro: 0.6194889906649655 \n",
      "Precision micro: 0.823557954752229, Recall micro: 0.7934318938818442, F1 micro: 0.8082142857142857 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.03652804461307824\n",
      "Precision macro: 0.7125418666703405, Recall macro: 0.5488615660904731, F1 macro: 0.598367985686568 \n",
      "Precision micro: 0.828894518939628, Recall micro: 0.7838485362005493, F1 micro: 0.805742431523306 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.036702506498899314\n",
      "Precision macro: 0.7343754440487494, Recall macro: 0.551080050523678, F1 macro: 0.6073500029710401 \n",
      "Precision micro: 0.8443310950413223, Recall micro: 0.7641559048676445, F1 micro: 0.8022453298978559 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 120, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.13630179553478955\n",
      "Precision macro: 0.11659054986529371, Recall macro: 0.04743950783590918, F1 macro: 0.05803633157508698 \n",
      "Precision micro: 0.764262053735738, Recall micro: 0.24268100274645008, F1 micro: 0.3683860380538431 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.11095703762210905\n",
      "Precision macro: 0.17186892187771688, Recall macro: 0.09885388768630535, F1 macro: 0.11116875000931921 \n",
      "Precision micro: 0.7957767722473604, Recall micro: 0.46245544323029275, F1 micro: 0.5849656293887205 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.06717309903353452\n",
      "Precision macro: 0.3261956747655616, Recall macro: 0.15293384843741112, F1 macro: 0.18255841677569223 \n",
      "Precision micro: 0.8386655728739404, Recall micro: 0.5376614269853328, F1 micro: 0.6552485400940037 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.06417417622357607\n",
      "Precision macro: 0.4429807913410945, Recall macro: 0.1914417119976988, F1 macro: 0.22955097584964632 \n",
      "Precision micro: 0.8297523372150237, Recall micro: 0.5912464208496465, F1 micro: 0.6904835022349609 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.0549564942587167\n",
      "Precision macro: 0.451238288553283, Recall macro: 0.24165822752026012, F1 macro: 0.28720405954214595 \n",
      "Precision micro: 0.8341117857698894, Recall micro: 0.6261321802138725, F1 micro: 0.7153109249307387 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.053699215625412765\n",
      "Precision macro: 0.5299788375712638, Recall macro: 0.2595140606116177, F1 macro: 0.3120661700945456 \n",
      "Precision micro: 0.8402783087392003, Recall micro: 0.6422018348623854, F1 micro: 0.7280074191838899 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.050058854915201664\n",
      "Precision macro: 0.5648793654638925, Recall macro: 0.2950249881295756, F1 macro: 0.35667863725781823 \n",
      "Precision micro: 0.846626698034454, Recall micro: 0.6519020627593058, F1 micro: 0.7366127434796963 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.0492376228319481\n",
      "Precision macro: 0.563228989311552, Recall macro: 0.3385445168801582, F1 macro: 0.39523628058018423 \n",
      "Precision micro: 0.8357646054223298, Recall micro: 0.6863203412610296, F1 micro: 0.7537059616248477 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04713763890042901\n",
      "Precision macro: 0.5681723194063486, Recall macro: 0.3526382583550126, F1 macro: 0.41176761326750316 \n",
      "Precision micro: 0.8422893027902661, Recall micro: 0.6897095775141705, F1 micro: 0.7584013365032449 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04656573319248855\n",
      "Precision macro: 0.5885707570643572, Recall macro: 0.37597973283838876, F1 macro: 0.43170145242094554 \n",
      "Precision micro: 0.843402507259721, Recall micro: 0.6958452638345117, F1 micro: 0.7625512295081968 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.045309811178594825\n",
      "Precision macro: 0.5788305928253645, Recall macro: 0.3814799374893499, F1 macro: 0.4360036350917966 \n",
      "Precision micro: 0.8399305555555555, Recall micro: 0.7067726289955005, F1 micro: 0.7676197124996034 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04484391111135483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.5817601780873157, Recall macro: 0.392252566119444, F1 macro: 0.4461023207027095 \n",
      "Precision micro: 0.8431591432358893, Recall micro: 0.7061882779173727, F1 micro: 0.7686192202505883 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04364843302033842\n",
      "Precision macro: 0.5790036334805413, Recall macro: 0.4050145679935968, F1 macro: 0.4555478294927539 \n",
      "Precision micro: 0.8402782513810271, Recall micro: 0.7199789633611874, F1 micro: 0.7754909365558912 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.043516783931292595\n",
      "Precision macro: 0.5748626317822277, Recall macro: 0.4070644262588185, F1 macro: 0.45976595150097416 \n",
      "Precision micro: 0.8338801473049883, Recall micro: 0.7277508327002863, F1 micro: 0.777209186220669 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04294166282564402\n",
      "Precision macro: 0.5836192437061455, Recall macro: 0.3976959045131155, F1 macro: 0.4528373881782642 \n",
      "Precision micro: 0.8458054626532887, Recall micro: 0.7093437737392625, F1 micro: 0.7715874781503257 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04256708711478859\n",
      "Precision macro: 0.5798247975770064, Recall macro: 0.40757595930088847, F1 macro: 0.46232692766333494 \n",
      "Precision micro: 0.83774545941961, Recall micro: 0.7304388476596739, F1 micro: 0.7804208028969221 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04179564345441759\n",
      "Precision macro: 0.5854615239466746, Recall macro: 0.41158587730603474, F1 macro: 0.466265532344361 \n",
      "Precision micro: 0.849792817679558, Recall micro: 0.719044001636183, F1 micro: 0.7789700250055392 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04177429387718439\n",
      "Precision macro: 0.5996085221730281, Recall macro: 0.42774945518258617, F1 macro: 0.4811447489226399 \n",
      "Precision micro: 0.8454070201643017, Recall micro: 0.7276339624846608, F1 micro: 0.7821116764022361 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04130488941632211\n",
      "Precision macro: 0.6242970673206828, Recall macro: 0.4283274798455398, F1 macro: 0.4860757417776687 \n",
      "Precision micro: 0.8455847417362383, Recall micro: 0.7279845731315374, F1 micro: 0.78239025309301 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.041192495043389496\n",
      "Precision macro: 0.6184328898856376, Recall macro: 0.43100781156708795, F1 macro: 0.48403861532370945 \n",
      "Precision micro: 0.8477711789200628, Recall micro: 0.7257056039268392, F1 micro: 0.7820036521629621 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 120, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.07818441451340914\n",
      "Precision macro: 0.4610424444147147, Recall macro: 0.237083360921424, F1 macro: 0.2870385531870919 \n",
      "Precision micro: 0.8405089628681178, Recall micro: 0.6137439373575644, F1 micro: 0.7094464520922692 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.06399723706673831\n",
      "Precision macro: 0.5604543415879243, Recall macro: 0.3931975065969615, F1 macro: 0.43543931724485296 \n",
      "Precision micro: 0.8081922016541946, Recall micro: 0.7194530473908725, F1 micro: 0.7612452468544224 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.044731988383457065\n",
      "Precision macro: 0.6137055978066533, Recall macro: 0.3974089953828896, F1 macro: 0.45552269809753054 \n",
      "Precision micro: 0.8359037631091919, Recall micro: 0.7126161397767778, F1 micro: 0.7693520913507034 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.0439282207749784\n",
      "Precision macro: 0.6433929062701963, Recall macro: 0.438569613892987, F1 macro: 0.4971186912674629 \n",
      "Precision micro: 0.8200431034482759, Recall micro: 0.7559749897738561, F1 micro: 0.7867068016662105 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.04145550063624978\n",
      "Precision macro: 0.6859524082839513, Recall macro: 0.4711920499625451, F1 macro: 0.5219410273507507 \n",
      "Precision micro: 0.8135510003682337, Recall micro: 0.774615789166131, F1 micro: 0.7936061304517017 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.041166671853512526\n",
      "Precision macro: 0.7164006300740646, Recall macro: 0.46472902963912693, F1 macro: 0.5257952113898224 \n",
      "Precision micro: 0.8512238479420211, Recall micro: 0.7275170922690353, F1 micro: 0.7845237720155014 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.03967116524837911\n",
      "Precision macro: 0.6850673677024226, Recall macro: 0.4955934722444786, F1 macro: 0.548911182875168 \n",
      "Precision micro: 0.8334922148077534, Recall micro: 0.7663764389645299, F1 micro: 0.7985265465172917 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.03964625882543624\n",
      "Precision macro: 0.6803739819740624, Recall macro: 0.5206886664440606, F1 macro: 0.566508990589557 \n",
      "Precision micro: 0.813460044229275, Recall micro: 0.795301817331853, F1 micro: 0.8042784540834417 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.03887145973742008\n",
      "Precision macro: 0.7136568056429475, Recall macro: 0.5188667693349285, F1 macro: 0.5747818110232307 \n",
      "Precision micro: 0.8320025348542459, Recall micro: 0.7671945304739087, F1 micro: 0.7982853494664519 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.03874803202599287\n",
      "Precision macro: 0.7232103165286865, Recall macro: 0.5523259976702833, F1 macro: 0.5986061364343609 \n",
      "Precision micro: 0.8193843340444986, Recall micro: 0.785484719219307, F1 micro: 0.8020764962109911 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.03808291762135923\n",
      "Precision macro: 0.7422367882485704, Recall macro: 0.49617562520480357, F1 macro: 0.5665030806019291 \n",
      "Precision micro: 0.8486414844267727, Recall micro: 0.7483199906503828, F1 micro: 0.7953296276744403 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.038268345231190326\n",
      "Precision macro: 0.7079938659619338, Recall macro: 0.5255040476230561, F1 macro: 0.5877312804158504 \n",
      "Precision micro: 0.8405107824613981, Recall micro: 0.7538713258925963, F1 micro: 0.7948370402316555 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.037722150501795115\n",
      "Precision macro: 0.6990243172462762, Recall macro: 0.5402305363944176, F1 macro: 0.5896550383373675 \n",
      "Precision micro: 0.8321335484671714, Recall micro: 0.7835563606614854, F1 micro: 0.8071146958798566 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.03777393437363207\n",
      "Precision macro: 0.7342131260058415, Recall macro: 0.5276140052202601, F1 macro: 0.5936549900140922 \n",
      "Precision micro: 0.8248768472906404, Recall micro: 0.7827967042599193, F1 micro: 0.8032860612238779 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.03741930257063359\n",
      "Precision macro: 0.7300181387550964, Recall macro: 0.5247555142244534, F1 macro: 0.5842909544929876 \n",
      "Precision micro: 0.8389278876834716, Recall micro: 0.7681879273067259, F1 micro: 0.8020010371228992 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.03721956433588639\n",
      "Precision macro: 0.7465846161758058, Recall macro: 0.5012467544977267, F1 macro: 0.5718242353650083 \n",
      "Precision micro: 0.8532696447793326, Recall micro: 0.7411324723894116, F1 micro: 0.7932576539387686 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.036856920195743444\n",
      "Precision macro: 0.7333671979469408, Recall macro: 0.5117651985903945, F1 macro: 0.5783178517535604 \n",
      "Precision micro: 0.8503843879361325, Recall micro: 0.75626716531292, F1 micro: 0.8005690956328095 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.03694960021972656\n",
      "Precision macro: 0.7405502156911369, Recall macro: 0.5426578406400067, F1 macro: 0.5998353330266057 \n",
      "Precision micro: 0.8268854064642507, Recall micro: 0.78934143633495, F1 micro: 0.8076773595623187 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.03631483139656484\n",
      "Precision macro: 0.7677678521226011, Recall macro: 0.5762926641186915, F1 macro: 0.6347353396300336 \n",
      "Precision micro: 0.8338843490736698, Recall micro: 0.7811605212411616, F1 micro: 0.8066618392469225 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.03657861806079745\n",
      "Precision macro: 0.7329486650410337, Recall macro: 0.5382576339342632, F1 macro: 0.5932267072477413 \n",
      "Precision micro: 0.8433765900038546, Recall micro: 0.767136095366096, F1 micro: 0.8034517580097311 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 40, 'dropout_rate': 0, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.14253009344637393\n",
      "Precision macro: 0.08200019513641747, Recall macro: 0.0552004628407887, F1 macro: 0.05878903917840519 \n",
      "Precision micro: 0.7668377164849263, Recall micro: 0.27943668556068485, F1 micro: 0.409610689965309 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.11574636036157608\n",
      "Precision macro: 0.13825829295652622, Recall macro: 0.08820664751913755, F1 macro: 0.09856946557933863 \n",
      "Precision micro: 0.8156235404016815, Recall micro: 0.4081692280722258, F1 micro: 0.5440666744557386 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.07166537376865745\n",
      "Precision macro: 0.24398103876876578, Recall macro: 0.1276465682194606, F1 macro: 0.14598977128175597 \n",
      "Precision micro: 0.8187722989662428, Recall micro: 0.5229942149243265, F1 micro: 0.6382826986164598 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.06902579127065837\n",
      "Precision macro: 0.2719979382102876, Recall macro: 0.15582256190579902, F1 macro: 0.1769902165466558 \n",
      "Precision micro: 0.812046383582214, Recall micro: 0.5688073394495413, F1 micro: 0.6690034364261168 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.06223594386875629\n",
      "Precision macro: 0.342815606632342, Recall macro: 0.18135631524155313, F1 macro: 0.20974580738599335 \n",
      "Precision micro: 0.8228013029315961, Recall micro: 0.5904283293402677, F1 micro: 0.6875106317830777 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.06078484490327537\n",
      "Precision macro: 0.40673455848186574, Recall macro: 0.19171123533983866, F1 macro: 0.22677893339806882 \n",
      "Precision micro: 0.8387444988790168, Recall micro: 0.5902530240168293, F1 micro: 0.6928934010152286 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.05558268977329135\n",
      "Precision macro: 0.43265167258804743, Recall macro: 0.23455206364919234, F1 macro: 0.27419452255986393 \n",
      "Precision micro: 0.828042328042328, Recall micro: 0.6401566060889382, F1 micro: 0.7220775796724122 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.054655437129549685\n",
      "Precision macro: 0.4447362778846639, Recall macro: 0.27466164232663187, F1 macro: 0.316461611075596 \n",
      "Precision micro: 0.822673997974244, Recall micro: 0.6644656109390522, F1 micro: 0.7351543559075481 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.052394422421231865\n",
      "Precision macro: 0.4659602682464279, Recall macro: 0.28748916194843144, F1 macro: 0.33491834949838356 \n",
      "Precision micro: 0.8219324497648568, Recall micro: 0.6740489686203471, F1 micro: 0.7406812855170642 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.051489351969212296\n",
      "Precision macro: 0.5364493881639792, Recall macro: 0.3024872939411431, F1 macro: 0.34888916166094613 \n",
      "Precision micro: 0.8394798707023214, Recall micro: 0.6677379769765676, F1 micro: 0.7438242473555735 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04961693079024553\n",
      "Precision macro: 0.535683712223202, Recall macro: 0.3205057977225398, F1 macro: 0.3694991309038802 \n",
      "Precision micro: 0.8373684969015708, Recall micro: 0.6790743878922456, F1 micro: 0.7499596657093994 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04916033287532628\n",
      "Precision macro: 0.5474360779377129, Recall macro: 0.3317463310881526, F1 macro: 0.37697438746199213 \n",
      "Precision micro: 0.8326330532212886, Recall micro: 0.6947934318938819, F1 micro: 0.7574937087885835 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04728434949927032\n",
      "Precision macro: 0.5598536955569267, Recall macro: 0.3477639206494668, F1 macro: 0.39372942175595255 \n",
      "Precision micro: 0.8307240704500979, Recall micro: 0.6945596914626307, F1 micro: 0.7565640813468699 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04753418208938092\n",
      "Precision macro: 0.5399734866083853, Recall macro: 0.3498262054395123, F1 macro: 0.39977201793704575 \n",
      "Precision micro: 0.8286492004083021, Recall micro: 0.711564307836148, F1 micro: 0.7656564386317908 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.046872636234387755\n",
      "Precision macro: 0.5292611316856503, Recall macro: 0.35979719396457566, F1 macro: 0.4055158956282449 \n",
      "Precision micro: 0.838164592760181, Recall micro: 0.6927482031204347, F1 micro: 0.758550084781009 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04640189981739968\n",
      "Precision macro: 0.5619775789574052, Recall macro: 0.35832502766643826, F1 macro: 0.4081718644088378 \n",
      "Precision micro: 0.8406374501992032, Recall micro: 0.7027990416642319, F1 micro: 0.7655633354551242 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04552429478056729\n",
      "Precision macro: 0.5685315472745223, Recall macro: 0.3690617845857906, F1 macro: 0.4186955597805131 \n",
      "Precision micro: 0.8341023205319582, Recall micro: 0.7183427803424297, F1 micro: 0.7719066905277698 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04537132203485817\n",
      "Precision macro: 0.564677156500458, Recall macro: 0.3757462892490011, F1 macro: 0.42477194037012134 \n",
      "Precision micro: 0.8318494536624849, Recall micro: 0.7206801846549407, F1 micro: 0.7722846676477034 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04443261796422303\n",
      "Precision macro: 0.5709388423524746, Recall macro: 0.3845559698201091, F1 macro: 0.4345714602166428 \n",
      "Precision micro: 0.835603506405934, Recall micro: 0.7241278560158944, F1 micro: 0.7758820398835425 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.044290370205417276\n",
      "Precision macro: 0.5725508988355, Recall macro: 0.3810811651456722, F1 macro: 0.43301846324341314 \n",
      "Precision micro: 0.8412928123492522, Recall micro: 0.7133757961783439, F1 micro: 0.7720718441689856 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 40, 'dropout_rate': 0, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.0946200914233923\n",
      "Precision macro: 0.23488790766653855, Recall macro: 0.14119893490710134, F1 macro: 0.15767493264650892 \n",
      "Precision micro: 0.8144728743515344, Recall micro: 0.5412844036697247, F1 micro: 0.6503545601348031 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.0795295186676085\n",
      "Precision macro: 0.38492047127938117, Recall macro: 0.2330208620939921, F1 macro: 0.26984006480120654 \n",
      "Precision micro: 0.8216473243068988, Recall micro: 0.5957459241512301, F1 micro: 0.6906947596626131 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.05378210987709463\n",
      "Precision macro: 0.5064700418097888, Recall macro: 0.2964760837797203, F1 macro: 0.34322240524737424 \n",
      "Precision micro: 0.8235251850255084, Recall micro: 0.6697247706422018, F1 micro: 0.7387044795359329 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.052897623672150076\n",
      "Precision macro: 0.5583558120589491, Recall macro: 0.325396468497033, F1 macro: 0.37064147046518897 \n",
      "Precision micro: 0.8311594202898551, Recall micro: 0.6702506866125169, F1 micro: 0.742082618962896 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.04950794888101518\n",
      "Precision macro: 0.5735606601300769, Recall macro: 0.34679229811487106, F1 macro: 0.4013507181726185 \n",
      "Precision micro: 0.8400889590357988, Recall micro: 0.6842751124875825, F1 micro: 0.7542187298724721 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.04894057007320225\n",
      "Precision macro: 0.5445329370451694, Recall macro: 0.35241551189408316, F1 macro: 0.4067456860332337 \n",
      "Precision micro: 0.8280624223173595, Recall micro: 0.7007538128907848, F1 micro: 0.7591074537110302 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.047400134824216365\n",
      "Precision macro: 0.5647776336631982, Recall macro: 0.3805278880558334, F1 macro: 0.4358405218896477 \n",
      "Precision micro: 0.8132832080200502, Recall micro: 0.7205633144393151, F1 micro: 0.7641208365608056 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.047155186164192855\n",
      "Precision macro: 0.584320330753069, Recall macro: 0.4017877272306654, F1 macro: 0.4534379045530761 \n",
      "Precision micro: 0.8230312729212509, Recall micro: 0.7212645357330685, F1 micro: 0.7687947679850513 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04635783175751567\n",
      "Precision macro: 0.5849277780911019, Recall macro: 0.42001557491973485, F1 macro: 0.45741507170798423 \n",
      "Precision micro: 0.807625434674615, Recall micro: 0.7600070122129375, F1 micro: 0.783092994550983 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04600451347976923\n",
      "Precision macro: 0.6072686049383627, Recall macro: 0.3805881908228454, F1 macro: 0.4367970947702627 \n",
      "Precision micro: 0.8438788568178587, Recall micro: 0.6936247297376263, F1 micro: 0.7614099233458417 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.045545316915959116\n",
      "Precision macro: 0.6068478425458583, Recall macro: 0.4182221623955893, F1 macro: 0.4626835903635213 \n",
      "Precision micro: 0.8355626034197463, Recall micro: 0.7081750715830071, F1 micro: 0.7666128981244268 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.045479980200529097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.5988624980782887, Recall macro: 0.40758248892859966, F1 macro: 0.4629935850094855 \n",
      "Precision micro: 0.8242622950819672, Recall micro: 0.7345293052065681, F1 micro: 0.7768130272224454 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.0451319096442312\n",
      "Precision macro: 0.6246148517991488, Recall macro: 0.3904141065509503, F1 macro: 0.4500549550693489 \n",
      "Precision micro: 0.8469576343474552, Recall micro: 0.6962543095892012, F1 micro: 0.764247458388121 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.045149185807444155\n",
      "Precision macro: 0.5852519691600826, Recall macro: 0.4023786178679728, F1 macro: 0.45487589728858224 \n",
      "Precision micro: 0.832366473294659, Recall micro: 0.7294454508268567, F1 micro: 0.7775147928994083 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04508294541761279\n",
      "Precision macro: 0.633269863640506, Recall macro: 0.40315591530881867, F1 macro: 0.4613627635230595 \n",
      "Precision micro: 0.8443966115817247, Recall micro: 0.7106293461111436, F1 micro: 0.7717594796128828 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04484926872327924\n",
      "Precision macro: 0.6398566246380697, Recall macro: 0.41208143169930356, F1 macro: 0.4677675133249488 \n",
      "Precision micro: 0.840550496976437, Recall micro: 0.706655758779875, F1 micro: 0.7678095238095237 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04465026443824172\n",
      "Precision macro: 0.5835454499746245, Recall macro: 0.4128183248939031, F1 macro: 0.4700367252976911 \n",
      "Precision micro: 0.8447218108737042, Recall micro: 0.6999941564892187, F1 micro: 0.7655780660829552 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04465328386798501\n",
      "Precision macro: 0.6070776152078591, Recall macro: 0.41484637629614923, F1 macro: 0.470798199068023 \n",
      "Precision micro: 0.8412281308346587, Recall micro: 0.7108630865423947, F1 micro: 0.7705707227465637 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.044674004655331376\n",
      "Precision macro: 0.6022320148685033, Recall macro: 0.4151248442107762, F1 macro: 0.46832247551048095 \n",
      "Precision micro: 0.8305027857957978, Recall micro: 0.7229591538596388, F1 micro: 0.773008434864105 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04462061639223248\n",
      "Precision macro: 0.6176136649708168, Recall macro: 0.40604000950494135, F1 macro: 0.4603119650898325 \n",
      "Precision micro: 0.8404160678847602, Recall micro: 0.7176415590486764, F1 micro: 0.7741915148458679 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 40, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.15617915404587984\n",
      "Precision macro: 0.05544415368517783, Recall macro: 0.040719281560013525, F1 macro: 0.046309937080149674 \n",
      "Precision micro: 0.7494969818913481, Recall micro: 0.21767077660258283, F1 micro: 0.3373635828465335 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.12839598730951549\n",
      "Precision macro: 0.10984420872862265, Recall macro: 0.0704027338564841, F1 macro: 0.07550513507084974 \n",
      "Precision micro: 0.7382673691949516, Recall micro: 0.35207152457196284, F1 micro: 0.476774550921896 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.08094483255967498\n",
      "Precision macro: 0.1512040567458635, Recall macro: 0.10725111882553072, F1 macro: 0.11783604202635352 \n",
      "Precision micro: 0.8010960975148823, Recall micro: 0.4954128440366973, F1 micro: 0.6122183708838821 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.07766573458723725\n",
      "Precision macro: 0.2391621109482719, Recall macro: 0.12580858646375484, F1 macro: 0.14101515062606362 \n",
      "Precision micro: 0.8203445447087777, Recall micro: 0.5259159703149653, F1 micro: 0.64093433983763 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.06843894310668111\n",
      "Precision macro: 0.2843799188892746, Recall macro: 0.16303145664226973, F1 macro: 0.18592997654963303 \n",
      "Precision micro: 0.8305041588864369, Recall micro: 0.5717875299479928, F1 micro: 0.6772798061948434 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.06636948460713028\n",
      "Precision macro: 0.34602569797346294, Recall macro: 0.16649226546016005, F1 macro: 0.19411323640943132 \n",
      "Precision micro: 0.8400924024640657, Recall micro: 0.573774323613627, F1 micro: 0.6818513246067844 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.06145637621358037\n",
      "Precision macro: 0.3734925649136296, Recall macro: 0.21119508439199794, F1 macro: 0.24612264771113634 \n",
      "Precision micro: 0.842896174863388, Recall micro: 0.6129258458481855, F1 micro: 0.709747267990662 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.06031193970330059\n",
      "Precision macro: 0.456679061053976, Recall macro: 0.22037934845028548, F1 macro: 0.25501309546491435 \n",
      "Precision micro: 0.8290970154950004, Recall micro: 0.6347221410623503, F1 micro: 0.7190044350301185 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.05750603926181793\n",
      "Precision macro: 0.43887568412286215, Recall macro: 0.24525750667329058, F1 macro: 0.28724519591976705 \n",
      "Precision micro: 0.8402038332826285, Recall micro: 0.6455326360077135, F1 micro: 0.7301146690459668 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.05688401149120182\n",
      "Precision macro: 0.46591866262186143, Recall macro: 0.25600042365704256, F1 macro: 0.3023897509375637 \n",
      "Precision micro: 0.8489319194713385, Recall micro: 0.6455910711155262, F1 micro: 0.7334284860756132 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.05513695058040321\n",
      "Precision macro: 0.4808979043756393, Recall macro: 0.273474440553189, F1 macro: 0.32089929326154193 \n",
      "Precision micro: 0.843322818086225, Recall micro: 0.6561093905218255, F1 micro: 0.7380287244881191 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.05474425088986754\n",
      "Precision macro: 0.5318507983098931, Recall macro: 0.28538459079216183, F1 macro: 0.3352359703664881 \n",
      "Precision micro: 0.8450052262206958, Recall micro: 0.6613685502249752, F1 micro: 0.74199364080375 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.05370459087193012\n",
      "Precision macro: 0.5599505781875875, Recall macro: 0.30434152708682255, F1 macro: 0.3577642423938956 \n",
      "Precision micro: 0.8478502183082958, Recall micro: 0.6694910302109507, F1 micro: 0.7481878142754521 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.05316452063154429\n",
      "Precision macro: 0.5360467889237249, Recall macro: 0.30147853931200236, F1 macro: 0.35635664100905046 \n",
      "Precision micro: 0.8483842408145197, Recall micro: 0.6719453047390872, F1 micro: 0.7499266312322691 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.0525484127048403\n",
      "Precision macro: 0.5514843691901218, Recall macro: 0.32121070378105115, F1 macro: 0.3801510712344728 \n",
      "Precision micro: 0.8528798928411966, Recall micro: 0.6697247706422018, F1 micro: 0.7502864063369448 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.052285337312147025\n",
      "Precision macro: 0.5459511862974401, Recall macro: 0.3240812319252979, F1 macro: 0.3788368540257871 \n",
      "Precision micro: 0.8454979043214337, Recall micro: 0.6836907614094548, F1 micro: 0.7560337307356789 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.05124884437210858\n",
      "Precision macro: 0.561741559025437, Recall macro: 0.3270502343518966, F1 macro: 0.38391488794276846 \n",
      "Precision micro: 0.8451617500356278, Recall micro: 0.6930988137673114, F1 micro: 0.7616142806690853 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.051232802851125595\n",
      "Precision macro: 0.5538365097523056, Recall macro: 0.3428669414353511, F1 macro: 0.3962769224348772 \n",
      "Precision micro: 0.8392495989957459, Recall micro: 0.7032080874189213, F1 micro: 0.7652295561490525 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.050672131204977634\n",
      "Precision macro: 0.5515585392038147, Recall macro: 0.34492958354369146, F1 macro: 0.3977829441345793 \n",
      "Precision micro: 0.8426367461430575, Recall micro: 0.7021562554782913, F1 micro: 0.7660089886207886 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.05042958003375679\n",
      "Precision macro: 0.5475829685477326, Recall macro: 0.35076861480597177, F1 macro: 0.404195045138867 \n",
      "Precision micro: 0.8435754189944135, Recall micro: 0.7058961023783089, F1 micro: 0.7686189673273313 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 40, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.09803991762176156\n",
      "Precision macro: 0.30762777740602626, Recall macro: 0.14310854770093281, F1 macro: 0.17082481388811724 \n",
      "Precision micro: 0.8431011061517563, Recall micro: 0.5077426517851925, F1 micro: 0.6337940843940333 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.08205952916853129\n",
      "Precision macro: 0.377908636679229, Recall macro: 0.1986125644978971, F1 macro: 0.23622834648741886 \n",
      "Precision micro: 0.8388254486133768, Recall micro: 0.6009466487465669, F1 micro: 0.700234909610867 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.05984596536308527\n",
      "Precision macro: 0.466277683763165, Recall macro: 0.28347055767618556, F1 macro: 0.3254842674086581 \n",
      "Precision micro: 0.8222954106107253, Recall micro: 0.6711272132297084, F1 micro: 0.739060489060489 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.05881220149807632\n",
      "Precision macro: 0.5344880450876562, Recall macro: 0.2662284017303579, F1 macro: 0.3188390814327839 \n",
      "Precision micro: 0.8368389423076923, Recall micro: 0.6509671010343014, F1 micro: 0.7322925225965489 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.056550253948196766\n",
      "Precision macro: 0.5216366921688649, Recall macro: 0.29469839209150844, F1 macro: 0.3464098739927599 \n",
      "Precision micro: 0.8240525089985179, Recall micro: 0.6822883188219482, F1 micro: 0.7464995844255482 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.05626996062323451\n",
      "Precision macro: 0.5755008726988217, Recall macro: 0.3064211938464563, F1 macro: 0.3628749831005791 \n",
      "Precision micro: 0.8556779339156856, Recall micro: 0.6582714895108982, F1 micro: 0.7441046304247309 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.05471096637658775\n",
      "Precision macro: 0.5481000547084798, Recall macro: 0.3445697221526383, F1 macro: 0.39422898745714385 \n",
      "Precision micro: 0.8446714482358908, Recall micro: 0.6812949219891311, F1 micro: 0.7542372881355932 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.054577374445274475\n",
      "Precision macro: 0.5485084572524085, Recall macro: 0.36120740293606274, F1 macro: 0.4110610367382885 \n",
      "Precision micro: 0.8230274297324754, Recall micro: 0.7101034301408287, F1 micro: 0.7624066754501538 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.054017339061945675\n",
      "Precision macro: 0.5510177095630335, Recall macro: 0.3591232821292802, F1 macro: 0.4065175684590518 \n",
      "Precision micro: 0.835096453018046, Recall micro: 0.7057792321626833, F1 micro: 0.7650114010640995 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.05380535666458309\n",
      "Precision macro: 0.5601666453877057, Recall macro: 0.36225931832109093, F1 macro: 0.41406758689180384 \n",
      "Precision micro: 0.8276659412404788, Recall micro: 0.7111552620814585, F1 micro: 0.764999842851306 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.05263447101414204\n",
      "Precision macro: 0.5464349062427455, Recall macro: 0.3580048973131176, F1 macro: 0.41057542719664464 \n",
      "Precision micro: 0.8383447880870561, Recall micro: 0.6842751124875825, F1 micro: 0.7535150091695892 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.05294366324041039\n",
      "Precision macro: 0.6199791794129726, Recall macro: 0.36672815406724557, F1 macro: 0.4188864973520387 \n",
      "Precision micro: 0.8105358647055012, Recall micro: 0.7309647636299889, F1 micro: 0.7686966140232288 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.05267958332598209\n",
      "Precision macro: 0.5771980246847102, Recall macro: 0.37397218710965896, F1 macro: 0.4262429483639795 \n",
      "Precision micro: 0.8393214261085442, Recall micro: 0.7112136971892713, F1 micro: 0.7699753273866008 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.05229822675976902\n",
      "Precision macro: 0.5773620704108735, Recall macro: 0.3868190225123204, F1 macro: 0.42858627939656935 \n",
      "Precision micro: 0.804905803514983, Recall micro: 0.7439957926722375, F1 micro: 0.7732531657101213 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.05176104240864515\n",
      "Precision macro: 0.5696252139000134, Recall macro: 0.38983417778182816, F1 macro: 0.4339622514592076 \n",
      "Precision micro: 0.8246701319472212, Recall micro: 0.7231344591830772, F1 micro: 0.7705719356144338 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.05193949448410422\n",
      "Precision macro: 0.6162946471665096, Recall macro: 0.38094979616670366, F1 macro: 0.4348797489006922 \n",
      "Precision micro: 0.8457134779843098, Recall micro: 0.6992345000876526, F1 micro: 0.7655300364659969 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.051657518900930885\n",
      "Precision macro: 0.5826768322276762, Recall macro: 0.3794173176975063, F1 macro: 0.4318985669584087 \n",
      "Precision micro: 0.835363778670115, Recall micro: 0.7172325132939871, F1 micro: 0.7718040621266428 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.05148446929641068\n",
      "Precision macro: 0.6367873777400918, Recall macro: 0.35905956763569374, F1 macro: 0.417846809425133 \n",
      "Precision micro: 0.8211566165877753, Recall micro: 0.6994682405189038, F1 micro: 0.7554433575260333 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.051468664752319454\n",
      "Precision macro: 0.5824314652299498, Recall macro: 0.36657880847015406, F1 macro: 0.42319653197077733 \n",
      "Precision micro: 0.840540726946686, Recall micro: 0.7121486589142757, F1 micro: 0.7710363153232949 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.05149946888815612\n",
      "Precision macro: 0.5767889505534917, Recall macro: 0.3789357941397684, F1 macro: 0.4310463493915972 \n",
      "Precision micro: 0.8330915371329879, Recall micro: 0.7046689651142406, F1 micro: 0.7635177915664175 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 40, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.14970517155528068\n",
      "Precision macro: 0.036115452381431976, Recall macro: 0.02782510493509704, F1 macro: 0.03130411365204207 \n",
      "Precision micro: 0.7234270414993307, Recall micro: 0.15789166131011512, F1 micro: 0.2592095165003837 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.12683161756768824\n",
      "Precision macro: 0.11548331242191759, Recall macro: 0.0707698112509435, F1 macro: 0.08100838322823346 \n",
      "Precision micro: 0.783623417721519, Recall micro: 0.3472798457313154, F1 micro: 0.48127302911284775 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.08106971253827214\n",
      "Precision macro: 0.157182809262954, Recall macro: 0.10272208131418825, F1 macro: 0.11338687523851722 \n",
      "Precision micro: 0.8301353261967279, Recall micro: 0.48033658622100156, F1 micro: 0.6085508051082732 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.07820236770063639\n",
      "Precision macro: 0.2162256608297347, Recall macro: 0.12002425792773401, F1 macro: 0.13430597723819127 \n",
      "Precision micro: 0.840766179742672, Recall micro: 0.5078595220008181, F1 micro: 0.633224043715847 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.0700998570881784\n",
      "Precision macro: 0.2672766881830805, Recall macro: 0.14519178089494456, F1 macro: 0.16728471795192343 \n",
      "Precision micro: 0.8471471746496932, Recall micro: 0.5405247472681587, F1 micro: 0.6599600456621004 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.06847161103598773\n",
      "Precision macro: 0.31299489496761806, Recall macro: 0.1703356788145982, F1 macro: 0.19677325380105473 \n",
      "Precision micro: 0.8451013069029033, Recall micro: 0.5630222637760767, F1 micro: 0.6758083748334153 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.06356255627796054\n",
      "Precision macro: 0.3716229272528117, Recall macro: 0.18729710187367232, F1 macro: 0.22176738470182825 \n",
      "Precision micro: 0.8493127439334804, Recall micro: 0.5849354292058668, F1 micro: 0.692757534862798 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.06302618626877665\n",
      "Precision macro: 0.4278723095910054, Recall macro: 0.20300982964962014, F1 macro: 0.24264031306028908 \n",
      "Precision micro: 0.8380830055390543, Recall micro: 0.6100625255653597, F1 micro: 0.7061210686506594 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.06042699712701142\n",
      "Precision macro: 0.44476740486178473, Recall macro: 0.22249377357408412, F1 macro: 0.2675593160814762 \n",
      "Precision micro: 0.8520772476468679, Recall micro: 0.6136270671419389, F1 micro: 0.713455854876516 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.06000963697768748\n",
      "Precision macro: 0.4595298056144401, Recall macro: 0.23593236767027603, F1 macro: 0.2843311728484528 \n",
      "Precision micro: 0.856945908685427, Recall micro: 0.617483784257582, F1 micro: 0.7177693248199972 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.058148860268294814\n",
      "Precision macro: 0.4820266205947656, Recall macro: 0.2561266994222811, F1 macro: 0.3071616470193435 \n",
      "Precision micro: 0.8537198523057585, Recall micro: 0.6350143166014142, F1 micro: 0.7283023926010321 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.05788129392825067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.47496203035714585, Recall macro: 0.2768662221316592, F1 macro: 0.32912426459429617 \n",
      "Precision micro: 0.8510605356866388, Recall micro: 0.6424355752936364, F1 micro: 0.7321767506909527 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.05635429961234331\n",
      "Precision macro: 0.49319697154471026, Recall macro: 0.28033381706461974, F1 macro: 0.33147243519990177 \n",
      "Precision micro: 0.8433996113021378, Recall micro: 0.659323321451528, F1 micro: 0.7400872388573678 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.05623367530293763\n",
      "Precision macro: 0.546822995967095, Recall macro: 0.29510115115770896, F1 macro: 0.3498079959280013 \n",
      "Precision micro: 0.8564428451114003, Recall micro: 0.653655115993689, F1 micro: 0.7414330218068536 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.055470702858641745\n",
      "Precision macro: 0.49248917082003785, Recall macro: 0.2988849242736315, F1 macro: 0.3541815476832773 \n",
      "Precision micro: 0.8492570579494799, Recall micro: 0.6679717174078186, F1 micro: 0.747783992411605 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.055204884155653416\n",
      "Precision macro: 0.5264609853092415, Recall macro: 0.3026244673277477, F1 macro: 0.3599639238829879 \n",
      "Precision micro: 0.8592350887843375, Recall micro: 0.661660725764039, F1 micro: 0.7476148030768214 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.05408548931218684\n",
      "Precision macro: 0.49202104053033024, Recall macro: 0.3147721191544088, F1 macro: 0.36893408021769586 \n",
      "Precision micro: 0.8598990127364534, Recall micro: 0.6667445801437504, F1 micro: 0.7511026265551972 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.053983649784699085\n",
      "Precision macro: 0.5379653901409885, Recall macro: 0.3196594273550957, F1 macro: 0.37523336436487204 \n",
      "Precision micro: 0.847442872687704, Recall micro: 0.6826389294688249, F1 micro: 0.7561654476017866 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.05356960642710328\n",
      "Precision macro: 0.5131155319152477, Recall macro: 0.32864067202881014, F1 macro: 0.38240855900489373 \n",
      "Precision micro: 0.8532652316672747, Recall micro: 0.6833401507625781, F1 micro: 0.7589071321954702 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.05318790289014578\n",
      "Precision macro: 0.5101701154086348, Recall macro: 0.3335865729906409, F1 macro: 0.38473307290896064 \n",
      "Precision micro: 0.8586924219910846, Recall micro: 0.6753929761000409, F1 micro: 0.7560919765806431 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 40, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.09984434646368026\n",
      "Precision macro: 0.2218063228057936, Recall macro: 0.12405568256089446, F1 macro: 0.14325640804646492 \n",
      "Precision micro: 0.8689831048772713, Recall micro: 0.47788231169286505, F1 micro: 0.6166490725380787 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.0861182273235172\n",
      "Precision macro: 0.32088645576205066, Recall macro: 0.17683599348895723, F1 macro: 0.20189442051132003 \n",
      "Precision micro: 0.8343911808919325, Recall micro: 0.5838251621574242, F1 micro: 0.6869735620724036 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.0665406203456223\n",
      "Precision macro: 0.47824842661536715, Recall macro: 0.23413963109762131, F1 macro: 0.28554373018822493 \n",
      "Precision micro: 0.8427304402914159, Recall micro: 0.62186641734354, F1 micro: 0.7156450690965335 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.0652498368602246\n",
      "Precision macro: 0.4541913856830393, Recall macro: 0.23638693952475406, F1 macro: 0.2807793795929104 \n",
      "Precision micro: 0.8401537599103067, Recall micro: 0.6130427160638111, F1 micro: 0.7088513513513514 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.06276473545283079\n",
      "Precision macro: 0.48570637624013674, Recall macro: 0.27303911939193576, F1 macro: 0.3276764851554022 \n",
      "Precision micro: 0.8474485502256904, Recall micro: 0.6472856892420966, F1 micro: 0.7339650145772594 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.06222546610981226\n",
      "Precision macro: 0.5341975013122466, Recall macro: 0.2757557562810998, F1 macro: 0.3344332571699882 \n",
      "Precision micro: 0.8244793569601754, Recall micro: 0.659323321451528, F1 micro: 0.7327099162283265 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.06107964814081788\n",
      "Precision macro: 0.5196162575509198, Recall macro: 0.32068231506473494, F1 macro: 0.3703140692115437 \n",
      "Precision micro: 0.8254821825482183, Recall micro: 0.672763396248466, F1 micro: 0.7413393432066967 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.06075087642204016\n",
      "Precision macro: 0.5133343977464683, Recall macro: 0.31845915698084576, F1 macro: 0.37256949550958646 \n",
      "Precision micro: 0.8328572474913938, Recall micro: 0.6644656109390522, F1 micro: 0.7391926152245987 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.05998991365358233\n",
      "Precision macro: 0.5439731571546088, Recall macro: 0.3135127088699685, F1 macro: 0.37033721492295085 \n",
      "Precision micro: 0.8284977433913604, Recall micro: 0.6758020218547304, F1 micro: 0.7444001029866117 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.05970653839409351\n",
      "Precision macro: 0.5403030540810334, Recall macro: 0.2937228949442712, F1 macro: 0.3525305803513434 \n",
      "Precision micro: 0.8536735795235525, Recall micro: 0.6470519488108456, F1 micro: 0.7361388113282809 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.05948707355558872\n",
      "Precision macro: 0.542253613429457, Recall macro: 0.314075005966527, F1 macro: 0.3695750454722344 \n",
      "Precision micro: 0.8270874893131946, Recall micro: 0.6783731665984923, F1 micro: 0.7453850845934058 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.05950801670737565\n",
      "Precision macro: 0.5600626671252779, Recall macro: 0.30735145885121146, F1 macro: 0.37054251861260956 \n",
      "Precision micro: 0.8459771818047118, Recall micro: 0.6672704961140653, F1 micro: 0.7460716735813924 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.05857211615890265\n",
      "Precision macro: 0.5376914372877637, Recall macro: 0.31055968945616635, F1 macro: 0.37073357363674597 \n",
      "Precision micro: 0.8553194741669214, Recall micro: 0.6539472915327529, F1 micro: 0.741199456899692 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.058818332815542815\n",
      "Precision macro: 0.5212396763454976, Recall macro: 0.2879866254348765, F1 macro: 0.34413474677502287 \n",
      "Precision micro: 0.8283715391485561, Recall micro: 0.6503827499561736, F1 micro: 0.7286654227634292 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.05895675153285265\n",
      "Precision macro: 0.5396027472942625, Recall macro: 0.2886484483154861, F1 macro: 0.34770561050660975 \n",
      "Precision micro: 0.8274647887323944, Recall micro: 0.6728802664640916, F1 micro: 0.7422089013503497 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.05853491010796279\n",
      "Precision macro: 0.5655550231358586, Recall macro: 0.2934515158337119, F1 macro: 0.35239807039646903 \n",
      "Precision micro: 0.8423168552709946, Recall micro: 0.6611348097937241, F1 micro: 0.7408086429857587 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.05802971943467856\n",
      "Precision macro: 0.5194146472947145, Recall macro: 0.31080579508383643, F1 macro: 0.3651238551502612 \n",
      "Precision micro: 0.8397096188747731, Recall micro: 0.6759188920703558, F1 micro: 0.7489639989639989 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.05801363979652524\n",
      "Precision macro: 0.5305771783235644, Recall macro: 0.3296561422410603, F1 macro: 0.3839151171307618 \n",
      "Precision micro: 0.8378165124453052, Recall micro: 0.6825220592531993, F1 micro: 0.7522380369678624 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.05808690579608083\n",
      "Precision macro: 0.5507154528372492, Recall macro: 0.31912372839973124, F1 macro: 0.37972711127335523 \n",
      "Precision micro: 0.8479336326965522, Recall micro: 0.6510255361421142, F1 micro: 0.736546344043369 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.057759580804035066\n",
      "Precision macro: 0.5471523077791602, Recall macro: 0.30372851009367585, F1 macro: 0.365664490961624 \n",
      "Precision micro: 0.8290757749712974, Recall micro: 0.6751592356687898, F1 micro: 0.7442429707881091 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 80, 'dropout_rate': 0, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.1306956081315875\n",
      "Precision macro: 0.08137949494805756, Recall macro: 0.06350336464462727, F1 macro: 0.06742048851751073 \n",
      "Precision micro: 0.7204682978163641, Recall micro: 0.3200490854905627, F1 micro: 0.4432126239126037 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.10780827636830509\n",
      "Precision macro: 0.18723651441783826, Recall macro: 0.10469812634657762, F1 macro: 0.11864273785384438 \n",
      "Precision micro: 0.8292380755729919, Recall micro: 0.4693507859522001, F1 micro: 0.5994253516922273 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.0647628179192543\n",
      "Precision macro: 0.311365530239528, Recall macro: 0.1674408553324832, F1 macro: 0.19641837123470737 \n",
      "Precision micro: 0.8363334498427123, Recall micro: 0.5592824168760592, F1 micro: 0.6703085057954267 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.06247532264702022\n",
      "Precision macro: 0.3796478922846708, Recall macro: 0.1994410798826686, F1 macro: 0.23173785733286764 \n",
      "Precision micro: 0.8417362270450751, Recall micro: 0.5892596271840121, F1 micro: 0.6932251744405871 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.054594326239079234\n",
      "Precision macro: 0.4393873518094574, Recall macro: 0.2461746646803321, F1 macro: 0.29036588353076803 \n",
      "Precision micro: 0.8350267587062747, Recall micro: 0.629112370712324, F1 micro: 0.7175898153702592 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.053479317501187326\n",
      "Precision macro: 0.4625401889222131, Recall macro: 0.28789038355364244, F1 macro: 0.3366342330306994 \n",
      "Precision micro: 0.8292133197615239, Recall micro: 0.6664524046046865, F1 micro: 0.7389769008973985 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04994681799784303\n",
      "Precision macro: 0.5483337277900525, Recall macro: 0.33020788546602564, F1 macro: 0.37941535803287574 \n",
      "Precision micro: 0.8344645281131671, Recall micro: 0.6825220592531993, F1 micro: 0.7508839601414337 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04927066992223263\n",
      "Precision macro: 0.5377607190958137, Recall macro: 0.34369919389193526, F1 macro: 0.3942650006912844 \n",
      "Precision micro: 0.8435433471404694, Recall micro: 0.6783147314906796, F1 micro: 0.751959577638142 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.0466871437728405\n",
      "Precision macro: 0.5395080238581446, Recall macro: 0.3572188525596815, F1 macro: 0.40716251137222326 \n",
      "Precision micro: 0.8385647025253246, Recall micro: 0.6869046923391574, F1 micro: 0.7551957855513798 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04661868759524077\n",
      "Precision macro: 0.5430315958031032, Recall macro: 0.3786948230565779, F1 macro: 0.4243832397090278 \n",
      "Precision micro: 0.8336963921034718, Recall micro: 0.7156547653830422, F1 micro: 0.7701789139389366 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.044511293658986685\n",
      "Precision macro: 0.5944579095861985, Recall macro: 0.3870921475856434, F1 macro: 0.43620827462543854 \n",
      "Precision micro: 0.8330445214609438, Recall micro: 0.7303804125518611, F1 micro: 0.7783416882025097 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04475610144715756\n",
      "Precision macro: 0.5839933062623799, Recall macro: 0.3858686677123154, F1 macro: 0.44002003513173354 \n",
      "Precision micro: 0.8422164667393675, Recall micro: 0.7220826272424473, F1 micro: 0.7775365738555923 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04304903372563422\n",
      "Precision macro: 0.589394476410025, Recall macro: 0.39519533602932033, F1 macro: 0.446171936475636 \n",
      "Precision micro: 0.8375234019791388, Recall micro: 0.731958160462806, F1 micro: 0.7811905578596153 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.0433533413792029\n",
      "Precision macro: 0.5905231185538292, Recall macro: 0.41173405211569664, F1 macro: 0.4618448858620414 \n",
      "Precision micro: 0.8406050420168067, Recall micro: 0.7306725880909251, F1 micro: 0.781793172439665 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04222026270627975\n",
      "Precision macro: 0.6044025484001864, Recall macro: 0.42325989891549604, F1 macro: 0.4772880414453414 \n",
      "Precision micro: 0.8367320089461913, Recall micro: 0.7432945713784842, F1 micro: 0.7872505028624477 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04226040637306869\n",
      "Precision macro: 0.6093568444912703, Recall macro: 0.4063498394162682, F1 macro: 0.46549628513913865 \n",
      "Precision micro: 0.8524239707606373, Recall micro: 0.7223163676736983, F1 micro: 0.7819953185297653 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04151004351489246\n",
      "Precision macro: 0.6111599217632766, Recall macro: 0.41933933192259615, F1 macro: 0.4715351792524235 \n",
      "Precision micro: 0.8321957501609788, Recall micro: 0.7552153333722901, F1 micro: 0.7918389853873725 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04144251178763807\n",
      "Precision macro: 0.6278070455269261, Recall macro: 0.42650366352219016, F1 macro: 0.48086229853756457 \n",
      "Precision micro: 0.8428703581340029, Recall micro: 0.745398235259744, F1 micro: 0.7911433621732255 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.040698069939389826\n",
      "Precision macro: 0.6249360602088733, Recall macro: 0.4313672344523647, F1 macro: 0.48783197451029653 \n",
      "Precision micro: 0.8406882059364329, Recall micro: 0.7480862502191317, F1 micro: 0.7916885686898982 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04065149666275829\n",
      "Precision macro: 0.6120073626135447, Recall macro: 0.4349462826263604, F1 macro: 0.4832992663108827 \n",
      "Precision micro: 0.8400732888365398, Recall micro: 0.7501899141003915, F1 micro: 0.7925914492977312 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 80, 'dropout_rate': 0, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.0846820057556033\n",
      "Precision macro: 0.3739297653757708, Recall macro: 0.21616017027185272, F1 macro: 0.25273021154464376 \n",
      "Precision micro: 0.8326557163766466, Recall micro: 0.5983755040028049, F1 micro: 0.6963381047907246 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.06952392303291709\n",
      "Precision macro: 0.5298127737428161, Recall macro: 0.32548800476727613, F1 macro: 0.37757151955354096 \n",
      "Precision micro: 0.8445935756296185, Recall micro: 0.6545316426108806, F1 micro: 0.7375144032921811 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.04792605110630393\n",
      "Precision macro: 0.5659025728513692, Recall macro: 0.3958742089023744, F1 macro: 0.4399430811975986 \n",
      "Precision micro: 0.82586831165348, Recall micro: 0.7197452229299363, F1 micro: 0.7691635182814501 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.04698436233960092\n",
      "Precision macro: 0.5761230846448231, Recall macro: 0.3949718302234816, F1 macro: 0.4424883004939933 \n",
      "Precision micro: 0.8130096839959225, Recall micro: 0.7456904107988079, F1 micro: 0.7778963089396204 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.04440797686576843\n",
      "Precision macro: 0.6170963003558225, Recall macro: 0.4297213414306899, F1 macro: 0.4835112502463223 \n",
      "Precision micro: 0.8296085602058784, Recall micro: 0.7158300707064804, F1 micro: 0.7685310078735218 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.04415918899234384\n",
      "Precision macro: 0.6345264811756548, Recall macro: 0.4361434402954656, F1 macro: 0.4856280384008539 \n",
      "Precision micro: 0.8064821834723275, Recall micro: 0.7459241512300591, F1 micro: 0.7750220090464769 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.042435986567288635\n",
      "Precision macro: 0.6479182961899, Recall macro: 0.44125658578803223, F1 macro: 0.4929197214134527 \n",
      "Precision micro: 0.8529717013768767, Recall micro: 0.7203880091158769, F1 micro: 0.7810935817018312 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.042709121936000884\n",
      "Precision macro: 0.6524334205528275, Recall macro: 0.440143742662635, F1 macro: 0.488218446507825 \n",
      "Precision micro: 0.8321728849185497, Recall micro: 0.7403143808800328, F1 micro: 0.7835606271453753 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04189905343949795\n",
      "Precision macro: 0.6092968091224141, Recall macro: 0.44957868943764223, F1 macro: 0.49760617556492265 \n",
      "Precision micro: 0.837138611186166, Recall micro: 0.7241862911237071, F1 micro: 0.7765767459347684 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04188083265721798\n",
      "Precision macro: 0.6520320556207085, Recall macro: 0.45993298033400004, F1 macro: 0.5149019440212984 \n",
      "Precision micro: 0.8344836652862683, Recall micro: 0.7418336936831649, F1 micro: 0.7854358720534553 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.041147052885964513\n",
      "Precision macro: 0.7267095582927676, Recall macro: 0.442367650693958, F1 macro: 0.5177258607646902 \n",
      "Precision micro: 0.8581413593894872, Recall micro: 0.7030912172032957, F1 micro: 0.7729170681569988 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04105341692455113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.6812406583151309, Recall macro: 0.48321145403113214, F1 macro: 0.5336334497678278 \n",
      "Precision micro: 0.8351648351648352, Recall micro: 0.754981592941039, F1 micro: 0.7930515913206273 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04047713184729219\n",
      "Precision macro: 0.6931145424966016, Recall macro: 0.44080582198745905, F1 macro: 0.5023701612844168 \n",
      "Precision micro: 0.8278024562320355, Recall micro: 0.740489686203471, F1 micro: 0.7817155547330433 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04069216725602746\n",
      "Precision macro: 0.6725915018779126, Recall macro: 0.5080584444543695, F1 macro: 0.5560558421413889 \n",
      "Precision micro: 0.838452566096423, Recall micro: 0.7560918599894817, F1 micro: 0.7951451835919495 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.040297376489266755\n",
      "Precision macro: 0.6856330757754305, Recall macro: 0.48352526263456247, F1 macro: 0.5382338034722454 \n",
      "Precision micro: 0.8210131101126362, Recall micro: 0.7794659031145913, F1 micro: 0.7997002398081535 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.040547504304908215\n",
      "Precision macro: 0.7131286986397835, Recall macro: 0.49589081998475615, F1 macro: 0.5561008049418514 \n",
      "Precision micro: 0.8445281025031371, Recall micro: 0.74720972360194, F1 micro: 0.7928939046319836 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.03983247937820852\n",
      "Precision macro: 0.7143399344485394, Recall macro: 0.4928349287965856, F1 macro: 0.5561733596924893 \n",
      "Precision micro: 0.8378692119179673, Recall micro: 0.7591889207035587, F1 micro: 0.7965909439283854 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.040101177144795656\n",
      "Precision macro: 0.7628471000948174, Recall macro: 0.476024363497313, F1 macro: 0.5496067532849317 \n",
      "Precision micro: 0.8490289970736898, Recall micro: 0.7459825863378718, F1 micro: 0.7941771128184392 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.03971746339648962\n",
      "Precision macro: 0.6873863270734236, Recall macro: 0.4883611151561048, F1 macro: 0.542593483607965 \n",
      "Precision micro: 0.8415603021620214, Recall micro: 0.7551568982644773, F1 micro: 0.7960208198589425 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.03982860639318824\n",
      "Precision macro: 0.715342982373267, Recall macro: 0.5036621160443763, F1 macro: 0.5564694632222046 \n",
      "Precision micro: 0.8381032912709769, Recall micro: 0.7529363641675919, F1 micro: 0.7932403730723059 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 80, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.1285728970170021\n",
      "Precision macro: 0.10857011771925067, Recall macro: 0.07941582353428603, F1 macro: 0.08488074949198246 \n",
      "Precision micro: 0.7468693139248636, Recall micro: 0.40776018231753636, F1 micro: 0.5275173873601451 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.1039684822820127\n",
      "Precision macro: 0.2231686602045378, Recall macro: 0.1190704978423745, F1 macro: 0.13433236388925873 \n",
      "Precision micro: 0.8148423695712824, Recall micro: 0.512008414655525, F1 micro: 0.6288667192995048 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.06539211496710777\n",
      "Precision macro: 0.2974696010447102, Recall macro: 0.18342241913690707, F1 macro: 0.20766934719943173 \n",
      "Precision micro: 0.8290322580645161, Recall micro: 0.585695085607433, F1 micro: 0.6864363250350991 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.06260717432387174\n",
      "Precision macro: 0.36572748547190126, Recall macro: 0.21178145148225305, F1 macro: 0.24972873012613606 \n",
      "Precision micro: 0.84096, Recall micro: 0.6142698533278794, F1 micro: 0.7099584641880254 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.0554932724069804\n",
      "Precision macro: 0.4644774169203256, Recall macro: 0.2466060576889151, F1 macro: 0.2919511048123536 \n",
      "Precision micro: 0.8489057892286203, Recall micro: 0.6392216443639338, F1 micro: 0.7292909763658789 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.05414185064006597\n",
      "Precision macro: 0.5235806404069778, Recall macro: 0.29871164300845754, F1 macro: 0.34618538527521375 \n",
      "Precision micro: 0.8349005029521102, Recall micro: 0.6693157248875125, F1 micro: 0.742994291645044 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.050492138607427475\n",
      "Precision macro: 0.5249118457616042, Recall macro: 0.32640056206119916, F1 macro: 0.3763396323076439 \n",
      "Precision micro: 0.8381713144568861, Recall micro: 0.6867293870157191, F1 micro: 0.7549303012783454 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04992830354440957\n",
      "Precision macro: 0.5389178238243665, Recall macro: 0.3384359960239033, F1 macro: 0.3926526865173569 \n",
      "Precision micro: 0.8346265761396702, Recall micro: 0.7039677438204873, F1 micro: 0.7637493264018765 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04799946712702513\n",
      "Precision macro: 0.5180133632417336, Recall macro: 0.3618944179738089, F1 macro: 0.4109000515905668 \n",
      "Precision micro: 0.8338068181818182, Recall micro: 0.7203295740080641, F1 micro: 0.7729253534815187 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04743390784878284\n",
      "Precision macro: 0.5559275959487824, Recall macro: 0.36647434262740075, F1 macro: 0.4183441496627465 \n",
      "Precision micro: 0.8418269230769231, Recall micro: 0.7162391164611699, F1 micro: 0.7739715214851766 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.0460246475469321\n",
      "Precision macro: 0.5626955402887429, Recall macro: 0.37631161883161784, F1 macro: 0.43099584456417334 \n",
      "Precision micro: 0.85587899543379, Recall micro: 0.7009875533220359, F1 micro: 0.7707282598220309 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04553836730495095\n",
      "Precision macro: 0.5797513888392382, Recall macro: 0.3870402988857423, F1 macro: 0.44254108455181246 \n",
      "Precision micro: 0.8481884557101529, Recall micro: 0.719569917606498, F1 micro: 0.7786032689450224 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04469858526811004\n",
      "Precision macro: 0.571515315864205, Recall macro: 0.39944236424094987, F1 macro: 0.4481292279075743 \n",
      "Precision micro: 0.8304502753482346, Recall micro: 0.7490212119441361, F1 micro: 0.787636721150301 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04437902907840908\n",
      "Precision macro: 0.56905174877386, Recall macro: 0.41764144453362123, F1 macro: 0.46228786738827043 \n",
      "Precision micro: 0.8392470649963927, Recall micro: 0.747735639572255, F1 micro: 0.7908529048207662 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04365091417729854\n",
      "Precision macro: 0.5756998039215818, Recall macro: 0.42111493872500927, F1 macro: 0.46510560303624593 \n",
      "Precision micro: 0.8324915286746372, Recall micro: 0.7608835388301292, F1 micro: 0.7950784636990902 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04346672808844596\n",
      "Precision macro: 0.5809131027038404, Recall macro: 0.41667039517527105, F1 macro: 0.4675345279489756 \n",
      "Precision micro: 0.8519396114007176, Recall micro: 0.735347396715947, F1 micro: 0.7893614352026094 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04281376081705093\n",
      "Precision macro: 0.6058810486360213, Recall macro: 0.4276218599632892, F1 macro: 0.47372930653531237 \n",
      "Precision micro: 0.8391304347826087, Recall micro: 0.7556243791269794, F1 micro: 0.7951910955323925 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.042592930822633206\n",
      "Precision macro: 0.5980822352600209, Recall macro: 0.41894808149047763, F1 macro: 0.46795554400063016 \n",
      "Precision micro: 0.8348259979529171, Recall micro: 0.7625781569566996, F1 micro: 0.7970682546953735 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04203620546683669\n",
      "Precision macro: 0.5706377312179822, Recall macro: 0.42336736046712214, F1 macro: 0.4718279752737887 \n",
      "Precision micro: 0.8441079657669519, Recall micro: 0.7492549523753871, F1 micro: 0.7938581555892642 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04177749356254935\n",
      "Precision macro: 0.5792979651529901, Recall macro: 0.4280553384752784, F1 macro: 0.47601563652682555 \n",
      "Precision micro: 0.8396159086485434, Recall micro: 0.7562087302051073, F1 micro: 0.7957326446535078 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 80, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.08678920882940293\n",
      "Precision macro: 0.3482793406790722, Recall macro: 0.21189762820101202, F1 macro: 0.23652203400458074 \n",
      "Precision micro: 0.7923859781379571, Recall micro: 0.6142114182200666, F1 micro: 0.6920139574692211 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.07226512005273253\n",
      "Precision macro: 0.4943114459107067, Recall macro: 0.32577340389970044, F1 macro: 0.360082265554097 \n",
      "Precision micro: 0.8048643329115703, Recall micro: 0.6690819844562613, F1 micro: 0.7307189125370943 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.051819091457873584\n",
      "Precision macro: 0.557995798365737, Recall macro: 0.3681195541145214, F1 macro: 0.41606328934437414 \n",
      "Precision micro: 0.8290046233342399, Recall micro: 0.7124992695611524, F1 micro: 0.7663492662078502 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.05128204574249685\n",
      "Precision macro: 0.5916097325404469, Recall macro: 0.366687829174172, F1 macro: 0.4196952682593602 \n",
      "Precision micro: 0.8316581196581196, Recall micro: 0.7107462163267692, F1 micro: 0.7664629151175248 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.04885962312854827\n",
      "Precision macro: 0.6039305514268769, Recall macro: 0.3936387003383521, F1 macro: 0.44588129299228957 \n",
      "Precision micro: 0.8420403971242725, Recall micro: 0.7186349558814936, F1 micro: 0.7754587300586416 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.048536005255766214\n",
      "Precision macro: 0.5971683583206504, Recall macro: 0.3804559676578203, F1 macro: 0.42846886363431963 \n",
      "Precision micro: 0.8473446719888927, Recall micro: 0.7132589259627184, F1 micro: 0.7745415318230853 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04711683262884617\n",
      "Precision macro: 0.6398409595303508, Recall macro: 0.4251161282948217, F1 macro: 0.4812107592834081 \n",
      "Precision micro: 0.8347540983606557, Recall micro: 0.7438789224566119, F1 micro: 0.786700862095603 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.047296547511592506\n",
      "Precision macro: 0.6137060980329515, Recall macro: 0.43505186398473056, F1 macro: 0.4842224820732453 \n",
      "Precision micro: 0.8245022581260734, Recall micro: 0.7574358674691755, F1 micro: 0.789547420356947 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04639570519141853\n",
      "Precision macro: 0.6240642240643368, Recall macro: 0.42653443673136915, F1 macro: 0.4795441036193367 \n",
      "Precision micro: 0.8540480488820997, Recall micro: 0.7187518260971192, F1 micro: 0.780580675868634 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04647076601721346\n",
      "Precision macro: 0.6889020152543535, Recall macro: 0.4452844616935579, F1 macro: 0.5025522220667787 \n",
      "Precision micro: 0.8516411978345783, Recall micro: 0.7262315198971542, F1 micro: 0.7839525641834354 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04562491546943784\n",
      "Precision macro: 0.6117756282730362, Recall macro: 0.43253384709862513, F1 macro: 0.48570581432058335 \n",
      "Precision micro: 0.8442190940022684, Recall micro: 0.7394378542628411, F1 micro: 0.7883620958195752 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04601140814367682\n",
      "Precision macro: 0.6145012733091287, Recall macro: 0.4524911372954495, F1 macro: 0.5037833150360318 \n",
      "Precision micro: 0.8394204851752022, Recall micro: 0.7279261380237246, F1 micro: 0.7797076956780271 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04556158686801791\n",
      "Precision macro: 0.6472140002169214, Recall macro: 0.4143601181522577, F1 macro: 0.4690396385226339 \n",
      "Precision micro: 0.8337183972539441, Recall micro: 0.7380354116753346, F1 micro: 0.7829644783336432 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04559713360201567\n",
      "Precision macro: 0.630651463739796, Recall macro: 0.4431670081478644, F1 macro: 0.4897185617290503 \n",
      "Precision micro: 0.8334411802769509, Recall micro: 0.752644188628528, F1 micro: 0.7909847391531305 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04549477517791092\n",
      "Precision macro: 0.6506996976829246, Recall macro: 0.4598151559014841, F1 macro: 0.5069337713865174 \n",
      "Precision micro: 0.8209775195756505, Recall micro: 0.7597148366738736, F1 micro: 0.7891590033081428 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04515595937240869\n",
      "Precision macro: 0.623475688448199, Recall macro: 0.4448805705999552, F1 macro: 0.4985623362065725 \n",
      "Precision micro: 0.8285786898786454, Recall micro: 0.7620522409863846, F1 micro: 0.7939242664069158 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04540629018843174\n",
      "Precision macro: 0.63116698498618, Recall macro: 0.4461409599278495, F1 macro: 0.5041279226684136 \n",
      "Precision micro: 0.8414021164021164, Recall micro: 0.7434114415941098, F1 micro: 0.7893773461979958 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04504263909719884\n",
      "Precision macro: 0.6754099090502518, Recall macro: 0.4453339238348144, F1 macro: 0.5075703191988805 \n",
      "Precision micro: 0.8369137475001612, Recall micro: 0.758078653655116, F1 micro: 0.7955479242043294 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04501040116883814\n",
      "Precision macro: 0.643555300586035, Recall macro: 0.4505623030822034, F1 macro: 0.5008765372854086 \n",
      "Precision micro: 0.8284529505582137, Recall micro: 0.758838310056682, F1 micro: 0.7921190679516895 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04517582178674638\n",
      "Precision macro: 0.6484014164309628, Recall macro: 0.44754631796765715, F1 macro: 0.5018235046855156 \n",
      "Precision micro: 0.8293625649474085, Recall micro: 0.7648571261613978, F1 micro: 0.7958048335613315 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 80, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.1307255734652281\n",
      "Precision macro: 0.11145125998640984, Recall macro: 0.06480852656851303, F1 macro: 0.07384991451720632 \n",
      "Precision micro: 0.7717422298260621, Recall micro: 0.3163092385905452, F1 micro: 0.448708915323082 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.10793099523708224\n",
      "Precision macro: 0.17616969385690692, Recall macro: 0.10111892892623443, F1 macro: 0.11035615658904571 \n",
      "Precision micro: 0.7969518664994695, Recall micro: 0.4827908607491381, F1 micro: 0.6013100436681224 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.06883073539286852\n",
      "Precision macro: 0.3339565128505683, Recall macro: 0.16710629679555586, F1 macro: 0.18938485084438025 \n",
      "Precision micro: 0.8327414465136422, Recall micro: 0.5617951265120085, F1 micro: 0.6709470304975923 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.0662187828887254\n",
      "Precision macro: 0.36152599298324684, Recall macro: 0.19054041494450374, F1 macro: 0.22449073029688463 \n",
      "Precision micro: 0.8531317494600432, Recall micro: 0.5770466896511424, F1 micro: 0.6884411600669269 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.058822714738547804\n",
      "Precision macro: 0.4370062384462233, Recall macro: 0.22493144370547735, F1 macro: 0.26690793864480117 \n",
      "Precision micro: 0.8429269831256899, Recall micro: 0.6246713025185532, F1 micro: 0.7175700620909548 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.05787137019634247\n",
      "Precision macro: 0.45710349810885803, Recall macro: 0.2452621733466015, F1 macro: 0.28806077096365135 \n",
      "Precision micro: 0.8366836076765531, Recall micro: 0.6445392391748963, F1 micro: 0.728148930551888 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.054154972054064276\n",
      "Precision macro: 0.5206256009629405, Recall macro: 0.29045257737021585, F1 macro: 0.33786809265890244 \n",
      "Precision micro: 0.846489577846339, Recall micro: 0.6573365277858938, F1 micro: 0.7400171041378857 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.053644077646546065\n",
      "Precision macro: 0.5225454181969057, Recall macro: 0.30550772143696664, F1 macro: 0.35695982553651 \n",
      "Precision micro: 0.8377697841726619, Recall micro: 0.6804768304797523, F1 micro: 0.7509753974139877 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.05158761010505259\n",
      "Precision macro: 0.4895094229137227, Recall macro: 0.3254218263315166, F1 macro: 0.3705172283988402 \n",
      "Precision micro: 0.8415707996023292, Recall micro: 0.6925144626891837, F1 micro: 0.7598012502003526 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.05081963659264147\n",
      "Precision macro: 0.5524524565607662, Recall macro: 0.3471903086988799, F1 macro: 0.3960517388246989 \n",
      "Precision micro: 0.8374101713653952, Recall micro: 0.7081750715830071, F1 micro: 0.7673895836631313 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04932369111478329\n",
      "Precision macro: 0.5293788115775775, Recall macro: 0.34691293715252997, F1 macro: 0.3976081990332359 \n",
      "Precision micro: 0.8507653969384122, Recall micro: 0.6982411032548355, F1 micro: 0.7669940304255729 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04910696007031947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.535561117235536, Recall macro: 0.3630099184096291, F1 macro: 0.41106968262994 \n",
      "Precision micro: 0.8410951365481186, Recall micro: 0.7144860632267866, F1 micro: 0.7726382306477094 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04822840136662126\n",
      "Precision macro: 0.5512006897512257, Recall macro: 0.36014911716281556, F1 macro: 0.41123044794746116 \n",
      "Precision micro: 0.8457490999723069, Recall micro: 0.7138432770408462, F1 micro: 0.7742180815666889 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.047897883104160426\n",
      "Precision macro: 0.5469268012364696, Recall macro: 0.37289220474289503, F1 macro: 0.4235556266821442 \n",
      "Precision micro: 0.8428610503282276, Recall micro: 0.7202711389002513, F1 micro: 0.7767589879320668 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.0466722111441195\n",
      "Precision macro: 0.5681824096839797, Recall macro: 0.3751030977351963, F1 macro: 0.42303833421959847 \n",
      "Precision micro: 0.8339079697652831, Recall micro: 0.7349383509612575, F1 micro: 0.7813014443236527 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04667680233903229\n",
      "Precision macro: 0.5781103127100052, Recall macro: 0.3712219759626299, F1 macro: 0.4206540065196603 \n",
      "Precision micro: 0.8350016572754392, Recall micro: 0.7360486180097002, F1 micro: 0.7824088452698925 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.045619351333007215\n",
      "Precision macro: 0.5666702588788247, Recall macro: 0.3956460537988519, F1 macro: 0.44453108200026814 \n",
      "Precision micro: 0.8302190721649485, Recall micro: 0.7529363641675919, F1 micro: 0.789691416664113 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04597256585489959\n",
      "Precision macro: 0.5781743707659147, Recall macro: 0.3925526532545007, F1 macro: 0.4417216449681709 \n",
      "Precision micro: 0.848179652943178, Recall micro: 0.7283351837784141, F1 micro: 0.7837022132796782 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04548211664147675\n",
      "Precision macro: 0.5827799578227703, Recall macro: 0.3998596737994091, F1 macro: 0.4514325413921888 \n",
      "Precision micro: 0.8360763228640745, Recall micro: 0.7451060597206802, F1 micro: 0.7879742924236807 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.045400810697115955\n",
      "Precision macro: 0.5954286003616956, Recall macro: 0.3867747274209367, F1 macro: 0.43660516039178016 \n",
      "Precision micro: 0.8389261744966443, Recall micro: 0.7450476246128674, F1 micro: 0.78920491473492 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 80, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.08857277508825064\n",
      "Precision macro: 0.36819918989527434, Recall macro: 0.1970987573542489, F1 macro: 0.22677966700531502 \n",
      "Precision micro: 0.8146534333898169, Recall micro: 0.5899608484777654, F1 micro: 0.684335389412323 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.07482588674873114\n",
      "Precision macro: 0.4888441934292047, Recall macro: 0.31856852475607944, F1 macro: 0.3621555114653125 \n",
      "Precision micro: 0.8297338021094928, Recall micro: 0.6757435867469176, F1 micro: 0.7448631239935588 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.05495106233097613\n",
      "Precision macro: 0.5196056662410223, Recall macro: 0.3364483563251012, F1 macro: 0.3802752851562607 \n",
      "Precision micro: 0.8347475604582096, Recall micro: 0.6898264477297961, F1 micro: 0.7553991361382179 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.053951169968582686\n",
      "Precision macro: 0.5329642919411495, Recall macro: 0.3732758068225943, F1 macro: 0.4220007781011193 \n",
      "Precision micro: 0.8214450981688372, Recall micro: 0.7261146496815286, F1 micro: 0.7708436724565757 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.05140944941714406\n",
      "Precision macro: 0.5868916989685444, Recall macro: 0.36588104290765355, F1 macro: 0.42254758434463874 \n",
      "Precision micro: 0.8453579021970233, Recall micro: 0.6970139659907673, F1 micro: 0.7640521410498671 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.0511282229134813\n",
      "Precision macro: 0.5450041574999225, Recall macro: 0.3900651847828577, F1 macro: 0.434659342290936 \n",
      "Precision micro: 0.8290433859625845, Recall micro: 0.7302635423362356, F1 micro: 0.7765246838785845 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04998413474485278\n",
      "Precision macro: 0.6207770189188989, Recall macro: 0.38499611958522895, F1 macro: 0.4378916871884004 \n",
      "Precision micro: 0.8360622869745372, Recall micro: 0.7310231987378016, F1 micro: 0.7800224466891134 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04972935751639306\n",
      "Precision macro: 0.5600336522724892, Recall macro: 0.4284824378169081, F1 macro: 0.4670023624880874 \n",
      "Precision micro: 0.8180113783294544, Recall micro: 0.7393794191550284, F1 micro: 0.7767103526595256 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04905490200780332\n",
      "Precision macro: 0.6001980338462588, Recall macro: 0.4175444181523904, F1 macro: 0.47007164516670374 \n",
      "Precision micro: 0.8338082300709032, Recall micro: 0.7352889616081342, F1 micro: 0.7814557197863621 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04926635633781552\n",
      "Precision macro: 0.5766147897064775, Recall macro: 0.4293608381871992, F1 macro: 0.47260363983108894 \n",
      "Precision micro: 0.8243734455710733, Recall micro: 0.7553906386957284, F1 micro: 0.7883759224248339 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04885189814679325\n",
      "Precision macro: 0.5725305316171819, Recall macro: 0.4161122255768069, F1 macro: 0.4580470360976214 \n",
      "Precision micro: 0.8169830551662632, Recall micro: 0.7494302576988254, F1 micro: 0.7817500228581878 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04851227933168411\n",
      "Precision macro: 0.6130737116262749, Recall macro: 0.4430131356473277, F1 macro: 0.491831003071243 \n",
      "Precision micro: 0.8226582278481013, Recall micro: 0.7595395313504354, F1 micro: 0.7898398808981253 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.047887836581096056\n",
      "Precision macro: 0.6484212223878949, Recall macro: 0.4240425957870396, F1 macro: 0.4647730564661207 \n",
      "Precision micro: 0.8195986875315497, Recall micro: 0.7590136153801204, F1 micro: 0.7881435636054731 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04812494703941047\n",
      "Precision macro: 0.6188539458732915, Recall macro: 0.4452925013100221, F1 macro: 0.4870546945944113 \n",
      "Precision micro: 0.8224552979086371, Recall micro: 0.760649798398878, F1 micro: 0.7903460837887069 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04753620217368007\n",
      "Precision macro: 0.623258338223272, Recall macro: 0.42808601215866776, F1 macro: 0.4725075974191417 \n",
      "Precision micro: 0.8203566216130466, Recall micro: 0.7554490738035412, F1 micro: 0.7865660744706743 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04755021416489035\n",
      "Precision macro: 0.6265059258003723, Recall macro: 0.4007651005559485, F1 macro: 0.45059792889834677 \n",
      "Precision micro: 0.839135892184713, Recall micro: 0.7422427394378542, F1 micro: 0.7877209302325582 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04775016071088612\n",
      "Precision macro: 0.6277281145029753, Recall macro: 0.42472719406579623, F1 macro: 0.47891556974943245 \n",
      "Precision micro: 0.833268958413802, Recall micro: 0.7563840355285455, F1 micro: 0.7929671945354856 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.047448842382989825\n",
      "Precision macro: 0.6133616405606502, Recall macro: 0.42551358934022954, F1 macro: 0.48011055304779865 \n",
      "Precision micro: 0.8429604085151535, Recall micro: 0.7427686554081693, F1 micro: 0.7896993041749504 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04721644829772413\n",
      "Precision macro: 0.6187298907295613, Recall macro: 0.40715779169304656, F1 macro: 0.454125984250428 \n",
      "Precision micro: 0.8454782489364622, Recall micro: 0.7200373984690002, F1 micro: 0.7777321930129075 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04740832487680018\n",
      "Precision macro: 0.6374849010971942, Recall macro: 0.4446395701516428, F1 macro: 0.4936932315728462 \n",
      "Precision micro: 0.8376390967657968, Recall micro: 0.7521767077660259, F1 micro: 0.7926108374384238 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 120, 'dropout_rate': 0, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.12351649480313062\n",
      "Precision macro: 0.1015303837896247, Recall macro: 0.07475838819776454, F1 macro: 0.08301859972649632 \n",
      "Precision micro: 0.7357531331283992, Recall micro: 0.36364167591889207, F1 micro: 0.4867232411716398 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.10021068346500396\n",
      "Precision macro: 0.23171483960599534, Recall macro: 0.12341549360802881, F1 macro: 0.14042744238071073 \n",
      "Precision micro: 0.8324236330556354, Recall micro: 0.5079763922164436, F1 micro: 0.6309333720423864 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.060278641080483794\n",
      "Precision macro: 0.4266578571930124, Recall macro: 0.19736925235166905, F1 macro: 0.2339525493955252 \n",
      "Precision micro: 0.8361600526922444, Recall micro: 0.5934669549465319, F1 micro: 0.6942137461977511 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.05786326465476304\n",
      "Precision macro: 0.462903390722628, Recall macro: 0.2620368604353917, F1 macro: 0.30975249037373254 \n",
      "Precision micro: 0.8385908358277688, Recall micro: 0.6384619879623677, F1 micro: 0.7249684825160906 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.05105982833728194\n",
      "Precision macro: 0.5012526593278674, Recall macro: 0.30083131575550687, F1 macro: 0.3549667165684775 \n",
      "Precision micro: 0.841747572815534, Recall micro: 0.6586221001577748, F1 micro: 0.7390092777759565 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.049989625660702586\n",
      "Precision macro: 0.5224189550487148, Recall macro: 0.34484924715018433, F1 macro: 0.39191127734123293 \n",
      "Precision micro: 0.8238705327039785, Recall micro: 0.7139601472564717, F1 micro: 0.7649876342234606 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04673972607590258\n",
      "Precision macro: 0.5505714734608772, Recall macro: 0.36301106520179266, F1 macro: 0.41126377988530255 \n",
      "Precision micro: 0.8380311024715357, Recall micro: 0.7053701864079939, F1 micro: 0.7659993019640194 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.046143841446377336\n",
      "Precision macro: 0.5784587935762908, Recall macro: 0.36393948218131267, F1 macro: 0.41907394235182466 \n",
      "Precision micro: 0.8449509116409537, Recall micro: 0.7040846140361129, F1 micro: 0.7681127083798169 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.0443649555388838\n",
      "Precision macro: 0.5517766039662965, Recall macro: 0.40093522227467293, F1 macro: 0.44767642076328157 \n",
      "Precision micro: 0.8404614139233675, Recall micro: 0.7280430082393502, F1 micro: 0.780223565143877 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04382369634136558\n",
      "Precision macro: 0.5904338199632648, Recall macro: 0.3916823448474809, F1 macro: 0.4478485677195325 \n",
      "Precision micro: 0.8514067580941609, Recall micro: 0.703792438497049, F1 micro: 0.7705940689081544 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.042534475617110726\n",
      "Precision macro: 0.5985665028839461, Recall macro: 0.4030631041928642, F1 macro: 0.45658256209037495 \n",
      "Precision micro: 0.8445694152720374, Recall micro: 0.7283936188862269, F1 micro: 0.7821912650602411 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04229521333705634\n",
      "Precision macro: 0.587051630222096, Recall macro: 0.3960815551955219, F1 macro: 0.45166591953977964 \n",
      "Precision micro: 0.841702470461869, Recall micro: 0.7326593817565593, F1 micro: 0.7834046674372832 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04117584003321827\n",
      "Precision macro: 0.6352891461473776, Recall macro: 0.42888484835144686, F1 macro: 0.48101763920701385 \n",
      "Precision micro: 0.8264155696920074, Recall micro: 0.7667270496114066, F1 micro: 0.7954531676265535 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04099917011614889\n",
      "Precision macro: 0.6317016251126906, Recall macro: 0.4320758328816537, F1 macro: 0.4849383076992942 \n",
      "Precision micro: 0.834971546818417, Recall micro: 0.7545141120785368, F1 micro: 0.7927065107284281 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04018768596835434\n",
      "Precision macro: 0.622402263956489, Recall macro: 0.4349796254077843, F1 macro: 0.4888941055234732 \n",
      "Precision micro: 0.8294593389798127, Recall micro: 0.7611172792613803, F1 micro: 0.7938200877620674 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.0400034048743546\n",
      "Precision macro: 0.663670468150428, Recall macro: 0.46966962691124914, F1 macro: 0.525488836514855 \n",
      "Precision micro: 0.8337134169517294, Recall micro: 0.7690644539239175, F1 micro: 0.8000851089698776 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.03932991706766188\n",
      "Precision macro: 0.6564989203302211, Recall macro: 0.4571466347784722, F1 macro: 0.5168166836692714 \n",
      "Precision micro: 0.8522316043425814, Recall micro: 0.7431192660550459, F1 micro: 0.7939441236147963 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.039228454591706395\n",
      "Precision macro: 0.6313084912547027, Recall macro: 0.44625861714003745, F1 macro: 0.49976005457307104 \n",
      "Precision micro: 0.8478604344963792, Recall micro: 0.7525857535207152, F1 micro: 0.7973872395752716 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.03830263116583228\n",
      "Precision macro: 0.6334339906108272, Recall macro: 0.439626399124001, F1 macro: 0.49391716780307815 \n",
      "Precision micro: 0.8434461299105407, Recall micro: 0.7602991877520014, F1 micro: 0.7997172623620886 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.03840258434228599\n",
      "Precision macro: 0.6786439799849906, Recall macro: 0.4599590225327996, F1 macro: 0.5219820225955661 \n",
      "Precision micro: 0.8576147778601766, Recall micro: 0.743353006486297, F1 micro: 0.7964064358605146 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 120, 'dropout_rate': 0, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.07990431266650558\n",
      "Precision macro: 0.45409956103208304, Recall macro: 0.24109000475529135, F1 macro: 0.28021402788552424 \n",
      "Precision micro: 0.8200504317261404, Recall micro: 0.6271255770466897, F1 micro: 0.710728476821192 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.06619284883514047\n",
      "Precision macro: 0.5022348367112431, Recall macro: 0.3818060152222509, F1 macro: 0.4136045548983636 \n",
      "Precision micro: 0.7879992428544388, Recall micro: 0.7297960614737334, F1 micro: 0.7577816880043687 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.04702842919901013\n",
      "Precision macro: 0.6038422689649058, Recall macro: 0.4019191330995639, F1 macro: 0.44451831505467115 \n",
      "Precision micro: 0.8306605055721663, Recall micro: 0.7143107579033483, F1 micro: 0.7681045587357441 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.04639777507167309\n",
      "Precision macro: 0.5677476446122594, Recall macro: 0.40905133560548246, F1 macro: 0.4581164800930233 \n",
      "Precision micro: 0.823603555791429, Recall micro: 0.7254718634955881, F1 micro: 0.7714294590983938 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.043385131508111956\n",
      "Precision macro: 0.605116655187303, Recall macro: 0.4467077214107127, F1 macro: 0.49127507847405627 \n",
      "Precision micro: 0.8098136645962732, Recall micro: 0.7618769356629463, F1 micro: 0.7851142624875802 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.043510482725687326\n",
      "Precision macro: 0.594912100171456, Recall macro: 0.4317804855273698, F1 macro: 0.4858933086684507 \n",
      "Precision micro: 0.8539389237604909, Recall micro: 0.7075322853970666, F1 micro: 0.7738719161447015 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.042254120845347645\n",
      "Precision macro: 0.6505400646384102, Recall macro: 0.4543920002901485, F1 macro: 0.5141108800289688 \n",
      "Precision micro: 0.8446537543114885, Recall micro: 0.744112662887863, F1 micro: 0.7912019634036471 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04226543598715216\n",
      "Precision macro: 0.6866597850246342, Recall macro: 0.4407392910667623, F1 macro: 0.502215487477049 \n",
      "Precision micro: 0.847034127843987, Recall micro: 0.7309647636299889, F1 micro: 0.7847307173551644 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04128270739503205\n",
      "Precision macro: 0.6836783701452096, Recall macro: 0.49420374611127843, F1 macro: 0.539721386003623 \n",
      "Precision micro: 0.8254384322536354, Recall micro: 0.7728627359317478, F1 micro: 0.7982858522452921 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.041289437513798474\n",
      "Precision macro: 0.6788569370533981, Recall macro: 0.48931355039522084, F1 macro: 0.5401884791950846 \n",
      "Precision micro: 0.8280104545164786, Recall micro: 0.7590136153801204, F1 micro: 0.7920121951219512 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04067950893938541\n",
      "Precision macro: 0.667619245903186, Recall macro: 0.4868934608025578, F1 macro: 0.5338349333750201 \n",
      "Precision micro: 0.8211770469716455, Recall micro: 0.7835563606614854, F1 micro: 0.8019257221458046 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.0408408619184047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.683112588186061, Recall macro: 0.4767863620107758, F1 macro: 0.5298161393731088 \n",
      "Precision micro: 0.8346349964500097, Recall micro: 0.7556243791269794, F1 micro: 0.7931669017972153 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04008946685306728\n",
      "Precision macro: 0.6791170310019266, Recall macro: 0.48421268730505107, F1 macro: 0.531809384684331 \n",
      "Precision micro: 0.830791530123519, Recall micro: 0.7703500262957985, F1 micro: 0.7994299748339955 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04007507802825421\n",
      "Precision macro: 0.7075053714527659, Recall macro: 0.48511888268387504, F1 macro: 0.5503568225880637 \n",
      "Precision micro: 0.8429489302510432, Recall micro: 0.7436451820253608, F1 micro: 0.7901893821794473 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.03962388139590621\n",
      "Precision macro: 0.6675610811030623, Recall macro: 0.46522002391074957, F1 macro: 0.5214004250285933 \n",
      "Precision micro: 0.8287602265575834, Recall micro: 0.7695319347864197, F1 micro: 0.7980486622428263 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.039706089675426486\n",
      "Precision macro: 0.7224774478371295, Recall macro: 0.4902448603086171, F1 macro: 0.5508521205969745 \n",
      "Precision micro: 0.8320434402696293, Recall micro: 0.778998422252089, F1 micro: 0.8046476535385545 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.03941585638932884\n",
      "Precision macro: 0.7289579496098902, Recall macro: 0.5144699877848028, F1 macro: 0.5693885679912971 \n",
      "Precision micro: 0.8321279025504378, Recall micro: 0.7664348740723427, F1 micro: 0.7979315589353612 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.03927901573199779\n",
      "Precision macro: 0.6960446747155402, Recall macro: 0.4990443570151149, F1 macro: 0.5551970321078458 \n",
      "Precision micro: 0.8080071706005378, Recall micro: 0.7901595278443289, F1 micro: 0.798983691798629 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.03919845419190824\n",
      "Precision macro: 0.7081870280349928, Recall macro: 0.48232602987634804, F1 macro: 0.5440214678708695 \n",
      "Precision micro: 0.8458567875376358, Recall micro: 0.7551568982644773, F1 micro: 0.7979376987434781 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.03930749430600554\n",
      "Precision macro: 0.7370285064703612, Recall macro: 0.5154326458743038, F1 macro: 0.5787176609355956 \n",
      "Precision micro: 0.8406764286639886, Recall micro: 0.7581955238707415, F1 micro: 0.7973085076965619 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 120, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.12264920829236507\n",
      "Precision macro: 0.12596145322558983, Recall macro: 0.07887159353464003, F1 macro: 0.08650807093958117 \n",
      "Precision micro: 0.7519464853602369, Recall micro: 0.40068953427219073, F1 micro: 0.5227965843244892 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.09946608790569007\n",
      "Precision macro: 0.21603281348394984, Recall macro: 0.12348772129422249, F1 macro: 0.14049625302651952 \n",
      "Precision micro: 0.8384843555640609, Recall micro: 0.5120668497633378, F1 micro: 0.6358293426208097 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.061112030629068614\n",
      "Precision macro: 0.4064857642669025, Recall macro: 0.20500848898904778, F1 macro: 0.24218175843006004 \n",
      "Precision micro: 0.8381863898126368, Recall micro: 0.6038684041372057, F1 micro: 0.7019903539161741 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.0587722990391776\n",
      "Precision macro: 0.4681924149585891, Recall macro: 0.2540387806396731, F1 macro: 0.2967161109609323 \n",
      "Precision micro: 0.8260901591043017, Recall micro: 0.6553497341202594, F1 micro: 0.7308807716119783 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.05165378910303116\n",
      "Precision macro: 0.5273852381016106, Recall macro: 0.3043489832551152, F1 macro: 0.3522406508208485 \n",
      "Precision micro: 0.8461310625694187, Recall micro: 0.6677379769765676, F1 micro: 0.7464236723495983 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.050553192293271425\n",
      "Precision macro: 0.5406894904592326, Recall macro: 0.3468625962797613, F1 macro: 0.3908865479442941 \n",
      "Precision micro: 0.8405313254723682, Recall micro: 0.6914626307485537, F1 micro: 0.7587445096341894 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.047145008012652395\n",
      "Precision macro: 0.5338154013068901, Recall macro: 0.3606028023043884, F1 macro: 0.4132721657230008 \n",
      "Precision micro: 0.8469653280821436, Recall micro: 0.7037340033892363, F1 micro: 0.7687348397804162 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04675626115128398\n",
      "Precision macro: 0.5366977960609716, Recall macro: 0.3701124261415669, F1 macro: 0.42008438423223377 \n",
      "Precision micro: 0.8389330075341072, Recall micro: 0.7222579325658856, F1 micro: 0.77623563398857 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04521126961149275\n",
      "Precision macro: 0.5589622611929438, Recall macro: 0.3969818283569739, F1 macro: 0.4427750713529313 \n",
      "Precision micro: 0.8363612340622316, Recall micro: 0.7397884649097177, F1 micro: 0.7851162790697674 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04454620376881212\n",
      "Precision macro: 0.5821210337135928, Recall macro: 0.40330502529906503, F1 macro: 0.4518635086793939 \n",
      "Precision micro: 0.8383858397728023, Recall micro: 0.7417752585753521, F1 micro: 0.7871271780244311 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04310587429255247\n",
      "Precision macro: 0.5759747071699587, Recall macro: 0.39719120332845376, F1 macro: 0.447360496587458 \n",
      "Precision micro: 0.8517932126385351, Recall micro: 0.7230760240752644, F1 micro: 0.7821744627054361 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.0429223894616589\n",
      "Precision macro: 0.5955668349946023, Recall macro: 0.414858278167085, F1 macro: 0.4654835799036457 \n",
      "Precision micro: 0.8408005755772123, Recall micro: 0.7511833109332087, F1 micro: 0.7934695389173507 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.042102467164397236\n",
      "Precision macro: 0.6278453502686946, Recall macro: 0.4402881359877817, F1 macro: 0.4876756165170307 \n",
      "Precision micro: 0.8418200809505157, Recall micro: 0.7535207152457196, F1 micro: 0.7952267891831889 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04185781270638108\n",
      "Precision macro: 0.5929553375379808, Recall macro: 0.42631497943088886, F1 macro: 0.47074640689163255 \n",
      "Precision micro: 0.8408044112877068, Recall micro: 0.7573774323613627, F1 micro: 0.7969134284308903 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.041043619586154816\n",
      "Precision macro: 0.6194687083418712, Recall macro: 0.44450158922806726, F1 macro: 0.49220364081993945 \n",
      "Precision micro: 0.8383095709992967, Recall micro: 0.7662011336410915, F1 micro: 0.8006350369420528 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04091511633712799\n",
      "Precision macro: 0.6248674425868495, Recall macro: 0.4426552434439849, F1 macro: 0.49476842351782013 \n",
      "Precision micro: 0.831468400325061, Recall micro: 0.7772453690177058, F1 micro: 0.8034430685593477 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04015375380963087\n",
      "Precision macro: 0.6569148468660887, Recall macro: 0.4432122289156195, F1 macro: 0.4953287093005597 \n",
      "Precision micro: 0.8386994749826049, Recall micro: 0.7747910944895693, F1 micro: 0.8054796184921937 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04001159438118339\n",
      "Precision macro: 0.6514234669806847, Recall macro: 0.46313687421159044, F1 macro: 0.5163221318679881 \n",
      "Precision micro: 0.8435011559208836, Recall micro: 0.7675451411207853, F1 micro: 0.8037325990515526 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.039856308629736305\n",
      "Precision macro: 0.6591706877518301, Recall macro: 0.46388749746526803, F1 macro: 0.5190768928357905 \n",
      "Precision micro: 0.8359665135016051, Recall micro: 0.7760766668614504, F1 micro: 0.8049090909090909 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.03962376671191305\n",
      "Precision macro: 0.6424359057339337, Recall macro: 0.46505868447584553, F1 macro: 0.5111869910593915 \n",
      "Precision micro: 0.8270290121189864, Recall micro: 0.7895751767662011, F1 micro: 0.8078682251651669 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 120, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.080486692301929\n",
      "Precision macro: 0.44960141787170577, Recall macro: 0.23913319822498755, F1 macro: 0.28659706971778864 \n",
      "Precision micro: 0.8397640306122449, Recall micro: 0.6155554256997604, F1 micro: 0.7103887783659844 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.06738194921519608\n",
      "Precision macro: 0.5586083962785972, Recall macro: 0.3508016100295773, F1 macro: 0.40309435940549593 \n",
      "Precision micro: 0.8194027815653122, Recall micro: 0.7023315608017296, F1 micro: 0.7563638652024794 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.04830069831199944\n",
      "Precision macro: 0.5810203070671314, Recall macro: 0.39960987418330496, F1 macro: 0.4472243726729613 \n",
      "Precision micro: 0.8176318063958513, Recall micro: 0.7186349558814936, F1 micro: 0.764943708403309 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.048030391653999686\n",
      "Precision macro: 0.6038337633390896, Recall macro: 0.3866744157403936, F1 macro: 0.44145490025930517 \n",
      "Precision micro: 0.8508275717578043, Recall micro: 0.7119149184830246, F1 micro: 0.775197251208959 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.04597555894218385\n",
      "Precision macro: 0.5900267869862037, Recall macro: 0.41228122692043007, F1 macro: 0.46339802272319824 \n",
      "Precision micro: 0.8490787385273618, Recall micro: 0.7189855665283702, F1 micro: 0.7786356157448424 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.045695417338982226\n",
      "Precision macro: 0.6005487756142961, Recall macro: 0.420649856446969, F1 macro: 0.4753033186010496 \n",
      "Precision micro: 0.8375216061693924, Recall micro: 0.7361654882253258, F1 micro: 0.7835795366194992 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.044890888165682555\n",
      "Precision macro: 0.6238439590420543, Recall macro: 0.4564914108728438, F1 macro: 0.5060059100932303 \n",
      "Precision micro: 0.8339023109243697, Recall micro: 0.7422427394378542, F1 micro: 0.7854073272530528 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04464663855731487\n",
      "Precision macro: 0.6219257945200114, Recall macro: 0.48425484964891086, F1 macro: 0.5281655528008351 \n",
      "Precision micro: 0.8345075016307894, Recall micro: 0.7475603342488167, F1 micro: 0.7886446999352712 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04352958635427058\n",
      "Precision macro: 0.6789520869526107, Recall macro: 0.42579718672386, F1 macro: 0.48904663980355706 \n",
      "Precision micro: 0.8492524066361712, Recall micro: 0.7268743060830947, F1 micro: 0.7833123425692696 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04376507507823408\n",
      "Precision macro: 0.645377268617874, Recall macro: 0.44465844687142897, F1 macro: 0.5004593910269827 \n",
      "Precision micro: 0.8353559608021286, Recall micro: 0.7521767077660259, F1 micro: 0.7915872332574871 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04277088153362274\n",
      "Precision macro: 0.6505171294769765, Recall macro: 0.4912507392348497, F1 macro: 0.5373123118603352 \n",
      "Precision micro: 0.8193862459945773, Recall micro: 0.7770116285864548, F1 micro: 0.7976365435949732 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04321477760095149\n",
      "Precision macro: 0.6839235818272537, Recall macro: 0.48609137889310644, F1 macro: 0.5424254527930065 \n",
      "Precision micro: 0.8456021386190259, Recall micro: 0.7578449132238649, F1 micro: 0.799322033898305 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04295063164271414\n",
      "Precision macro: 0.6461531878456336, Recall macro: 0.4331011476056387, F1 macro: 0.47683557144521255 \n",
      "Precision micro: 0.8105430528375733, Recall micro: 0.7744989189505055, F1 micro: 0.7921111609143882 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.042930897977203133\n",
      "Precision macro: 0.6751299516114303, Recall macro: 0.4577034082202331, F1 macro: 0.5148917252650681 \n",
      "Precision micro: 0.8417762729522074, Recall micro: 0.7554490738035412, F1 micro: 0.7962797573219181 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04216151384450495\n",
      "Precision macro: 0.6694212013006712, Recall macro: 0.47944277108679495, F1 macro: 0.5327211759899299 \n",
      "Precision micro: 0.828917629331994, Recall micro: 0.771518728452054, F1 micro: 0.7991888865349113 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.04252746765874326\n",
      "Precision macro: 0.7239065043363779, Recall macro: 0.48419234759131025, F1 macro: 0.536654621490727 \n",
      "Precision micro: 0.8485269778219132, Recall micro: 0.7489627768363233, F1 micro: 0.7956421875969955 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04170668269507587\n",
      "Precision macro: 0.6580298085872184, Recall macro: 0.4879557648266465, F1 macro: 0.5423697734330317 \n",
      "Precision micro: 0.8247709829165635, Recall micro: 0.7786478116052125, F1 micro: 0.8010460188163154 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04235944154113531\n",
      "Precision macro: 0.6556238269437895, Recall macro: 0.4644143099804188, F1 macro: 0.5248021611866057 \n",
      "Precision micro: 0.8401679044236358, Recall micro: 0.7602407526441887, F1 micro: 0.7982084790477944 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.041636345265433194\n",
      "Precision macro: 0.678952865947695, Recall macro: 0.5029160320967709, F1 macro: 0.5507166783166398 \n",
      "Precision micro: 0.8290771175726928, Recall micro: 0.7664348740723427, F1 micro: 0.7965262806303707 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04207003708835691\n",
      "Precision macro: 0.6636692890496001, Recall macro: 0.4739759822908123, F1 macro: 0.5230603602912778 \n",
      "Precision micro: 0.8386365109195775, Recall micro: 0.7562087302051073, F1 micro: 0.7952925270403147 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 120, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.12155552143976092\n",
      "Precision macro: 0.12868883942690865, Recall macro: 0.0824325608067608, F1 macro: 0.08723042932480606 \n",
      "Precision micro: 0.7607282184655396, Recall micro: 0.41021445684567287, F1 micro: 0.5330093770168179 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.09852915585972369\n",
      "Precision macro: 0.2441698051899194, Recall macro: 0.13562201103355942, F1 macro: 0.15376686542040457 \n",
      "Precision micro: 0.8340472067802789, Recall micro: 0.5347981067025068, F1 micro: 0.651712597023428 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.06165664715692401\n",
      "Precision macro: 0.4068675804797918, Recall macro: 0.19732804757350975, F1 macro: 0.23375789036107858 \n",
      "Precision micro: 0.8343730006397952, Recall micro: 0.6096534798106702, F1 micro: 0.7045278049768713 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.05898141754511744\n",
      "Precision macro: 0.4814653111147418, Recall macro: 0.2579161791150236, F1 macro: 0.30387760611517095 \n",
      "Precision micro: 0.8420489296636086, Recall micro: 0.6436042774498919, F1 micro: 0.7295730798529461 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.052806307872757315\n",
      "Precision macro: 0.5320723871200883, Recall macro: 0.32471706225184155, F1 macro: 0.367075316448955 \n",
      "Precision micro: 0.820879270450524, Recall micro: 0.7048442704376789, F1 micro: 0.758449397931273 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.05128385562915355\n",
      "Precision macro: 0.5282912829960315, Recall macro: 0.34097386140630087, F1 macro: 0.39041391752458005 \n",
      "Precision micro: 0.8439558865654541, Recall micro: 0.6886577455735405, F1 micro: 0.7584387167358497 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04807008039206266\n",
      "Precision macro: 0.5552350017296731, Recall macro: 0.36189164900188864, F1 macro: 0.4130357758952601 \n",
      "Precision micro: 0.8380151732622514, Recall micro: 0.716472856892421, F1 micro: 0.772492439516129 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04775018633250147\n",
      "Precision macro: 0.5482502847315183, Recall macro: 0.3802092475053625, F1 macro: 0.4306042047176119 \n",
      "Precision micro: 0.837656620386048, Recall micro: 0.7227254134283878, F1 micro: 0.7759583411757326 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.045624011183157565\n",
      "Precision macro: 0.5522138924262877, Recall macro: 0.3890387682092882, F1 macro: 0.43296501190580944 \n",
      "Precision micro: 0.831519601416022, Recall micro: 0.7411909074972244, F1 micro: 0.7837612382982667 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04543247833661735\n",
      "Precision macro: 0.560269831809089, Recall macro: 0.3971648762115102, F1 macro: 0.44533333062920893 \n",
      "Precision micro: 0.8440716481753776, Recall micro: 0.7379769765675218, F1 micro: 0.7874668745128605 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04426273812539876\n",
      "Precision macro: 0.577607821284132, Recall macro: 0.4040338279368735, F1 macro: 0.4535737536151224 \n",
      "Precision micro: 0.840485564304462, Recall micro: 0.748495295973821, F1 micro: 0.7918276512224522 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04397180628404021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.5895113790184081, Recall macro: 0.4099479648808291, F1 macro: 0.45437199269583234 \n",
      "Precision micro: 0.8421572500987752, Recall micro: 0.7473265938175656, F1 micro: 0.791913062323911 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.0431359302457422\n",
      "Precision macro: 0.5778843548853563, Recall macro: 0.41918017423671566, F1 macro: 0.4640605470245095 \n",
      "Precision micro: 0.8356780368728721, Recall micro: 0.7601823175363759, F1 micro: 0.7961444308445533 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.042893412466160956\n",
      "Precision macro: 0.5943807682086151, Recall macro: 0.4279402025005849, F1 macro: 0.47476112125546793 \n",
      "Precision micro: 0.8481596101929282, Recall micro: 0.7527026237363408, F1 micro: 0.7975851393188854 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04184435125626624\n",
      "Precision macro: 0.6072442351576505, Recall macro: 0.43554200075809973, F1 macro: 0.48076862345043964 \n",
      "Precision micro: 0.8373150579645168, Recall micro: 0.7639221644363934, F1 micro: 0.7989366253132066 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.041851655430160464\n",
      "Precision macro: 0.634691908062758, Recall macro: 0.44296811009526776, F1 macro: 0.4913004008869801 \n",
      "Precision micro: 0.8425776457674178, Recall micro: 0.7625197218488868, F1 micro: 0.8005521472392637 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.040960582099854946\n",
      "Precision macro: 0.6301467495277969, Recall macro: 0.44953793488829813, F1 macro: 0.4984302697670911 \n",
      "Precision micro: 0.840122785700582, Recall micro: 0.7676620113364109, F1 micro: 0.8022595419847327 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.041157981956377626\n",
      "Precision macro: 0.6517207851828827, Recall macro: 0.4510111230316338, F1 macro: 0.5021632400627911 \n",
      "Precision micro: 0.8497310770038042, Recall micro: 0.7570268217144861, F1 micro: 0.8007045953212398 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04072287373803556\n",
      "Precision macro: 0.6472982076220347, Recall macro: 0.45793788812000497, F1 macro: 0.5114833928963433 \n",
      "Precision micro: 0.8390870366838324, Recall micro: 0.7712265529129901, F1 micro: 0.8037269350222277 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04043783767148852\n",
      "Precision macro: 0.6491732013933399, Recall macro: 0.45703788323180383, F1 macro: 0.5067944714424398 \n",
      "Precision micro: 0.8411113936927772, Recall micro: 0.7730380412551862, F1 micro: 0.8056392923479797 \n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 3, 'dim_hidden': 120, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.0883097191080451\n",
      "Precision macro: 0.2992894152884386, Recall macro: 0.2031104853576611, F1 macro: 0.23200892535044132 \n",
      "Precision micro: 0.8356830335439961, Recall micro: 0.6026997019809501, F1 micro: 0.7003225258869462 \n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.07423127198126167\n",
      "Precision macro: 0.5598422622393268, Recall macro: 0.29520596812935407, F1 macro: 0.343509937031588 \n",
      "Precision micro: 0.8448328267477203, Recall micro: 0.6496815286624203, F1 micro: 0.7345159052621147 \n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.05321144414693117\n",
      "Precision macro: 0.5673921176414303, Recall macro: 0.380718660174817, F1 macro: 0.42555279996793066 \n",
      "Precision micro: 0.809227579056506, Recall micro: 0.7297376263659207, F1 micro: 0.7674297127054847 \n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.05233286406565458\n",
      "Precision macro: 0.5505455679751452, Recall macro: 0.39715494269710433, F1 macro: 0.4316737862052029 \n",
      "Precision micro: 0.8197795071335927, Recall micro: 0.7386781978612751, F1 micro: 0.7771186180186271 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.050286914540454745\n",
      "Precision macro: 0.5514185037118053, Recall macro: 0.40042491642760236, F1 macro: 0.4490623757164002 \n",
      "Precision micro: 0.8277872396174506, Recall micro: 0.738444457430024, F1 micro: 0.7805676518731277 \n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.04981533459667117\n",
      "Precision macro: 0.5797588374509787, Recall macro: 0.38263075566686333, F1 macro: 0.43128856208939526 \n",
      "Precision micro: 0.8404102564102565, Recall micro: 0.7182259101268041, F1 micro: 0.7745289558258239 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.04903699448145926\n",
      "Precision macro: 0.5789165815052043, Recall macro: 0.3924684130193056, F1 macro: 0.43820556768410185 \n",
      "Precision micro: 0.8334232897044933, Recall micro: 0.7218488868111962, F1 micro: 0.7736339439486456 \n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04879730170406401\n",
      "Precision macro: 0.5934794265237273, Recall macro: 0.3861585016586789, F1 macro: 0.4466837345486999 \n",
      "Precision micro: 0.8366420489193035, Recall micro: 0.7215567112721323, F1 micro: 0.7748493975903614 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.047509554866701366\n",
      "Precision macro: 0.6449182447622984, Recall macro: 0.42648338955225906, F1 macro: 0.4789108569171511 \n",
      "Precision micro: 0.8186361336032388, Recall micro: 0.7562087302051073, F1 micro: 0.786185109808329 \n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04774539741221815\n",
      "Precision macro: 0.6335737226580193, Recall macro: 0.4316020364940581, F1 macro: 0.48301248055456425 \n",
      "Precision micro: 0.8282919878230456, Recall micro: 0.7472681587097528, F1 micro: 0.7856967313836324 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04711339961364865\n",
      "Precision macro: 0.6525392686494282, Recall macro: 0.4356545857072355, F1 macro: 0.4924674408046259 \n",
      "Precision micro: 0.835679806918745, Recall micro: 0.7283936188862269, F1 micro: 0.7783571138655594 \n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04738881912827492\n",
      "Precision macro: 0.6553804084571785, Recall macro: 0.43423399281653785, F1 macro: 0.49430023438627335 \n",
      "Precision micro: 0.8391886517300808, Recall micro: 0.7397884649097177, F1 micro: 0.7863598248392807 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.047097329968586565\n",
      "Precision macro: 0.6403195351804642, Recall macro: 0.43561494275832935, F1 macro: 0.49920791224132033 \n",
      "Precision micro: 0.8419473157719239, Recall micro: 0.7377432361362707, F1 micro: 0.7864083717453594 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04692800891958177\n",
      "Precision macro: 0.6488990919433547, Recall macro: 0.4490023264987141, F1 macro: 0.5022190448794297 \n",
      "Precision micro: 0.8354610848292399, Recall micro: 0.7533454099222813, F1 micro: 0.7922812192723698 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.04636572361923754\n",
      "Precision macro: 0.612212369037283, Recall macro: 0.4460293684270456, F1 macro: 0.5013248223971914 \n",
      "Precision micro: 0.8398384925975774, Recall micro: 0.7292701455034184, F1 micro: 0.7806586807618928 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.046586036753840744\n",
      "Precision macro: 0.6560645081199709, Recall macro: 0.4416167977327767, F1 macro: 0.504914777337919 \n",
      "Precision micro: 0.8351855462403327, Recall micro: 0.7509495705019575, F1 micro: 0.7908307692307692 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04673535005003214\n",
      "Precision macro: 0.668937179424443, Recall macro: 0.4677856828175386, F1 macro: 0.5254438493959562 \n",
      "Precision micro: 0.8432357411935761, Recall micro: 0.7455735405831824, F1 micro: 0.7914030517305546 \n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04639213209878653\n",
      "Precision macro: 0.6597399152034624, Recall macro: 0.4337557585044305, F1 macro: 0.48866611942645477 \n",
      "Precision micro: 0.8359410801963993, Recall micro: 0.7461578916613101, F1 micro: 0.788501914289243 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.0456831354983151\n",
      "Precision macro: 0.6529486002368378, Recall macro: 0.4240458383317502, F1 macro: 0.4862121319936799 \n",
      "Precision micro: 0.8508656963509692, Recall micro: 0.7207970548705662, F1 micro: 0.7804492249288199 \n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.045896265597082674\n",
      "Precision macro: 0.7030860471456659, Recall macro: 0.43400394656018054, F1 macro: 0.49592178318769253 \n",
      "Precision micro: 0.8394606198846102, Recall micro: 0.73119850406124, F1 micro: 0.7815984259346014 \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=[\n",
    "    \"optimizer\", \"num_hidden\", \"dim_hidden\", \"dropout_rate\", \"learning_rate\", \"num_epochs\", \n",
    "    'precision_macro', 'recall_macro', 'f1_macro', \n",
    "    'precision_micro', 'recall_micro', 'f1_micro'\n",
    "])\n",
    "\n",
    "\n",
    "for num_hidden, dim_hidden, dropout_rate, lr in itertools.product(range_num_hidden, range_dim_hidden, range_dropout, range_lr):\n",
    "    # model\n",
    "    options = {\n",
    "        \"VOCAB_SIZE\": len(index_to_word),\n",
    "        \"dim_e\": weights_matrix.shape[1],\n",
    "        \"pretrained_embeddings\": weights_matrix,\n",
    "        \"num_layers\": num_hidden,\n",
    "        \"num_classes\": len(mlb.classes_),\n",
    "        \"mid_features\": dim_hidden,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"activation\": nn.ReLU()\n",
    "    }\n",
    "    num_epochs = 10\n",
    "    \n",
    "    result = {\n",
    "        \"optimizer\": \"SWA\", \n",
    "        \"num_hidden\": num_hidden,\n",
    "        \"dim_hidden\": dim_hidden,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    print(\"\\n\", result)\n",
    "    \n",
    "    model = FinalModel(options)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    base_opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = SWA(base_opt) \n",
    "    \n",
    "    # train the model\n",
    "    metrics_dict = train_model(wiki_loaders, model, criterion, optimizer, num_epochs=num_epochs)\n",
    "    result.update(metrics_dict)\n",
    "    \n",
    "    results_df = results_df.append(result, ignore_index=True)\n",
    "    results_df.to_csv(\"results/results_tuning_2_3_layers_maxlen_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv(\"results_tuning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
