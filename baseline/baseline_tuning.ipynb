{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import nltk\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "# import importlib\n",
    "# importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_EMBEDDINGS_FOLDER = \"/scratch/mz2476/wiki/embeddings/\"\n",
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import create_lookups_for_vocab, pad_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is: 682850\n"
     ]
    }
   ],
   "source": [
    "# LOAD vocab, tensor dataset, classes\n",
    "vocab = torch.load(PATH_TO_DATA_FOLDER + \"vocab_all_en.pt\")\n",
    "print(\"Vocab size is:\", len(vocab))\n",
    "index_to_word, word_to_index = create_lookups_for_vocab(vocab)\n",
    "\n",
    "wiki_tensor_dataset = torch.load(PATH_TO_DATA_FOLDER + \"wiki_tensor_dataset_vocab_all_en.pt\")\n",
    "\n",
    "classes = torch.load(PATH_TO_DATA_FOLDER + \"classes_list.pt\")\n",
    "mlb = MultiLabelBinarizer(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([13030,  8330,  3721,  8330,  3721,   132,  2496, 13031,  4719,  3982,\n",
       "         13031,  3178,   303,  5510, 13032,  8334,  2496, 13031,  4719,  1828,\n",
       "          2496,  1985, 13033, 10701, 13034,     7,  5299,  2338,  6948,     5,\n",
       "             9,     9,     8, 10510,   480, 13035, 13036, 11814, 13035, 13036,\n",
       "           965,   933,  2789,     5,   223,    10,   933, 13037,  6777,  1646,\n",
       "          3271, 13038,  2496, 13031,  4719,  1036, 13039,  1985,  2300,  1495,\n",
       "           601, 13040,  1495,     5,     9,   208,     6,     5,     9,     9,\n",
       "            11,   568,     5,     9,     9,   208, 13041,  1467,   403, 13042,\n",
       "          9309,  1065, 13043, 13044, 13043, 13044,  2300,  2189,  1880,  8330,\n",
       "          4719,   452,    10,     8,     8,     8, 13035, 13036,    21, 13045,\n",
       "          2300, 13045,  2641,  3721,  4340,  4251, 13043, 13044, 13046,  2496,\n",
       "         13031,  4719,  4340, 13045, 13047, 13048, 13049, 13050,  5496,  9571,\n",
       "           648,     5,     9,    10,     8,     5,    11, 13051,  6945,  9127,\n",
       "          2496,    53,  5458, 13051,  6945,  9127,  6945,  9127,  2025,   833,\n",
       "          6777,  3721, 13052,  3271, 13053, 13054,  3721, 11814, 13035, 13036,\n",
       "           689,  1261,    10,     8,     5,   253, 13055,  2496, 13031,  4719,\n",
       "         12197, 13052,  3271, 13053,  3721,  5251,   952,  3078,   167,  6674,\n",
       "            10,  8340,   439,  1366, 13056,   952,  1199,    10,     8,     5,\n",
       "             6,  3541,  1816,   794,    10,     8,     8,   611,   532,    10,\n",
       "             8,    10,     8,  1108, 13057,  2471,  5251,   952,   439,  8340,\n",
       "          1140,  2496, 13031,  4719,   132, 10436,  3078,  3271, 13053,   132,\n",
       "            58, 13058,   414, 13031,   132,   277,  2496,  2076, 13031,  1869,\n",
       "           525,    24, 13031,  4719,   526,  8330,  8340,  2496,    81,   397,\n",
       "           398, 13059,  3435,  2869,  2496, 13031,  8330,  8340,  2055,     5,\n",
       "             9,     9,     8,  8330,  8340,  2496, 13035, 13036]),\n",
       " tensor([248.]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings and make a pretrained embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2519370it [03:11, 13165.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Aligned fasstext. 2.5 million\n",
    "embeddings = utils.load_vectors(PATH_TO_EMBEDDINGS_FOLDER + \"wiki.en.align.vec\")\n",
    "\n",
    "# # CHANGE to googlenews vectors\n",
    "# import gensim\n",
    " \n",
    "# model = gensim.models.KeyedVectors.load(\"/scratch/mz2476/GoogleNews-vectors-negative300.bin\", binary=True)  \n",
    " \n",
    "# embeddings = model.vocab.keys()\n",
    "# wordsInVocab = len(embeddings)\n",
    "# print (wordsInVocab)\n",
    "\n",
    "# # embeddings = load_vectors(\"/scratch/mz2476/GoogleNews-vectors-negative300.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 682850\n",
      "No. of words from vocab found in embeddings: 528314\n"
     ]
    }
   ],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "weights_matrix_ve = utils.create_embeddings_matrix(word_to_index, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(mlb.classes_),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "    \n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = SWA(base_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (layer_bag_of_words): BagOfWords(\n",
       "    (embed_e): Embedding(595366, 300)\n",
       "  )\n",
       "  (layer_out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# from utils import test_model\n",
    "\n",
    "# best_val_f1_micro = 0\n",
    "# num_epochs = 20\n",
    "# for epoch in range(num_epochs):\n",
    "#     runnin_loss = 0.0\n",
    "#     for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "#         model.train()\n",
    "#         data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(data_batch, length_batch)\n",
    "#         loss = criterion(outputs, label_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         runnin_loss += loss.item()\n",
    "#         #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "#         if i>0 and i % 300 == 0:\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "#                 epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "#         # validate every 300 iterations\n",
    "#         if i > 0 and i % 300 == 0:\n",
    "#             metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "#             print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "#                 metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "#             ))\n",
    "#             print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "#                 metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "#             ))\n",
    "            \n",
    "#             if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "#                 best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "#                 optimizer.swap_swa_sgd()\n",
    "#                 torch.save(model.state_dict(), 'baseline.pth')\n",
    "#                 print('Model Saved')\n",
    "#                 print()\n",
    "# optimizer.swap_swa_sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (layer_bag_of_words): BagOfWords(\n",
       "    (embed_e): Embedding(595366, 300)\n",
       "  )\n",
       "  (layer_out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(mlb.classes_),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    \"../../baseline_models_params/en_optimizer_SWA_num_hidden_2_dim_hidden_150_dropout_rate_0.2_learning_rate_0.01_num_epochs_10.pth\",\n",
    "#     map_location=torch.device('cpu')\n",
    "))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_macro': 0.019017772983219675,\n",
       " 'recall_macro': 0.0034896592022761463,\n",
       " 'f1_macro': 0.0049014079184453605,\n",
       " 'precision_micro': 0.19329896907216496,\n",
       " 'recall_micro': 0.017530532343832176,\n",
       " 'f1_micro': 0.0321457272970801}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.test_model(wiki_loaders[\"val\"], model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mz2476/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>count</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Culture.Arts</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9977</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Culture.Broadcasting</td>\n",
       "      <td>217.0</td>\n",
       "      <td>9778</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Culture.Crafts and hobbies</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9982</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Culture.Entertainment</td>\n",
       "      <td>295.0</td>\n",
       "      <td>9676</td>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Culture.Food and drink</td>\n",
       "      <td>67.0</td>\n",
       "      <td>9929</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Culture.Games and toys</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9886</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Culture.Internet culture</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9990</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Culture.Language and literature</td>\n",
       "      <td>3631.0</td>\n",
       "      <td>6323</td>\n",
       "      <td>3621</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.005430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Culture.Media</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9993</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Culture.Music</td>\n",
       "      <td>435.0</td>\n",
       "      <td>9561</td>\n",
       "      <td>435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Culture.Performing arts</td>\n",
       "      <td>72.0</td>\n",
       "      <td>9923</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Culture.Philosophy and religion</td>\n",
       "      <td>274.0</td>\n",
       "      <td>9721</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Culture.Plastic arts</td>\n",
       "      <td>302.0</td>\n",
       "      <td>9694</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Culture.Sports</td>\n",
       "      <td>1567.0</td>\n",
       "      <td>8426</td>\n",
       "      <td>1567</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Culture.Visual arts</td>\n",
       "      <td>139.0</td>\n",
       "      <td>9856</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Geography.Africa</td>\n",
       "      <td>225.0</td>\n",
       "      <td>9766</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Geography.Americas</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>8098</td>\n",
       "      <td>1881</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.003156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Geography.Antarctica</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9969</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Geography.Asia</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>8546</td>\n",
       "      <td>1392</td>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.005004</td>\n",
       "      <td>0.009609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Geography.Bodies of water</td>\n",
       "      <td>81.0</td>\n",
       "      <td>9915</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Geography.Europe</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>6953</td>\n",
       "      <td>1906</td>\n",
       "      <td>262</td>\n",
       "      <td>875</td>\n",
       "      <td>0.230431</td>\n",
       "      <td>0.120849</td>\n",
       "      <td>0.158548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Geography.Landforms</td>\n",
       "      <td>57.0</td>\n",
       "      <td>9939</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Geography.Maps</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9995</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Geography.Oceania</td>\n",
       "      <td>468.0</td>\n",
       "      <td>9464</td>\n",
       "      <td>468</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Geography.Parks</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9935</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>History_And_Society.Business and economics</td>\n",
       "      <td>207.0</td>\n",
       "      <td>9782</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>History_And_Society.Education</td>\n",
       "      <td>181.0</td>\n",
       "      <td>9814</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>History_And_Society.History and society</td>\n",
       "      <td>449.0</td>\n",
       "      <td>9543</td>\n",
       "      <td>449</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>History_And_Society.Military and warfare</td>\n",
       "      <td>331.0</td>\n",
       "      <td>9665</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>History_And_Society.Politics and government</td>\n",
       "      <td>343.0</td>\n",
       "      <td>9652</td>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>History_And_Society.Transportation</td>\n",
       "      <td>551.0</td>\n",
       "      <td>9445</td>\n",
       "      <td>551</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>STEM.Biology</td>\n",
       "      <td>771.0</td>\n",
       "      <td>9089</td>\n",
       "      <td>753</td>\n",
       "      <td>18</td>\n",
       "      <td>136</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.023346</td>\n",
       "      <td>0.038919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>STEM.Chemistry</td>\n",
       "      <td>36.0</td>\n",
       "      <td>9960</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>STEM.Engineering</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9974</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>STEM.Geosciences</td>\n",
       "      <td>90.0</td>\n",
       "      <td>9902</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>STEM.Information science</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9986</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>STEM.Mathematics</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9985</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>STEM.Medicine</td>\n",
       "      <td>154.0</td>\n",
       "      <td>9837</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>STEM.Meteorology</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9991</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>STEM.Physics</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>STEM.Science</td>\n",
       "      <td>43.0</td>\n",
       "      <td>9953</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>STEM.Space</td>\n",
       "      <td>67.0</td>\n",
       "      <td>9929</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>STEM.Technology</td>\n",
       "      <td>275.0</td>\n",
       "      <td>9713</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>STEM.Time</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9966</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class_name   count    TN    FN   TP   FP  \\\n",
       "0                                  Culture.Arts    19.0  9977    19    0    0   \n",
       "1                          Culture.Broadcasting   217.0  9778   217    0    1   \n",
       "2                    Culture.Crafts and hobbies    14.0  9982    14    0    0   \n",
       "3                         Culture.Entertainment   295.0  9676   295    0   25   \n",
       "4                        Culture.Food and drink    67.0  9929    67    0    0   \n",
       "5                        Culture.Games and toys   109.0  9886   109    0    1   \n",
       "6                      Culture.Internet culture     6.0  9990     6    0    0   \n",
       "7               Culture.Language and literature  3631.0  6323  3621   10   42   \n",
       "8                                 Culture.Media     3.0  9993     3    0    0   \n",
       "9                                 Culture.Music   435.0  9561   435    0    0   \n",
       "10                      Culture.Performing arts    72.0  9923    72    0    1   \n",
       "11              Culture.Philosophy and religion   274.0  9721   274    0    1   \n",
       "12                         Culture.Plastic arts   302.0  9694   302    0    0   \n",
       "13                               Culture.Sports  1567.0  8426  1567    0    3   \n",
       "14                          Culture.Visual arts   139.0  9856   139    0    1   \n",
       "15                             Geography.Africa   225.0  9766   225    0    5   \n",
       "16                           Geography.Americas  1884.0  8098  1881    3   14   \n",
       "17                         Geography.Antarctica    27.0  9969    27    0    0   \n",
       "18                               Geography.Asia  1399.0  8546  1392    7   51   \n",
       "19                    Geography.Bodies of water    81.0  9915    81    0    0   \n",
       "20                             Geography.Europe  2168.0  6953  1906  262  875   \n",
       "21                          Geography.Landforms    57.0  9939    57    0    0   \n",
       "22                               Geography.Maps     1.0  9995     1    0    0   \n",
       "23                            Geography.Oceania   468.0  9464   468    0   64   \n",
       "24                              Geography.Parks    60.0  9935    60    0    1   \n",
       "25   History_And_Society.Business and economics   207.0  9782   207    0    7   \n",
       "26                History_And_Society.Education   181.0  9814   181    0    1   \n",
       "27      History_And_Society.History and society   449.0  9543   449    0    4   \n",
       "28     History_And_Society.Military and warfare   331.0  9665   331    0    0   \n",
       "29  History_And_Society.Politics and government   343.0  9652   343    0    1   \n",
       "30           History_And_Society.Transportation   551.0  9445   551    0    0   \n",
       "31                                 STEM.Biology   771.0  9089   753   18  136   \n",
       "32                               STEM.Chemistry    36.0  9960    36    0    0   \n",
       "33                             STEM.Engineering    22.0  9974    22    0    0   \n",
       "34                             STEM.Geosciences    90.0  9902    90    0    4   \n",
       "35                     STEM.Information science    10.0  9986    10    0    0   \n",
       "36                             STEM.Mathematics    11.0  9985    11    0    0   \n",
       "37                                STEM.Medicine   154.0  9837   154    0    5   \n",
       "38                             STEM.Meteorology     5.0  9991     5    0    0   \n",
       "39                                 STEM.Physics    18.0  9978    18    0    0   \n",
       "40                                 STEM.Science    43.0  9953    43    0    0   \n",
       "41                                   STEM.Space    67.0  9929    67    0    0   \n",
       "42                              STEM.Technology   275.0  9713   275    0    8   \n",
       "43                                    STEM.Time    29.0  9966    29    0    1   \n",
       "\n",
       "    precision    recall        f1  \n",
       "0    0.000000  0.000000  0.000000  \n",
       "1    0.000000  0.000000  0.000000  \n",
       "2    0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.000000  \n",
       "4    0.000000  0.000000  0.000000  \n",
       "5    0.000000  0.000000  0.000000  \n",
       "6    0.000000  0.000000  0.000000  \n",
       "7    0.192308  0.002754  0.005430  \n",
       "8    0.000000  0.000000  0.000000  \n",
       "9    0.000000  0.000000  0.000000  \n",
       "10   0.000000  0.000000  0.000000  \n",
       "11   0.000000  0.000000  0.000000  \n",
       "12   0.000000  0.000000  0.000000  \n",
       "13   0.000000  0.000000  0.000000  \n",
       "14   0.000000  0.000000  0.000000  \n",
       "15   0.000000  0.000000  0.000000  \n",
       "16   0.176471  0.001592  0.003156  \n",
       "17   0.000000  0.000000  0.000000  \n",
       "18   0.120690  0.005004  0.009609  \n",
       "19   0.000000  0.000000  0.000000  \n",
       "20   0.230431  0.120849  0.158548  \n",
       "21   0.000000  0.000000  0.000000  \n",
       "22   0.000000  0.000000  0.000000  \n",
       "23   0.000000  0.000000  0.000000  \n",
       "24   0.000000  0.000000  0.000000  \n",
       "25   0.000000  0.000000  0.000000  \n",
       "26   0.000000  0.000000  0.000000  \n",
       "27   0.000000  0.000000  0.000000  \n",
       "28   0.000000  0.000000  0.000000  \n",
       "29   0.000000  0.000000  0.000000  \n",
       "30   0.000000  0.000000  0.000000  \n",
       "31   0.116883  0.023346  0.038919  \n",
       "32   0.000000  0.000000  0.000000  \n",
       "33   0.000000  0.000000  0.000000  \n",
       "34   0.000000  0.000000  0.000000  \n",
       "35   0.000000  0.000000  0.000000  \n",
       "36   0.000000  0.000000  0.000000  \n",
       "37   0.000000  0.000000  0.000000  \n",
       "38   0.000000  0.000000  0.000000  \n",
       "39   0.000000  0.000000  0.000000  \n",
       "40   0.000000  0.000000  0.000000  \n",
       "41   0.000000  0.000000  0.000000  \n",
       "42   0.000000  0.000000  0.000000  \n",
       "43   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.create_per_class_tables(wiki_loaders[\"val\"], model, device, class_names=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = torch.zeros_like(wiki_loaders[\"val\"].dataset[0][-1])\n",
    "# for idx in range(len(wiki_loaders[\"val\"].dataset)):\n",
    "#     counts += wiki_loaders[\"val\"].dataset[idx][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_per_class = pd.DataFrame(per_class_metrics_dict)\n",
    "# df_per_class[\"class_name\"] = mlb.classes_\n",
    "# # change columns order\n",
    "# df_per_class = df_per_class[[df_per_class.columns[-1]] + list(df_per_class.columns[:-1])]\n",
    "# df_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, multilabel_confusion_matrix\n",
    "\n",
    "def create_per_class_tables(loader, model, device, class_names, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    outputs_list_nc = []\n",
    "    true_list_nc = []\n",
    "    with torch.no_grad():\n",
    "        for data, length, labels in loader:\n",
    "            data_batch, length_batch, label_batch = data.to(device), length.to(device), labels.float()\n",
    "            outputs_bc = torch.sigmoid(model(data_batch, length_batch))\n",
    "            outputs_bc = outputs_bc.detach().cpu().numpy().astype(np.float)\n",
    "            outputs_bc = (outputs_bc > threshold)\n",
    "            outputs_list_nc.append(outputs_bc)\n",
    "            true_list_nc.append(label_batch.detach().cpu().numpy().astype(np.float))\n",
    "    # to np.array\n",
    "    outputs_list_nc = np.vstack(outputs_list_nc)\n",
    "    true_list_nc = np.vstack(true_list_nc)\n",
    "    \n",
    "    # per class counts\n",
    "    counts_c = true_list_nc.sum(axis=0)\n",
    "    \n",
    "    # per class confusion matrix: TN, FN, TP, FP\n",
    "    confusion_matrix_c22 = multilabel_confusion_matrix(\n",
    "        true_list_nc,\n",
    "        outputs_list_nc,\n",
    "    )\n",
    "    confusion_matrix_c4 = confusion_matrix_c22.reshape(-1, 4)\n",
    "    \n",
    "    # per class precision, recall, f-score\n",
    "    precision_c, recall_c, f1_c, _ = precision_recall_fscore_support(\n",
    "        true_list_nc,\n",
    "        outputs_list_nc,\n",
    "        average=None\n",
    "    )\n",
    "    \n",
    "    # combine all metrics in a dict\n",
    "    per_class_metrics = {\n",
    "        \"class_name\": class_names,\n",
    "        \"count\": counts_c,\n",
    "        \"TN\": confusion_matrix_c4[:,0], \n",
    "        \"FN\": confusion_matrix_c4[:,2],\n",
    "        \"TP\": confusion_matrix_c4[:,3],\n",
    "        \"FP\": confusion_matrix_c4[:,1],\n",
    "        \"precision\": precision_c, \n",
    "        \"recall\": recall_c, \n",
    "        \"f1\": f1_c\n",
    "    }\n",
    "    return pd.DataFrame(per_class_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2], [3,4]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search vs. Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> dropout </li>\n",
    "    <li> learning rate </li>\n",
    "    <li> optimizer </li>\n",
    "    <li> num of hidden layers </li>\n",
    "    <li> dim of hidden layers </li>\n",
    "    <li> take only first 500 words from the article </li>\n",
    "    <li> TODO threshold </li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I focused on SWA optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one layer\n",
    "range_dropout = [0]\n",
    "range_num_hidden = [1]\n",
    "range_dim_hidden = [80, 120]\n",
    "range_lr = [0.01]\n",
    "\n",
    "# # many layers\n",
    "# range_dropout = [0, 0.1, 0.2]\n",
    "# range_num_hidden = [2, 3]\n",
    "# range_dim_hidden = [40, 80, 120]\n",
    "# range_lr = [0.001, 0.01]\n",
    "\n",
    "# # best hyperparams\n",
    "# range_dropout = [0.2]\n",
    "# range_num_hidden = [2]\n",
    "# range_dim_hidden = [120, 150, 200]\n",
    "# range_lr = [0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, num_epochs=10, device=device, model_name=\"model\"):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 1000 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "                ))\n",
    "                print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "                ))\n",
    "\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "                    optimizer.swap_swa_sgd()\n",
    "                    torch.save(model.state_dict(), f\"../../baseline_models_params/en_{model_name}.pth\")\n",
    "                    print('Model Saved')\n",
    "                    print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 1, 'dim_hidden': 80, 'dropout_rate': 0, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.10299256878718734\n",
      "Precision macro: 0.3465349861689932, Recall macro: 0.10804991679312481, F1 macro: 0.13437249152734204 \n",
      "Precision micro: 0.8255222426669553, Recall micro: 0.44568456728802663, F1 micro: 0.5788554948391013 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.08429846991226077\n",
      "Precision macro: 0.49426200799447045, Recall macro: 0.19439115909278187, F1 macro: 0.24695920385385606 \n",
      "Precision micro: 0.8382196162046909, Recall micro: 0.5513352422135219, F1 micro: 0.6651626775705876 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.057701095275580885\n",
      "Precision macro: 0.5692249504963379, Recall macro: 0.26692777340916324, F1 macro: 0.3330868574492047 \n",
      "Precision micro: 0.8275697211155378, Recall micro: 0.6069070297434699, F1 micro: 0.7002663250514108 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.05676551521942019\n",
      "Precision macro: 0.5921045254054674, Recall macro: 0.28757684459362787, F1 macro: 0.3583790224910686 \n",
      "Precision micro: 0.8446988147426531, Recall micro: 0.6080172967919126, F1 micro: 0.7070775712683905 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.05192196919955313\n",
      "Precision macro: 0.5859682886505116, Recall macro: 0.3298853114241472, F1 macro: 0.39779688174398165 \n",
      "Precision micro: 0.8408917972911277, Recall micro: 0.6457663764389645, F1 micro: 0.7305238803503553 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.05191774589475244\n",
      "Precision macro: 0.5798996264562101, Recall macro: 0.334190032400515, F1 macro: 0.3998536432713196 \n",
      "Precision micro: 0.8409142857142857, Recall micro: 0.6449482849295857, F1 micro: 0.7300085984522786 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.0493697063960135\n",
      "Precision macro: 0.6206157947423218, Recall macro: 0.3724537917810857, F1 macro: 0.4393227192999054 \n",
      "Precision micro: 0.8310728381772515, Recall micro: 0.6767369835797347, F1 micro: 0.7460061839732028 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04942354714404792\n",
      "Precision macro: 0.6285409917362158, Recall macro: 0.3508856600927696, F1 macro: 0.41669461237770217 \n",
      "Precision micro: 0.8413978494623656, Recall micro: 0.6584467948343364, F1 micro: 0.7387641370267168 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04780533712171018\n",
      "Precision macro: 0.6425150653347264, Recall macro: 0.387628108897613, F1 macro: 0.456387430983132 \n",
      "Precision micro: 0.8373684969015708, Recall micro: 0.6790743878922456, F1 micro: 0.7499596657093994 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04793544643651694\n",
      "Precision macro: 0.6745390796875618, Recall macro: 0.37957751053256134, F1 macro: 0.44961833765130055 \n",
      "Precision micro: 0.8431531002057009, Recall micro: 0.6706597323672062, F1 micro: 0.7470789259560617 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.046567588165402414\n",
      "Precision macro: 0.663388442902312, Recall macro: 0.4028532841836109, F1 macro: 0.4750794406306675 \n",
      "Precision micro: 0.8448401693183477, Recall micro: 0.6764448080406709, F1 micro: 0.7513224079182217 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04670938115287572\n",
      "Precision macro: 0.6977112985401157, Recall macro: 0.40269212597524146, F1 macro: 0.47769704833693516 \n",
      "Precision micro: 0.8418874773139746, Recall micro: 0.6776719453047391, F1 micro: 0.7509065009065008 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04582743711210787\n",
      "Precision macro: 0.7014907128887168, Recall macro: 0.43107543689308697, F1 macro: 0.505947336957138 \n",
      "Precision micro: 0.8347588334268088, Recall micro: 0.695786828726699, F1 micro: 0.7589635720432164 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.04587189910747111\n",
      "Precision macro: 0.6546324854290256, Recall macro: 0.40889540185228146, F1 macro: 0.48087955797317056 \n",
      "Precision micro: 0.849347393260084, Recall micro: 0.67305557178753, F1 micro: 0.7509943274434374 \n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.045234366219490764\n",
      "Precision macro: 0.7064139476978062, Recall macro: 0.42356564625221016, F1 macro: 0.49799503397297945 \n",
      "Precision micro: 0.8458724808147458, Recall micro: 0.6891836615438556, F1 micro: 0.7595311695002577 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.0452763677239418\n",
      "Precision macro: 0.73962336251207, Recall macro: 0.4248916779884313, F1 macro: 0.5056155584752277 \n",
      "Precision micro: 0.8454689070802814, Recall micro: 0.6880149593876, F1 micro: 0.758658461934985 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04511410992965102\n",
      "Precision macro: 0.710458961812863, Recall macro: 0.43281795624265745, F1 macro: 0.5096138764015609 \n",
      "Precision micro: 0.8432772515198643, Recall micro: 0.69707240109858, F1 micro: 0.7632361879778625 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04489125368464738\n",
      "Precision macro: 0.7426139566628861, Recall macro: 0.43481290464802425, F1 macro: 0.5168236140694136 \n",
      "Precision micro: 0.8426442545377498, Recall micro: 0.6971892713142056, F1 micro: 0.7630468150422102 \n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.04454941859655082\n",
      "Precision macro: 0.7334600257070041, Recall macro: 0.4547712729158075, F1 macro: 0.5334226377030232 \n",
      "Precision micro: 0.8424715309995783, Recall micro: 0.7003447671360954, F1 micro: 0.764861673952583 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.044593803998082875\n",
      "Precision macro: 0.7151027498986092, Recall macro: 0.4518527499449125, F1 macro: 0.5259092778911693 \n",
      "Precision micro: 0.835072641702476, Recall micro: 0.715421024951791, F1 micro: 0.7706300749040096 \n",
      "Model Saved\n",
      "\n",
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 1, 'dim_hidden': 120, 'dropout_rate': 0, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [1001/2499], Train_loss: 0.10340351222082972\n",
      "Precision macro: 0.3354775316988998, Recall macro: 0.10592703419923719, F1 macro: 0.13281553022740683 \n",
      "Precision micro: 0.8409841667615902, Recall micro: 0.4314264009817098, F1 micro: 0.5702919820794068 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [2001/2499], Train_loss: 0.08445129889808595\n",
      "Precision macro: 0.49744855696697426, Recall macro: 0.19130132082021586, F1 macro: 0.2441422387916899 \n",
      "Precision micro: 0.8390990990990991, Recall micro: 0.5442645941681762, F1 micro: 0.6602629993265515 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [1001/2499], Train_loss: 0.057162105761468414\n",
      "Precision macro: 0.5471851990566061, Recall macro: 0.2625860938340598, F1 macro: 0.3280172595360471 \n",
      "Precision micro: 0.8289875499177825, Recall micro: 0.6186524864138374, F1 micro: 0.7085396867889172 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [2001/2499], Train_loss: 0.05684742146544158\n",
      "Precision macro: 0.5729894790348895, Recall macro: 0.29033575959663216, F1 macro: 0.3596003178889419 \n",
      "Precision micro: 0.849135458493813, Recall micro: 0.6055045871559633, F1 micro: 0.7069177241097012 \n",
      "Epoch: [3/10], Step: [1001/2499], Train_loss: 0.05156498983316123\n",
      "Precision macro: 0.5796551654948038, Recall macro: 0.3376307523746558, F1 macro: 0.40408241940621176 \n",
      "Precision micro: 0.8384302500753239, Recall micro: 0.6504411850639864, F1 micro: 0.7325677054197242 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [2001/2499], Train_loss: 0.051883224669843915\n",
      "Precision macro: 0.5718778881043027, Recall macro: 0.33479151005212765, F1 macro: 0.39926952364991575 \n",
      "Precision micro: 0.8356205474315711, Recall micro: 0.6511424063577397, F1 micro: 0.7319364161849711 \n",
      "Epoch: [4/10], Step: [1001/2499], Train_loss: 0.049318539593368765\n",
      "Precision macro: 0.6205426121423119, Recall macro: 0.36007491015502513, F1 macro: 0.429460938761927 \n",
      "Precision micro: 0.8414881623449831, Recall micro: 0.6542394670718168, F1 micro: 0.7361430731803537 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [2001/2499], Train_loss: 0.04953842512052506\n",
      "Precision macro: 0.6049159717483168, Recall macro: 0.357581467292135, F1 macro: 0.42475528422915226 \n",
      "Precision micro: 0.8469961977186312, Recall micro: 0.6508502308186759, F1 micro: 0.7360803621584114 \n",
      "Epoch: [5/10], Step: [1001/2499], Train_loss: 0.04748791130259633\n",
      "Precision macro: 0.6594073578766966, Recall macro: 0.3831922329799976, F1 macro: 0.453696103017268 \n",
      "Precision micro: 0.8401511298408777, Recall micro: 0.6756851516391048, F1 micro: 0.748995983935743 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "\n",
      "Epoch: [5/10], Step: [2001/2499], Train_loss: 0.04775642579700798\n",
      "Precision macro: 0.619627358893633, Recall macro: 0.3814257011312245, F1 macro: 0.4486452340559879 \n",
      "Precision micro: 0.8386909513097743, Recall micro: 0.6753929761000409, F1 micro: 0.7482359034116657 \n",
      "Epoch: [6/10], Step: [1001/2499], Train_loss: 0.04645503962226212\n",
      "Precision macro: 0.6348776761616706, Recall macro: 0.4070470857177163, F1 macro: 0.4730722776741411 \n",
      "Precision micro: 0.8304895884114493, Recall micro: 0.6968386606673289, F1 micro: 0.7578164717844432 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [6/10], Step: [2001/2499], Train_loss: 0.04687268075440079\n",
      "Precision macro: 0.6595147829864435, Recall macro: 0.40743768890504833, F1 macro: 0.4786069538069574 \n",
      "Precision micro: 0.83796658371845, Recall micro: 0.6887161806813533, F1 micro: 0.7560459298223106 \n",
      "Epoch: [7/10], Step: [1001/2499], Train_loss: 0.04587547144666314\n",
      "Precision macro: 0.6794285680244305, Recall macro: 0.4107954777411001, F1 macro: 0.48413018931590435 \n",
      "Precision micro: 0.8420374874803263, Recall micro: 0.687781218956349, F1 micro: 0.757132289086874 \n",
      "Epoch: [7/10], Step: [2001/2499], Train_loss: 0.045775555733591315\n",
      "Precision macro: 0.6996342147104007, Recall macro: 0.4327443087940503, F1 macro: 0.5071976680817316 \n",
      "Precision micro: 0.8379286214135759, Recall micro: 0.6997019809501549, F1 micro: 0.762602299143394 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [8/10], Step: [1001/2499], Train_loss: 0.045682610733434555\n",
      "Precision macro: 0.6960393283133531, Recall macro: 0.4322213990895213, F1 macro: 0.505354976828466 \n",
      "Precision micro: 0.8360998744594783, Recall micro: 0.7005200724595337, F1 micro: 0.7623287017900863 \n",
      "Epoch: [8/10], Step: [2001/2499], Train_loss: 0.045750073495320974\n",
      "Precision macro: 0.733220275305163, Recall macro: 0.43365995664353246, F1 macro: 0.510894251768898 \n",
      "Precision micro: 0.8429446105652855, Recall micro: 0.6918716765032431, F1 micro: 0.7599730414968386 \n",
      "Epoch: [9/10], Step: [1001/2499], Train_loss: 0.04489561766013503\n",
      "Precision macro: 0.7304249180494218, Recall macro: 0.4445703271656665, F1 macro: 0.5219750513507532 \n",
      "Precision micro: 0.8427659723683287, Recall micro: 0.7022146905861041, F1 micro: 0.7660971567002421 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [9/10], Step: [2001/2499], Train_loss: 0.04497562742419541\n",
      "Precision macro: 0.7342124028458532, Recall macro: 0.44490846643417575, F1 macro: 0.5239704812845156 \n",
      "Precision micro: 0.8378978021216114, Recall micro: 0.7061882779173727, F1 micro: 0.7664256722475902 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [1001/2499], Train_loss: 0.044294384822249414\n",
      "Precision macro: 0.737026087563278, Recall macro: 0.45764313083428065, F1 macro: 0.5322815845888896 \n",
      "Precision micro: 0.8402125897294312, Recall micro: 0.7113305674048969, F1 micro: 0.7704186576374166 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [2001/2499], Train_loss: 0.04438021756056696\n",
      "Precision macro: 0.7370529045981832, Recall macro: 0.44147487483661707, F1 macro: 0.5209582625775822 \n",
      "Precision micro: 0.8429300808395972, Recall micro: 0.6946181265704435, F1 micro: 0.7616210155374018 \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=[\n",
    "    \"optimizer\", \"num_hidden\", \"dim_hidden\", \"dropout_rate\", \"learning_rate\", \"num_epochs\", \n",
    "    'precision_macro', 'recall_macro', 'f1_macro', \n",
    "    'precision_micro', 'recall_micro', 'f1_micro'\n",
    "])\n",
    "\n",
    "\n",
    "for num_hidden, dim_hidden, dropout_rate, lr in itertools.product(range_num_hidden, range_dim_hidden, range_dropout, range_lr):\n",
    "    # model\n",
    "    options = {\n",
    "        \"VOCAB_SIZE\": len(index_to_word),\n",
    "        \"dim_e\": weights_matrix.shape[1],\n",
    "        \"pretrained_embeddings\": weights_matrix,\n",
    "        \"num_layers\": num_hidden,\n",
    "        \"num_classes\": len(mlb.classes_),\n",
    "        \"mid_features\": dim_hidden,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"activation\": nn.ReLU()\n",
    "    }\n",
    "    num_epochs = 10\n",
    "    \n",
    "    result = {\n",
    "        \"optimizer\": \"SWA\", \n",
    "        \"num_hidden\": num_hidden,\n",
    "        \"dim_hidden\": dim_hidden,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    print(\"\\n\", result)\n",
    "    \n",
    "    model = FinalModel(options)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    base_opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = SWA(base_opt) \n",
    "    \n",
    "    # train the model\n",
    "    model_name = \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "    metrics_dict = train_model(wiki_loaders, model, criterion, optimizer, num_epochs=num_epochs, model_name=model_name)\n",
    "    result.update(metrics_dict)\n",
    "    \n",
    "#     results_df = results_df.append(result, ignore_index=True)\n",
    "#     results_df.to_csv(\"results/results_tuning_2_3_layers_maxlen_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv(\"results_tuning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
