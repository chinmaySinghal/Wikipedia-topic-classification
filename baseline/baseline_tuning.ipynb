{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import nltk\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_EMBEDDINGS_FOLDER = \"/scratch/mz2476/wiki/embeddings/\"\n",
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/\"\n",
    "PATH_TO_MODELS_FOLDER = \"/scratch/mz2476/wiki/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocess import create_lookups_for_vocab, pad_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is: 682850\n"
     ]
    }
   ],
   "source": [
    "# LOAD vocab, tensor dataset, classes\n",
    "vocab = torch.load(PATH_TO_DATA_FOLDER + \"vocab_all_en.pt\")\n",
    "print(\"Vocab size is:\", len(vocab))\n",
    "index_to_word, word_to_index = create_lookups_for_vocab(vocab)\n",
    "\n",
    "wiki_tensor_dataset = torch.load(PATH_TO_DATA_FOLDER + \"wiki_tensor_dataset_vocab_all_en.pt\")\n",
    "\n",
    "classes = torch.load(PATH_TO_DATA_FOLDER + \"classes_list.pt\")\n",
    "mlb = MultiLabelBinarizer(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 20000 for train 2000 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df_train = torch.load(PATH_TO_DATA_FOLDER + \"df_wiki_train_en.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q5346784</td>\n",
       "      <td>[Culture.Language and literature]</td>\n",
       "      <td>[edwin, romanzo, elmer, one, eight, five, zero...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q4723109</td>\n",
       "      <td>[Culture.Language and literature]</td>\n",
       "      <td>[alfred, george, fysh, machin, born, one, eigh...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q1456016</td>\n",
       "      <td>[Geography.Americas, Culture.Music]</td>\n",
       "      <td>[late, friends, first, full, length, studio, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q59149462</td>\n",
       "      <td>[Geography.Americas, Culture.Sports, Culture.L...</td>\n",
       "      <td>[mat, alexis, romero, born, one, february, one...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q30602920</td>\n",
       "      <td>[Culture.Plastic arts, Geography.Americas, Cul...</td>\n",
       "      <td>[confederate, memorial, fountain, historic, fo...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         QID                               mid_level_categories  \\\n",
       "0   Q5346784                  [Culture.Language and literature]   \n",
       "1   Q4723109                  [Culture.Language and literature]   \n",
       "2   Q1456016                [Geography.Americas, Culture.Music]   \n",
       "3  Q59149462  [Geography.Americas, Culture.Sports, Culture.L...   \n",
       "4  Q30602920  [Culture.Plastic arts, Geography.Americas, Cul...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [edwin, romanzo, elmer, one, eight, five, zero...   \n",
       "1  [alfred, george, fysh, machin, born, one, eigh...   \n",
       "2  [late, friends, first, full, length, studio, a...   \n",
       "3  [mat, alexis, romero, born, one, february, one...   \n",
       "4  [confederate, memorial, fountain, historic, fo...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train size: 20000 \n",
      "Combined val size: 2000\n"
     ]
    }
   ],
   "source": [
    "# train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Take 10000 articles for train, 1000 for val for each language\n",
    "# combine them in one train set\n",
    "train_size = 20000\n",
    "val_size = 2000\n",
    "SEED = 57\n",
    "\n",
    "wiki_train, wiki_valid = train_test_split(\n",
    "    wiki_df_train, \n",
    "    train_size=train_size, test_size=val_size, random_state=SEED\n",
    ")\n",
    "\n",
    "wiki_train = wiki_train.reset_index(drop=True)\n",
    "wiki_valid = wiki_valid.reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined train size: {wiki_train.shape[0]} \\nCombined val size: {wiki_valid.shape[0]}\")\n",
    "# wiki_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from preprocess import tokenize_dataset, TensoredDataset, pad_collate_fn\n",
    "\n",
    "def create_dict_of_tensor_datasets(dict_of_dfs, word_to_index, max_num_tokens=None):\n",
    "    \"\"\"\n",
    "    Creates dict of tensor datasets for each df in dict_of_dfs.\n",
    "    \n",
    "    Each df in dict_of_dfs should have columns 'tokens', 'labels'.\n",
    "    \"\"\"\n",
    "    wiki_tokenized_datasets = {}\n",
    "    wiki_tensor_dataset = {}\n",
    "    \n",
    "    for name_df, df in dict_of_dfs.items():\n",
    "        # Create feature matrix\n",
    "        wiki_tokenized_datasets[f\"X_{name_df}\"] = tokenize_dataset(df, word_to_index, max_num_tokens=max_num_tokens)\n",
    "        # Create labels matrix\n",
    "        wiki_tokenized_datasets[f\"y_{name_df}\"] = list(df.labels)\n",
    "        # Create tensor dataset\n",
    "        wiki_tensor_dataset[name_df] = TensoredDataset(\n",
    "            wiki_tokenized_datasets[f\"X_{name_df}\"], wiki_tokenized_datasets[f\"y_{name_df}\"]\n",
    "        )\n",
    "    return wiki_tensor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:01<00:00, 13842.24it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 11974.28it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_of_dfs = {\n",
    "    \"train\" : wiki_train,\n",
    "    \"val\" : wiki_valid,\n",
    "}\n",
    "wiki_tensor_dataset = create_dict_of_tensor_datasets(dict_of_dfs, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([421216,   2174,   2175,   2176,   3281,   3282,   2178,   2175,   2179,\n",
       "          10334,   2175,   2180,   4341, 421216,      5,   4094,    877,  20623,\n",
       "           3771, 110007,  10963,   1706,  21951,   1931,   2179,   3236,   8990,\n",
       "            877,  20008,   3236,  67822,    877,   4875, 296602,   3281,   3282,\n",
       "         296602, 421217, 421218, 421219, 421220,  52970, 421221,   2553, 421222,\n",
       "         421223,  24595,   2553, 421224,   7006,  20327,   5496,  52898,   3281,\n",
       "           3282,   5496,  52898,   5496, 421225,   3281,   3282,   5496, 421225,\n",
       "         421226,  24595, 421227,    525,     24,   2174,   3281,   3282,   2179,\n",
       "             81,   2181,    397,    398,   2553,  17818,    403, 421216,  14635,\n",
       "           2174,   3281,   3282, 421216,   3281,   3282,   2174,   2183,   2184,\n",
       "           2185,   2186,   2187]),\n",
       " tensor([93.]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings and make a pretrained embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/mz2476/topic-modeling/topic-modeling/baseline/utils.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2519370it [03:18, 12698.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Aligned fasstext. 2.5 million\n",
    "embeddings = utils.load_vectors(PATH_TO_EMBEDDINGS_FOLDER + \"wiki.en.align.vec\")\n",
    "\n",
    "# # CHANGE to googlenews vectors\n",
    "# import gensim\n",
    " \n",
    "# model = gensim.models.KeyedVectors.load(\"/scratch/mz2476/GoogleNews-vectors-negative300.bin\", binary=True)  \n",
    " \n",
    "# embeddings = model.vocab.keys()\n",
    "# wordsInVocab = len(embeddings)\n",
    "# print (wordsInVocab)\n",
    "\n",
    "# # embeddings = load_vectors(\"/scratch/mz2476/GoogleNews-vectors-negative300.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 682850\n",
      "No. of words from vocab found in embeddings: 528314\n"
     ]
    }
   ],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "weights_matrix_ve = utils.create_embeddings_matrix(word_to_index, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": weights_matrix_ve.shape[1],\n",
    "    \"pretrained_embeddings\": weights_matrix_ve,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(classes),\n",
    "    \"mid_features\": 150,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = FinalModel(options)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "    \n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "base_opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = SWA(base_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (layer_bag_of_words): BagOfWords(\n",
       "    (embed_e): Embedding(682850, 300)\n",
       "  )\n",
       "  (layer_out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=150, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search vs. Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> dropout </li>\n",
    "    <li> learning rate </li>\n",
    "    <li> optimizer </li>\n",
    "    <li> num of hidden layers </li>\n",
    "    <li> dim of hidden layers </li>\n",
    "    <li> take only first 500 words from the article </li>\n",
    "    <li> TODO threshold </li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I focused on SWA optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one layer\n",
    "# range_dropout = [0]\n",
    "# range_num_hidden = [1]\n",
    "# range_dim_hidden = [80, 120, 150]\n",
    "# range_lr = [0.01]\n",
    "\n",
    "# # many layers\n",
    "# range_dropout = [0, 0.1, 0.2]\n",
    "# range_num_hidden = [3] # 2\n",
    "# range_dim_hidden = [40, 80, 120]\n",
    "# range_lr = [0.001, 0.01]\n",
    "\n",
    "# best hyperparams\n",
    "range_dropout = [0.2]\n",
    "range_num_hidden = [2]\n",
    "range_dim_hidden = [150]\n",
    "range_lr = [0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "from model import FinalModel\n",
    "from torchcontrib.optim import SWA\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "def train_model(wiki_loaders, model, criterion, optimizer, num_epochs=10, device=device, model_name=\"model\"):\n",
    "    best_val_f1_micro = 0\n",
    "    best_metrics_dict = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        runnin_loss = 0.0\n",
    "        for i, (data, length, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device),length.to(device), labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runnin_loss += loss.item()\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "            if i>0 and i % 300 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "            # validate every 300 iterations\n",
    "            if i > 0 and i % 300 == 0:\n",
    "                optimizer.update_swa()\n",
    "                metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "                print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                    metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "                ))\n",
    "                print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                    metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "                ))\n",
    "\n",
    "                if metrics_dict[\"f1_micro\"] > best_val_f1_micro:\n",
    "                    best_val_f1_micro = metrics_dict[\"f1_micro\"]\n",
    "                    best_metrics_dict = metrics_dict\n",
    "                    optimizer.swap_swa_sgd()\n",
    "#                     torch.save(model.state_dict(), f\"{PATH_TO_MODELS_FOLDER}en_{model_name}.pth\")\n",
    "                    print('Model Saved')\n",
    "                    print()\n",
    "    optimizer.swap_swa_sgd()\n",
    "    return best_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df_without_best = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'optimizer': 'SWA', 'num_hidden': 2, 'dim_hidden': 150, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'num_epochs': 10}\n",
      "Epoch: [1/10], Step: [301/625], Train_loss: 0.115905874359111\n",
      "Precision macro: 0.19204309234420947, Recall macro: 0.10180625850352962, F1 macro: 0.1177748744699762 \n",
      "Precision micro: 0.8257703081232493, Recall micro: 0.4326386850601702, F1 micro: 0.5677966101694915 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [1/10], Step: [601/625], Train_loss: 0.09301430057113369\n",
      "Precision macro: 0.3170461753693635, Recall macro: 0.17528779406034875, F1 macro: 0.20538757121729526 \n",
      "Precision micro: 0.8345039508340649, Recall micro: 0.5579688875843851, F1 micro: 0.6687774846086192 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [301/625], Train_loss: 0.0624065837264061\n",
      "Precision macro: 0.3986185063143095, Recall macro: 0.22978695633594748, F1 macro: 0.2627669919441542 \n",
      "Precision micro: 0.817367601246106, Recall micro: 0.616084531846199, F1 micro: 0.7025941422594142 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [2/10], Step: [601/625], Train_loss: 0.06058108494306604\n",
      "Precision macro: 0.48280717322291605, Recall macro: 0.2276085474521114, F1 macro: 0.2707708101314945 \n",
      "Precision micro: 0.8316713203031512, Recall micro: 0.6119753448781919, F1 micro: 0.7051065268853568 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [301/625], Train_loss: 0.05458696056157351\n",
      "Precision macro: 0.4849985820017368, Recall macro: 0.27649720600357036, F1 macro: 0.32374253996429725 \n",
      "Precision micro: 0.8196841718692618, Recall micro: 0.6551218080422659, F1 micro: 0.7282218597063621 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [3/10], Step: [601/625], Train_loss: 0.05362759977268676\n",
      "Precision macro: 0.48506658630697447, Recall macro: 0.282272355181761, F1 macro: 0.3272514002311417 \n",
      "Precision micro: 0.8321548195013025, Recall micro: 0.6562958614616965, F1 micro: 0.7338365605513619 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [301/625], Train_loss: 0.050172815198699636\n",
      "Precision macro: 0.5136582716811995, Recall macro: 0.3338439651706965, F1 macro: 0.37705301293951626 \n",
      "Precision micro: 0.8162917518745739, Recall micro: 0.7029644848840623, F1 micro: 0.755401356252957 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [4/10], Step: [601/625], Train_loss: 0.05010707300777237\n",
      "Precision macro: 0.5093570865053161, Recall macro: 0.3253261764281726, F1 macro: 0.373274120176835 \n",
      "Precision micro: 0.8276363636363636, Recall micro: 0.6680363956560024, F1 micro: 0.739321097937307 \n",
      "Epoch: [5/10], Step: [301/625], Train_loss: 0.04738413755471508\n",
      "Precision macro: 0.5357115772510302, Recall macro: 0.35376928997877677, F1 macro: 0.4044141712447353 \n",
      "Precision micro: 0.853454821564161, Recall micro: 0.6598180217199883, F1 micro: 0.74424764111902 \n",
      "Epoch: [5/10], Step: [601/625], Train_loss: 0.04691655338741839\n",
      "Precision macro: 0.5300679355983651, Recall macro: 0.39889913298077423, F1 macro: 0.44212194831463275 \n",
      "Precision micro: 0.8225644459323737, Recall micro: 0.7211623128852362, F1 micro: 0.7685329996872067 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [6/10], Step: [301/625], Train_loss: 0.046434161445746816\n",
      "Precision macro: 0.5177360970196042, Recall macro: 0.3658247999271925, F1 macro: 0.41451280323245165 \n",
      "Precision micro: 0.826713532513181, Recall micro: 0.6903434106251835, F1 micro: 0.7523992322456814 \n",
      "Epoch: [6/10], Step: [601/625], Train_loss: 0.04655702353765567\n",
      "Precision macro: 0.5027502217018115, Recall macro: 0.38355771185060594, F1 macro: 0.4265538479654806 \n",
      "Precision micro: 0.8373983739837398, Recall micro: 0.6953331376577634, F1 micro: 0.759781911481719 \n",
      "Epoch: [7/10], Step: [301/625], Train_loss: 0.04429701184853911\n",
      "Precision macro: 0.5006272556093925, Recall macro: 0.3746259169803139, F1 macro: 0.4137668873324582 \n",
      "Precision micro: 0.8357522123893806, Recall micro: 0.6929850308189023, F1 micro: 0.7577021822849809 \n",
      "Epoch: [7/10], Step: [601/625], Train_loss: 0.044387041879817846\n",
      "Precision macro: 0.5424173684428664, Recall macro: 0.4020754898266763, F1 macro: 0.4401474148181647 \n",
      "Precision micro: 0.8188141768797615, Recall micro: 0.725565013208101, F1 micro: 0.7693744164332399 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [8/10], Step: [301/625], Train_loss: 0.04456276305640737\n",
      "Precision macro: 0.5335170299916512, Recall macro: 0.37163394097500063, F1 macro: 0.42000689747148423 \n",
      "Precision micro: 0.8442446043165468, Recall micro: 0.6888758438508952, F1 micro: 0.7586875707127847 \n",
      "Epoch: [8/10], Step: [601/625], Train_loss: 0.04409204383380711\n",
      "Precision macro: 0.5818654211518146, Recall macro: 0.40361863348247123, F1 macro: 0.4630741976439569 \n",
      "Precision micro: 0.8483650736615164, Recall micro: 0.6929850308189023, F1 micro: 0.762843295638126 \n",
      "Epoch: [9/10], Step: [301/625], Train_loss: 0.043190930311878525\n",
      "Precision macro: 0.5535515471575575, Recall macro: 0.40475634241457475, F1 macro: 0.4482846733769614 \n",
      "Precision micro: 0.8418803418803419, Recall micro: 0.6938655708834752, F1 micro: 0.7607401448109413 \n",
      "Epoch: [9/10], Step: [601/625], Train_loss: 0.04238968863772849\n",
      "Precision macro: 0.5811209763268687, Recall macro: 0.41138216939259914, F1 macro: 0.46215588136371205 \n",
      "Precision micro: 0.8466690165667959, Recall micro: 0.7050190783680658, F1 micro: 0.7693786034593211 \n",
      "Model Saved\n",
      "\n",
      "Epoch: [10/10], Step: [301/625], Train_loss: 0.04274068986376127\n",
      "Precision macro: 0.5541304584758691, Recall macro: 0.41289831369707, F1 macro: 0.4556314120715898 \n",
      "Precision micro: 0.8215488215488216, Recall micro: 0.7161725858526563, F1 micro: 0.7652501176101616 \n",
      "Epoch: [10/10], Step: [601/625], Train_loss: 0.042611099096635977\n",
      "Precision macro: 0.5856369583295926, Recall macro: 0.40201305749360117, F1 macro: 0.45666192217357815 \n",
      "Precision micro: 0.839043540328337, Recall micro: 0.6900498972703258, F1 micro: 0.7572878080206153 \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=[\n",
    "    \"optimizer\", \"num_hidden\", \"dim_hidden\", \"dropout_rate\", \"learning_rate\", \"num_epochs\", \n",
    "    'precision_macro', 'recall_macro', 'f1_macro', \n",
    "    'precision_micro', 'recall_micro', 'f1_micro'\n",
    "])\n",
    "\n",
    "\n",
    "for num_hidden, dim_hidden, dropout_rate, lr in itertools.product(range_num_hidden, range_dim_hidden, range_dropout, range_lr):\n",
    "    # model\n",
    "    options = {\n",
    "        \"VOCAB_SIZE\": len(index_to_word),\n",
    "        \"dim_e\": weights_matrix_ve.shape[1],\n",
    "        \"pretrained_embeddings\": weights_matrix_ve,\n",
    "        \"num_layers\": num_hidden,\n",
    "        \"num_classes\": len(classes),\n",
    "        \"mid_features\": dim_hidden,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"activation\": nn.ReLU()\n",
    "    }\n",
    "    num_epochs = 10\n",
    "    \n",
    "    result = {\n",
    "        \"optimizer\": \"SWA\", \n",
    "        \"num_hidden\": num_hidden,\n",
    "        \"dim_hidden\": dim_hidden,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    print(\"\\n\", result)\n",
    "    \n",
    "    model = FinalModel(options)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    base_opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = SWA(base_opt) \n",
    "    \n",
    "    # train the model\n",
    "    model_name = \"_\".join([str(key) + \"_\" + str(value) for key, value in result.items()])\n",
    "    metrics_dict = train_model(wiki_loaders, model, criterion, optimizer, num_epochs=num_epochs, model_name=model_name)\n",
    "    result.update(metrics_dict)\n",
    "    \n",
    "    results_df = results_df.append(result, ignore_index=True)\n",
    "#     results_df.to_csv(\"results/results_tuning_2_layers_1101.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(metrics_dict):\n",
    "    metrics_dict = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "    print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "        metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "    ))\n",
    "    print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "        metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro: 0.5774, Recall macro: 0.4122, F1 macro: 0.465 \n",
      "Precision micro: 0.8468, Recall micro: 0.7091, F1 micro: 0.7719 \n"
     ]
    }
   ],
   "source": [
    "print_results(test_model(wiki_loaders[\"val\"], model, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision_macro': 0.5774363173936341,\n",
       " 'recall_macro': 0.4122008262609475,\n",
       " 'f1_macro': 0.465017536999072,\n",
       " 'precision_micro': 0.8468279004556607,\n",
       " 'recall_micro': 0.7091282653360728,\n",
       " 'f1_micro': 0.7718849840255592}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(wiki_loaders[\"val\"], model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\n",
    "    \"experiment\", \n",
    "    \"precision_macro\", \"recall_macro\", \"f1_macro\", \n",
    "    \"precision_micro\", \"recall_micro\", \"f1_micro\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_metrics = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "dict_metrics = {key: round(value, 4) for key, value in metrics_dict.items()}\n",
    "    \n",
    "dict_metrics.update({\"experiment\": \"Train on 20000 EN articles, validate on 2000.\"})\n",
    "df = df.append(dict_metrics, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Train on 20000 EN articles, validate on 2000.</td>\n",
       "      <td>0.5811</td>\n",
       "      <td>0.4114</td>\n",
       "      <td>0.4622</td>\n",
       "      <td>0.8467</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.7694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      experiment  precision_macro  \\\n",
       "0  Train on 20000 EN articles, validate on 2000.           0.5811   \n",
       "\n",
       "   recall_macro  f1_macro  precision_micro  recall_micro  f1_micro  \n",
       "0        0.4114    0.4622           0.8467         0.705    0.7694  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Train on 20K EN articles, validate on 2K.</td>\n",
       "      <td>0.5811</td>\n",
       "      <td>0.4114</td>\n",
       "      <td>0.4622</td>\n",
       "      <td>0.8467</td>\n",
       "      <td>0.7050</td>\n",
       "      <td>0.7694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Train on 10K EN articles and 10K RU articles, ...</td>\n",
       "      <td>0.6146</td>\n",
       "      <td>0.4238</td>\n",
       "      <td>0.4815</td>\n",
       "      <td>0.8418</td>\n",
       "      <td>0.7318</td>\n",
       "      <td>0.7830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Train on 10K EN articles and 10K RU articles, ...</td>\n",
       "      <td>0.5893</td>\n",
       "      <td>0.4714</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.8343</td>\n",
       "      <td>0.7579</td>\n",
       "      <td>0.7942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Train on 10K EN articles and 10K RU articles, ...</td>\n",
       "      <td>0.5631</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.4168</td>\n",
       "      <td>0.8493</td>\n",
       "      <td>0.7082</td>\n",
       "      <td>0.7724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          experiment  precision_macro  \\\n",
       "0          Train on 20K EN articles, validate on 2K.           0.5811   \n",
       "1  Train on 10K EN articles and 10K RU articles, ...           0.6146   \n",
       "2  Train on 10K EN articles and 10K RU articles, ...           0.5893   \n",
       "3  Train on 10K EN articles and 10K RU articles, ...           0.5631   \n",
       "\n",
       "   recall_macro  f1_macro  precision_micro  recall_micro  f1_micro  \n",
       "0        0.4114    0.4622           0.8467        0.7050    0.7694  \n",
       "1        0.4238    0.4815           0.8418        0.7318    0.7830  \n",
       "2        0.4714    0.5078           0.8343        0.7579    0.7942  \n",
       "3        0.3612    0.4168           0.8493        0.7082    0.7724  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"results/multilingual_results.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv(\"results_tuning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optimizer</th>\n",
       "      <th>num_hidden</th>\n",
       "      <th>dim_hidden</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>SWA</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  optimizer num_hidden dim_hidden  dropout_rate  learning_rate num_epochs  \\\n",
       "0       SWA          2        150           0.2           0.01         10   \n",
       "\n",
       "   precision_macro  recall_macro  f1_macro  precision_micro  recall_micro  \\\n",
       "0              NaN           NaN       NaN              NaN           NaN   \n",
       "\n",
       "   f1_micro  \n",
       "0       NaN  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
