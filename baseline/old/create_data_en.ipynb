{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import nltk\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "\n",
    "import wiki_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = \"/scratch/mz2476/wiki/data/aligned_datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load json, REMOVE categories, tokenize\n",
    "Also: remove_short_words, remove_stop_words, remove_empty_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53fce556c7b466284a3e1c70e9cb400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser = wiki_parser.Parser(\"hindi\")\n",
    "wiki_df = parser.get_wiki_tokenized_dataset(\n",
    "    PATH_TO_DATA_FOLDER + \"wikitext_topics_en_filtered.json\", \n",
    "    extract_section=True, extract_outlinks=True,\n",
    ")\n",
    "if SAVE:\n",
    "    pkl.dump(wiki_df, open(PATH_TO_DATA_FOLDER + \"wikitext_tokenized_text_sections_outlinks_en.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33823, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, preprocess it and save \n",
    "Output files:\n",
    "`vocab_train_en.pt`, `wiki_tensor_dataset_en.pt`, `classes_list.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data (the functions are in `preprocess.py`):\n",
    "<ol>\n",
    "    <li> Remove stopwords. </li>\n",
    "    <li> Remove rows with missing labels. </li>\n",
    "    <li> Remove rows with no tokens. </li>\n",
    "    <li> Create a set of all categories. Binarize the labels. </li>\n",
    "    <li> Split in train/val/test. </li>\n",
    "    <li> Build vocabulary for train. </li>\n",
    "</ol>\n",
    "\n",
    "Make DataLoader:\n",
    "<ol>\n",
    "    <li> Tokenize train/val/test. </li>\n",
    "    <li> Create batches using collate function that pads the short sentences. </li>\n",
    "</ol>\n",
    "\n",
    "Use pretrained embeddings:\n",
    "<ol>\n",
    "    <li> Load pretrained embeddings. </li>\n",
    "    <li> Create embedding matrix for given vocabulary. Words that are in given vocabualry but not in pretrained embeddings have zero embedding vector. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the dataframe from pickle file\n",
    "import pickle as pkl\n",
    "\n",
    "wiki_df =  pkl.load(open(PATH_TO_DATA_FOLDER + \"wikitext_en_tokenized.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mz2476/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import preprocess\n",
    "import importlib\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from preprocess import remove_stop_words, train_validate_test_split\n",
    "from preprocess import tokenize_dataset, TensoredDataset, pad_collate_fn\n",
    "from preprocess import create_vocab_from_tokens, create_lookups_for_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_with_no_labels(wiki_df):\n",
    "    pass\n",
    "\n",
    "def remove_rows_with_no_tokens(wiki_df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99969, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing rows with missing labels\n",
    "mask = wiki_df.mid_level_categories.apply(lambda x: len(x) > 0)\n",
    "wiki_df = wiki_df[mask]\n",
    "wiki_df = wiki_df.reset_index(drop=True)\n",
    "wiki_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99960, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing rows with no tokens\n",
    "mask = wiki_df.tokens.apply(lambda x: len(x) > 0)\n",
    "wiki_df = wiki_df[mask]\n",
    "wiki_df = wiki_df.reset_index(drop=True)\n",
    "wiki_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n",
      "['Culture.Arts' 'Culture.Broadcasting' 'Culture.Crafts and hobbies'\n",
      " 'Culture.Entertainment' 'Culture.Food and drink' 'Culture.Games and toys'\n",
      " 'Culture.Internet culture' 'Culture.Language and literature'\n",
      " 'Culture.Media' 'Culture.Music' 'Culture.Performing arts'\n",
      " 'Culture.Philosophy and religion' 'Culture.Plastic arts' 'Culture.Sports'\n",
      " 'Culture.Visual arts' 'Geography.Africa' 'Geography.Americas'\n",
      " 'Geography.Antarctica' 'Geography.Asia' 'Geography.Bodies of water'\n",
      " 'Geography.Europe' 'Geography.Landforms' 'Geography.Maps'\n",
      " 'Geography.Oceania' 'Geography.Parks'\n",
      " 'History_And_Society.Business and economics'\n",
      " 'History_And_Society.Education' 'History_And_Society.History and society'\n",
      " 'History_And_Society.Military and warfare'\n",
      " 'History_And_Society.Politics and government'\n",
      " 'History_And_Society.Transportation' 'STEM.Biology' 'STEM.Chemistry'\n",
      " 'STEM.Engineering' 'STEM.Geosciences' 'STEM.Information science'\n",
      " 'STEM.Mathematics' 'STEM.Medicine' 'STEM.Meteorology' 'STEM.Physics'\n",
      " 'STEM.Science' 'STEM.Space' 'STEM.Technology' 'STEM.Time']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q2000864</td>\n",
       "      <td>[Culture.Philosophy and religion]</td>\n",
       "      <td>[affirming, consequent, sometimes, called, con...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q1064113</td>\n",
       "      <td>[History_And_Society.Business and economics]</td>\n",
       "      <td>[growth, two, six, two, zero, one, six, zero, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q6941060</td>\n",
       "      <td>[Geography.Europe]</td>\n",
       "      <td>[museum, work, arbetets, museum, swedish, muse...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q843920</td>\n",
       "      <td>[History_And_Society.History and society, STEM...</td>\n",
       "      <td>[like, one, dorset, england, arable, land, lat...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q178999</td>\n",
       "      <td>[STEM.Biology, STEM.Medicine]</td>\n",
       "      <td>[axon, greek, axis, nerve, fiber, long, slende...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                               mid_level_categories  \\\n",
       "0  Q2000864                  [Culture.Philosophy and religion]   \n",
       "1  Q1064113       [History_And_Society.Business and economics]   \n",
       "2  Q6941060                                 [Geography.Europe]   \n",
       "3   Q843920  [History_And_Society.History and society, STEM...   \n",
       "4   Q178999                      [STEM.Biology, STEM.Medicine]   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [affirming, consequent, sometimes, called, con...   \n",
       "1  [growth, two, six, two, zero, one, six, zero, ...   \n",
       "2  [museum, work, arbetets, museum, swedish, muse...   \n",
       "3  [like, one, dorset, england, arable, land, lat...   \n",
       "4  [axon, greek, axis, nerve, fiber, long, slende...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize the labels\n",
    "# labels list: mlb.classes_\n",
    "mlb = MultiLabelBinarizer()\n",
    "wiki_df[\"labels\"] = list(mlb.fit_transform(wiki_df.mid_level_categories))\n",
    "\n",
    "if SAVE:\n",
    "    # SAVE classes list\n",
    "    torch.save(mlb.classes_, PATH_TO_DATA_FOLDER + 'classes_list.pt')\n",
    "    print(\"Saved.\")\n",
    "\n",
    "# LOAD\n",
    "classes = torch.load(PATH_TO_DATA_FOLDER + 'classes_list.pt')\n",
    "mlb = MultiLabelBinarizer(classes)\n",
    "\n",
    "print(classes)\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "# train/val/test split\n",
    "wiki_train, wiki_valid, wiki_test = train_validate_test_split(wiki_df, seed=1)\n",
    "\n",
    "wiki_train = wiki_train.reset_index(drop=True)\n",
    "wiki_valid = wiki_valid.reset_index(drop=True)\n",
    "wiki_test = wiki_test.reset_index(drop=True)\n",
    "\n",
    "if SAVE:\n",
    "    # SAVE train/val/test dfs\n",
    "    torch.save(wiki_train, PATH_TO_DATA_FOLDER + \"df_wiki_train_en.pt\")\n",
    "    torch.save(wiki_valid, PATH_TO_DATA_FOLDER + \"df_wiki_valid_en.pt\")\n",
    "    torch.save(wiki_test, PATH_TO_DATA_FOLDER + \"df_wiki_test_en.pt\")\n",
    "    print(\"Saved.\")\n",
    "\n",
    "wiki_train = torch.load(PATH_TO_DATA_FOLDER + \"df_wiki_train_en.pt\")\n",
    "wiki_valid = torch.load(PATH_TO_DATA_FOLDER + \"df_wiki_valid_en.pt\")\n",
    "wiki_test = torch.load(PATH_TO_DATA_FOLDER + \"df_wiki_test_en.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Q5346784</td>\n",
       "      <td>[Culture.Language and literature]</td>\n",
       "      <td>[edwin, romanzo, elmer, one, eight, five, zero...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q4723109</td>\n",
       "      <td>[Culture.Language and literature]</td>\n",
       "      <td>[alfred, george, fysh, machin, born, one, eigh...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Q1456016</td>\n",
       "      <td>[Geography.Americas, Culture.Music]</td>\n",
       "      <td>[late, friends, first, full, length, studio, a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q59149462</td>\n",
       "      <td>[Geography.Americas, Culture.Sports, Culture.L...</td>\n",
       "      <td>[mat, alexis, romero, born, one, february, one...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Q30602920</td>\n",
       "      <td>[Culture.Plastic arts, Geography.Americas, Cul...</td>\n",
       "      <td>[confederate, memorial, fountain, historic, fo...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         QID                               mid_level_categories  \\\n",
       "0   Q5346784                  [Culture.Language and literature]   \n",
       "1   Q4723109                  [Culture.Language and literature]   \n",
       "2   Q1456016                [Geography.Americas, Culture.Music]   \n",
       "3  Q59149462  [Geography.Americas, Culture.Sports, Culture.L...   \n",
       "4  Q30602920  [Culture.Plastic arts, Geography.Americas, Cul...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [edwin, romanzo, elmer, one, eight, five, zero...   \n",
       "1  [alfred, george, fysh, machin, born, one, eigh...   \n",
       "2  [late, friends, first, full, length, studio, a...   \n",
       "3  [mat, alexis, romero, born, one, february, one...   \n",
       "4  [confederate, memorial, fountain, historic, fo...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train vocab size is: 595364\n",
      "All vocab size is: 682848\n"
     ]
    }
   ],
   "source": [
    "# Building vocabulary: train and all\n",
    "vocab_train = create_vocab_from_tokens(wiki_train['tokens'])\n",
    "vocab_val = create_vocab_from_tokens(wiki_valid['tokens'])\n",
    "vocab_test = create_vocab_from_tokens(wiki_test['tokens'])\n",
    "\n",
    "vocab_all = create_vocab_from_tokens([vocab_train, vocab_val, vocab_test])\n",
    "\n",
    "print(\"Train vocab size is: {}\".format(len(vocab_train)))\n",
    "print(\"All vocab size is: {}\".format(len(vocab_all)))\n",
    "\n",
    "index_to_word_train, word_to_index_train = create_lookups_for_vocab(vocab_train, add_tokens_list=[\"<pad>\", \"<unk>\"])\n",
    "index_to_word_all, word_to_index_all = create_lookups_for_vocab(vocab_all, add_tokens_list=[\"<pad>\", \"<unk>\"])\n",
    "\n",
    "if SAVE:\n",
    "    # SAVE vocab from train, all\n",
    "    torch.save(index_to_word_train, PATH_TO_DATA_FOLDER + 'vocab_train_en.pt')\n",
    "    torch.save(index_to_word_all, PATH_TO_DATA_FOLDER + 'vocab_all_en.pt')\n",
    "    print(\"Saved.\")\n",
    "\n",
    "# LOAD\n",
    "vocab_train = torch.load(PATH_TO_DATA_FOLDER + 'vocab_train_en.pt')\n",
    "vocab_all = torch.load(PATH_TO_DATA_FOLDER + 'vocab_all_en.pt')\n",
    "\n",
    "index_to_word_train, word_to_index_train = create_lookups_for_vocab(vocab_train)\n",
    "index_to_word_all, word_to_index_all = create_lookups_for_vocab(vocab_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79968/79968 [00:09<00:00, 8749.83it/s] \n",
      "100%|██████████| 9996/9996 [00:00<00:00, 12383.81it/s]\n",
      "100%|██████████| 9996/9996 [00:00<00:00, 13668.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "# CHANGE max number of tokens per article\n",
    "max_num_tokens = None\n",
    "\n",
    "# # specify vocabulary (word_to_index): 2 options\n",
    "vocab_name = \"vocab_train\" \n",
    "word_to_index = word_to_index_train\n",
    "# OR\n",
    "# vocab_name = \"vocab_all\"\n",
    "# word_to_index = word_to_index_all\n",
    "\n",
    "wiki_tokenized_datasets = {}\n",
    "wiki_tokenized_datasets['X_train'] = tokenize_dataset(wiki_train, word_to_index, max_num_tokens=max_num_tokens)\n",
    "wiki_tokenized_datasets['X_val'] = tokenize_dataset(wiki_valid, word_to_index, max_num_tokens=max_num_tokens)\n",
    "wiki_tokenized_datasets['X_test'] = tokenize_dataset(wiki_test, word_to_index, max_num_tokens=max_num_tokens)\n",
    "\n",
    "wiki_tokenized_datasets['y_train'] = list(wiki_train.labels)\n",
    "wiki_tokenized_datasets['y_val'] = list(wiki_valid.labels)\n",
    "wiki_tokenized_datasets['y_test'] = list(wiki_test.labels)\n",
    "\n",
    "wiki_tensor_dataset = {}\n",
    "wiki_tensor_dataset['train'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_train'], wiki_tokenized_datasets['y_train']\n",
    ")\n",
    "wiki_tensor_dataset['val'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_val'], wiki_tokenized_datasets['y_val']\n",
    ")\n",
    "wiki_tensor_dataset['test'] = TensoredDataset(\n",
    "    wiki_tokenized_datasets['X_test'], wiki_tokenized_datasets['y_test']\n",
    ")\n",
    "\n",
    "if SAVE:\n",
    "    # SAVE tensor datasets\n",
    "    torch.save(wiki_tensor_dataset, f'{PATH_TO_DATA_FOLDER}wiki_tensor_dataset_{vocab_name}_en.pt')\n",
    "    print(\"Saved.\")\n",
    "\n",
    "# LOAD\n",
    "wiki_tensor_dataset = torch.load(f'{PATH_TO_DATA_FOLDER}wiki_tensor_dataset_{vocab_name}_en.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 595366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([13030,  8330,  3721,  8330,  3721,   132,  2496, 13031,  4719,  3982,\n",
       "         13031,  3178,   303,  5510, 13032,  8334,  2496, 13031,  4719,  1828,\n",
       "          2496,  1985, 13033, 10701, 13034,     7,  5299,  2338,  6948,     5,\n",
       "             9,     9,     8, 10510,   480, 13035, 13036, 11814, 13035, 13036,\n",
       "           965,   933,  2789,     5,   223,    10,   933, 13037,  6777,  1646,\n",
       "          3271, 13038,  2496, 13031,  4719,  1036, 13039,  1985,  2300,  1495,\n",
       "           601, 13040,  1495,     5,     9,   208,     6,     5,     9,     9,\n",
       "            11,   568,     5,     9,     9,   208, 13041,  1467,   403, 13042,\n",
       "          9309,  1065, 13043, 13044, 13043, 13044,  2300,  2189,  1880,  8330,\n",
       "          4719,   452,    10,     8,     8,     8, 13035, 13036,    21, 13045,\n",
       "          2300, 13045,  2641,  3721,  4340,  4251, 13043, 13044, 13046,  2496,\n",
       "         13031,  4719,  4340, 13045, 13047, 13048, 13049, 13050,  5496,  9571,\n",
       "           648,     5,     9,    10,     8,     5,    11, 13051,  6945,  9127,\n",
       "          2496,    53,  5458, 13051,  6945,  9127,  6945,  9127,  2025,   833,\n",
       "          6777,  3721, 13052,  3271, 13053, 13054,  3721, 11814, 13035, 13036,\n",
       "           689,  1261,    10,     8,     5,   253, 13055,  2496, 13031,  4719,\n",
       "         12197, 13052,  3271, 13053,  3721,  5251,   952,  3078,   167,  6674,\n",
       "            10,  8340,   439,  1366, 13056,   952,  1199,    10,     8,     5,\n",
       "             6,  3541,  1816,   794,    10,     8,     8,   611,   532,    10,\n",
       "             8,    10,     8,  1108, 13057,  2471,  5251,   952,   439,  8340,\n",
       "          1140,  2496, 13031,  4719,   132, 10436,  3078,  3271, 13053,   132,\n",
       "            58, 13058,   414, 13031,   132,   277,  2496,  2076, 13031,  1869,\n",
       "           525,    24, 13031,  4719,   526,  8330,  8340,  2496,    81,   397,\n",
       "           398, 13059,  3435,  2869,  2496, 13031,  8330,  8340,  2055,     5,\n",
       "             9,     9,     8,  8330,  8340,  2496, 13035, 13036]),\n",
       " tensor([248.]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vocab size:\", len(word_to_index))\n",
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Next step after loading tensor dataset -- create dataloader\n",
    "# wiki_loaders = {}\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "#     wiki_loaders[split] = DataLoader(\n",
    "#         wiki_dataset, \n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=True, \n",
    "#         collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
