{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import nltk\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convCategories(categories, category_to_index):\n",
    "    return [category_to_index[category] for category in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.8, validate_percent=.1, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, word_to_index):\n",
    "    _current_dictified = []\n",
    "    for l in tqdm(dataset['tokens']):\n",
    "        encoded_l = [word_to_index[i] if i in word_to_index else word_to_index['<UNK>'] for i in l]\n",
    "        _current_dictified.append(encoded_l)\n",
    "    return _current_dictified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensoredDataset(Dataset):\n",
    "    def __init__(self, list_of_lists_of_tokens, labels):\n",
    "        self.input_tensors = []\n",
    "        self.target_tensors = []\n",
    "        \n",
    "        for i in range(0, len(list_of_lists_of_tokens)):\n",
    "            self.input_tensors.append(torch.tensor(list_of_lists_of_tokens[i], dtype=torch.long))\n",
    "            self.target_tensors.append(torch.tensor(labels[i], dtype=torch.long))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.input_tensors[idx], self.target_tensors[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:\n",
    "        #print(t.reshape(1, -1).shape)\n",
    "        #print(torch.tensor([[pad_token]*(max_length - t.size(-1))])[0].shape)\n",
    "        padded_tensor = torch.cat([t.reshape(1, -1), torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = word_to_index['<PAD>']\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    #target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
    "    \n",
    "    return input_tensor, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = 'wikitext_tokenized.p'\n",
    "wiki_df =  pkl.load(open(OUTPUT_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for i in list(wiki_df['mid_level_categories']):\n",
    "    categories.extend(i)\n",
    "categories = list(set(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_index = {categories[i]:i for i in range(0, len(categories))}\n",
    "index_to_category = {v:k for k, v in category_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df['category_tokens'] = wiki_df.apply(lambda row: convCategories(row['mid_level_categories'], category_to_index), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train, wiki_valid, wiki_test = train_validate_test_split(wiki_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(wiki_train['category_tokens'])\n",
    "y_val = list(wiki_valid['category_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([y for x in list(wiki_train['tokens']) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605800"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "for word in vocab:\n",
    "    if(word not in word_to_index):\n",
    "        word_to_index[word]=len(word_to_index)\n",
    "index_to_word = {v:k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82547/82547 [00:09<00:00, 8974.95it/s] \n"
     ]
    }
   ],
   "source": [
    "wiki_tokenized_train = tokenize_dataset(wiki_train, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10318/10318 [00:01<00:00, 8928.07it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_tokenized_val = tokenize_dataset(wiki_valid, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tokenized_datasets = {}\n",
    "wiki_tokenized_datasets['train'] = wiki_tokenized_train\n",
    "wiki_tokenized_datasets['val'] = wiki_tokenized_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tensor_dataset = {}\n",
    "wiki_tensor_dataset['train'] = TensoredDataset(wiki_tokenized_datasets['train'], y_train)\n",
    "wiki_tensor_dataset['val'] = TensoredDataset(wiki_tokenized_datasets['val'], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inp, target) in enumerate(wiki_loaders['train']):\n",
    "    #print(inp, target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4644])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
