{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import spacy\n",
    "import string\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import mwparserfromhell\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pj891/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# set file names for dumping pickle files and input data source\n",
    "FILE_NAME = 'sample_1pct_wikitext.json'\n",
    "\n",
    "PRETRAINED_EMBEDDINGS = 'pretrained-embeddings/wiki.en.align.vec'#wiki-news-300d-1M.vec'\n",
    "\n",
    "TEXT_OUTPUT_FILE = 'tokenized-data/wikitext_tokenized.p'\n",
    "SECTION_OUTPUT_FILE = 'tokenized-data/wikisection_tokenized.p'\n",
    "\n",
    "#OUTPUT_FILE_2 = 'tokenized-data/wikitext_tokenized_reduced.p'\n",
    "\n",
    "WIKI_TRAIN_TOKENIZED_FILE = 'tokenized-data/wiki_train_tokenized.p'\n",
    "WIKI_VALID_TOKENIZED_FILE = 'tokenized-data/wiki_valid_tokenized.p'\n",
    "WIKI_TEST_TOKENIZED_FILE  = 'tokenized-data/wiki_test_tokenized.p'\n",
    "\n",
    "MAX_ARTICLE_LENGTH = 500\n",
    "\n",
    "# downloading and setting stop word list from NLTK\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "# clean text using regex - similar to what is used in FastText paper\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    patterns = [\n",
    "        \"{{.*}}\"\n",
    "        ,\"&amp;\"\n",
    "        ,\"&lt;\"\n",
    "        ,\"&gt;\"\n",
    "        ,\"<ref[^<]*<\\/ref>\"\n",
    "        ,\"<[^>]*>\"\n",
    "        ,\"\\|left\"\n",
    "        ,\"\\|\\d+px\"\n",
    "        ,\"\\[\\[category:\"\n",
    "        ,r\"(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b\"\n",
    "        ,\"\\|thumb\"\n",
    "        ,\"\\|right\"\n",
    "        ,\"\\[\\[image:[^\\[\\]]*\"\n",
    "        ,\"\\[\\[category:([^|\\]]*)[^]]*\\]\\]\"\n",
    "        ,\"\\[\\[[a-z\\-]*:[^\\]]*\\]\\]\"\n",
    "        ,\"\\[\"\n",
    "        ,\"\\]\"\n",
    "        ,\"\\{[^\\}]*\\}\"\n",
    "        ,r\"\\n\"\n",
    "        ,r\"[^a-zA-Z0-9 ]\"\n",
    "        ,r\"\\b[a-zA-Z]\\b\"\n",
    "        ,\" +\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        cleanr = re.compile(pattern)\n",
    "        text = re.sub(cleanr, ' ', text)\n",
    "    return text\n",
    "\n",
    "# covert numerals to their text equivalent\n",
    "def subsitute(text):\n",
    "    return text.strip().replace('0', ' zero') \\\n",
    "                        .replace('1',' one') \\\n",
    "                        .replace('2',' two') \\\n",
    "                        .replace('3',' three') \\\n",
    "                        .replace('4',' four') \\\n",
    "                        .replace('5',' five') \\\n",
    "                        .replace('6',' six') \\\n",
    "                        .replace('7',' seven') \\\n",
    "                        .replace('8',' eight') \\\n",
    "                        .replace('9',' nine')\n",
    "\n",
    "# remove empty token generated from inserting blank spaces\n",
    "def remove_empty_token(tokens):\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if not token.strip() == '':\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "# optional - remove other common stop words \n",
    "# get the stop words from NLTK package\n",
    "def remove_stop_words(tokens):\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if not token in STOP_WORDS:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "# optional - remove words less than 3 character long\n",
    "def remove_short_words(tokens):\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if len(token) >= 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "# get the tokenized dataframe containing - QID, Word Tokens & Categories\n",
    "def get_wiki_tokenized_dataset(file_name, extract_section=False):\n",
    "    wiki_dict = []\n",
    "    with open(FILE_NAME) as file:\n",
    "         for line in file:\n",
    "            wiki_row = {}\n",
    "            line = json.loads(line.strip())\n",
    "            wikitext = mwparserfromhell.parse(line['wikitext'])\n",
    "            wiki_row['QID'] = line['QID']\n",
    "            wiki_row['mid_level_categories'] = line['mid_level_categories']\n",
    "            if extract_section:\n",
    "                sections = wikitext.filter_headings()  \n",
    "                wiki_row['tokens'] = tokenize(subsitute(clean(str(sections))))\n",
    "            else:\n",
    "                wiki_row['tokens'] = tokenize(subsitute(clean(str(wikitext))))\n",
    "            wiki_dict.append(wiki_row)\n",
    "            \n",
    "    wiki_df = pd.DataFrame(wiki_dict)\n",
    "    wiki_df['tokens'] = wiki_df['tokens'].apply(remove_empty_token)\n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #save the Wiki SECTION dataset in pickle file for subsequent use\n",
    "# wiki_df = get_wiki_tokenized_dataset(FILE_NAME, True)\n",
    "# pkl.dump(wiki_df, open(SECTION_OUTPUT_FILE, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save the Wiki TEXT dataset in pickle file for subsequent use\n",
    "# # Download the pickle files from Google Drive (else creating new will take ~4 hours)\n",
    "# # https://drive.google.com/open?id=1DlNxxNh6WA5ds7px844LnBbhEzV0Ydyo\n",
    "# wiki_df = get_wiki_tokenized_dataset(FILE_NAME)\n",
    "# pkl.dump(wiki_df, open(TEXT_OUTPUT_FILE, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe from pickle file - Wiki Text\n",
    "wiki_df =  pkl.load(open(TEXT_OUTPUT_FILE, \"rb\"))\n",
    "\n",
    "# load the dataframe from pickle file - Wiki Sections\n",
    "#wiki_df =  pkl.load(open(SECTION_OUTPUT_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df['tokens'] = wiki_df['tokens'].apply(remove_short_words)\n",
    "wiki_df['tokens'] = wiki_df['tokens'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series([word for article in wiki_df.tokens for word in article])\n",
    "token_df = a.value_counts().sort_index().rename_axis('word').reset_index(name='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>aaa</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>aaaa</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>aaaaa</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>aaaayaaj</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>aaai</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694607</td>\n",
       "      <td>zytek</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694623</td>\n",
       "      <td>zyx</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694625</td>\n",
       "      <td>zyxin</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694629</td>\n",
       "      <td>zyzzyva</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694646</td>\n",
       "      <td>zzyzx</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  count\n",
       "0            aaa    561\n",
       "1           aaaa     41\n",
       "2          aaaaa      9\n",
       "15      aaaayaaj      6\n",
       "22          aaai     10\n",
       "...          ...    ...\n",
       "694607     zytek      7\n",
       "694623       zyx     15\n",
       "694625     zyxin      8\n",
       "694629   zyzzyva      7\n",
       "694646     zzyzx     15\n",
       "\n",
       "[156408 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_freq_df = token_df[token_df['count'] > 5]\n",
    "tokens_freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = wiki_df['tokens'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = b.value_counts().sort_index().rename_axis('count').reset_index(name='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x16f485b00>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_df.hist(column='count',bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL - Remove in-frequent tokens from corpus (words repeated less than 100 times) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infrequent_tokens_df = token_df[token_df['count'] <= 5]\n",
    "infrequent_tokens = infrequent_tokens_df['word'].tolist()\n",
    "infrequent_tokens = set(infrequent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove infrequent tokens\n",
    "def remove_infrequent_words(tokens):\n",
    "    result = [token for token in tokens if token not in infrequent_tokens]\n",
    "    return result\n",
    "\n",
    "wiki_df['tokens'] = wiki_df['tokens'].apply(remove_infrequent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into Train, Valid & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.8, validate_percent=.1, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train, wiki_valid, wiki_test = train_validate_test_split(wiki_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained FASTText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained fastText embeddings\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:]))).astype('float32')\n",
    "    return data\n",
    "\n",
    "# # Download from - https://drive.google.com/open?id=1vfoiWQkEjyNXRyi0JzA8Aq5Zzjfcpo2w\n",
    "pretrained_embs = load_vectors(PRETRAINED_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique tokens in the training dataset\n",
    "all_train_tokens = []\n",
    "\n",
    "for tokens in wiki_train['tokens']:\n",
    "    for token in tokens:\n",
    "        all_train_tokens.append(token)\n",
    "        \n",
    "all_train_tokens = list(set(all_train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the wiki tokens and fastText embeddings\n",
    "train_token_count = len(all_train_tokens)\n",
    "token_in_fasttext = []\n",
    "token_not_in_fasttext = []\n",
    "train_token_in_fasttxt = 0\n",
    "\n",
    "for token in all_train_tokens:\n",
    "    if token in pretrained_embs.keys():\n",
    "        token_in_fasttext.append(token)\n",
    "        train_token_in_fasttxt = train_token_in_fasttxt + 1\n",
    "    else:\n",
    "        token_not_in_fasttext.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of unique Wiki tokens in fASTText : 79.02%\n",
      "unique tokens not in fastText : 126252\n",
      "unique tokens in fastText : 475506\n"
     ]
    }
   ],
   "source": [
    "print(\"% of unique Wiki tokens in fASTText : {:4.2f}%\".format(train_token_in_fasttxt/train_token_count*100))\n",
    "print('unique tokens not in fastText :', len(token_not_in_fasttext))\n",
    "print('unique tokens in fastText :', len(token_in_fasttext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens not in the fastText\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['osaji',\n",
       " 'ankimobile',\n",
       " 'comastichus',\n",
       " 'gymnaium',\n",
       " 'riverwalkers',\n",
       " 'evilminded',\n",
       " 'fjellstr',\n",
       " 'thaiprisonlife',\n",
       " 'haigamai',\n",
       " 'kinematron']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample tokens not in the fastText\")\n",
    "token_not_in_fasttext[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running test to investigate which words are not in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_dict = []\n",
    "article_percentage = []\n",
    "for idx, tokens in enumerate(wiki_train['tokens']):\n",
    "    count = 0\n",
    "    tokens_not_in_fasttext = []\n",
    "    fasttext_row = {}\n",
    "    for token in tokens:\n",
    "        if token in pretrained_embs.keys():\n",
    "            count = count + 1\n",
    "        else :\n",
    "            tokens_not_in_fasttext.append(token)\n",
    "    \n",
    "    if len(tokens) > 0:\n",
    "        percent = count/len(tokens) * 100\n",
    "    else:\n",
    "        percent = 100\n",
    "    \n",
    "    article_percentage.append(percent)\n",
    "    \n",
    "    if percent < 75:\n",
    "        fasttext_row['QID'] = wiki_train.iloc[idx]['QID'] \n",
    "        fasttext_row['Coverage'] = percent \n",
    "        fasttext_row['Tokens not in fastText'] = tokens_not_in_fasttext\n",
    "        fasttext_row['Total Tokens'] = len(tokens)\n",
    "        fasttext_dict.append(fasttext_row)\n",
    "        \n",
    "fasttext_df = pd.DataFrame(fasttext_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_df = fasttext_df.sort_values(by=['Coverage','Total Tokens'])\n",
    "fasttext_df.to_csv('fastText-analysis.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average % of tokens coverage across articles in fastText : 98.78327125561432\n"
     ]
    }
   ],
   "source": [
    "print(\"Average % of tokens coverage across articles in fastText :\", sum(article_percentage)/len(wiki_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_NAME) as file:\n",
    "    for line in file:\n",
    "        line = json.loads(line.strip())\n",
    "        if line['QID'] == 'Q4464501':\n",
    "            wiki_text = mwparserfromhell.parse(line['wikitext'])\n",
    "            cleaned_text = subsitute(clean(str(wiki_text)))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build binarize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique categories in the training dataset\n",
    "all_categories = []\n",
    "\n",
    "for categories in wiki_train.mid_level_categories:\n",
    "    for category in categories:\n",
    "        all_categories.append(category)\n",
    "        \n",
    "unique_categories = list(set(all_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the multi-label binarizer on the categories and claims in the training dataset\n",
    "multi_label_binarizer_categories = MultiLabelBinarizer(classes=unique_categories)\n",
    "\n",
    "def binarize_target(result_df):\n",
    "    result_df = result_df.join(pd.DataFrame(multi_label_binarizer_categories.fit_transform(result_df.pop('mid_level_categories')),\n",
    "                          columns=multi_label_binarizer_categories.classes_,\n",
    "                          index=result_df.index))\n",
    "    return result_df\n",
    "\n",
    "wiki_train = binarize_target(wiki_train)\n",
    "wiki_valid = binarize_target(wiki_valid)\n",
    "wiki_test = binarize_target(wiki_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
