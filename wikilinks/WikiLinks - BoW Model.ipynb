{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set source file name\n",
    "INLINKS_RAW_FILE_NAME = 'data/en_inlinks.json'\n",
    "\n",
    "OUTLINKS_PROCESSED_FILE = 'data/en_outlinks.p'\n",
    "INLINKS_PROCESSED_FILE = 'data/wiki_links_df.p'\n",
    "\n",
    "FINAL_PROCESSED_FILE = 'data/en_all.p'\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_links(file_name):\n",
    "    wiki_dict = []\n",
    "    with open(FILE_NAME) as file:\n",
    "         for line in file:\n",
    "            wiki_row = {}\n",
    "            line = json.loads(line.strip())\n",
    "            wiki_row['qid'] = line['qid']\n",
    "            wiki_row['title'] = line['entitle']\n",
    "            wiki_row['rid'] = line['rid']\n",
    "            wiki_row['pid'] = line['pid']\n",
    "            wiki_row['inlinks'] = line['inlinks']\n",
    "            wiki_row['mid_level_categories'] = line['mid_level_categories']\n",
    "            wiki_dict.append(wiki_row)\n",
    "            \n",
    "    wiki_df = pd.DataFrame(wiki_dict)\n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_df = get_wiki_links(INLINKS_RAW_FILE_NAME)\n",
    "# pkl.dump(wiki_df, open(INLINKS_PROCESSED_FILE, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe from pickle file \n",
    "wiki_df =  pkl.load(open(INLINKS_PROCESSED_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>title</th>\n",
       "      <th>rid</th>\n",
       "      <th>pid</th>\n",
       "      <th>inlinks</th>\n",
       "      <th>mid_level_categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q18022170</td>\n",
       "      <td>Edwin Brown (actor)</td>\n",
       "      <td>853705188</td>\n",
       "      <td>43808614</td>\n",
       "      <td>[[158976, 0], [187709, 0], [242465, 0], [36406...</td>\n",
       "      <td>[Culture.People]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q28452584</td>\n",
       "      <td>A Merry Little Christmas (Matt Brouwer album)</td>\n",
       "      <td>895288516</td>\n",
       "      <td>52174626</td>\n",
       "      <td>[[14619462, 0], [27136451, 0], [35008514, 0], ...</td>\n",
       "      <td>[Culture.Music]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid                                          title        rid  \\\n",
       "0  Q18022170                            Edwin Brown (actor)  853705188   \n",
       "1  Q28452584  A Merry Little Christmas (Matt Brouwer album)  895288516   \n",
       "\n",
       "        pid                                            inlinks  \\\n",
       "0  43808614  [[158976, 0], [187709, 0], [242465, 0], [36406...   \n",
       "1  52174626  [[14619462, 0], [27136451, 0], [35008514, 0], ...   \n",
       "\n",
       "  mid_level_categories  \n",
       "0     [Culture.People]  \n",
       "1      [Culture.Music]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Wikipedia:Namespace\n",
    "# create dictionary based on the namespaces currently in use in wikipedia\n",
    "subject_namespaces = {\n",
    "     0:'article', \n",
    "     2:'user',\n",
    "     4:'wikipedia', \n",
    "     6:'file', \n",
    "     8:'media-wiki', \n",
    "     10:'template',\n",
    "     12:'help',\n",
    "     14:'category',\n",
    "     100:'portal',\n",
    "     108:'book',\n",
    "     118:'draft',\n",
    "     710:'timed-text',\n",
    "     828:'module'\n",
    "    }\n",
    "    \n",
    "    \n",
    "talk_namespaces = {\n",
    "     1:'article', \n",
    "     3:'user',\n",
    "     5:'wikipedia', \n",
    "     7:'file', \n",
    "     9:'media-wiki', \n",
    "     11:'template',\n",
    "     13:'help',\n",
    "     15:'category',\n",
    "     101:'portal',\n",
    "     109:'book',\n",
    "     119:'draft',\n",
    "     711:'timed-text',\n",
    "     829:'module'\n",
    "    }\n",
    "\n",
    "\n",
    "# ignore list (namespaces not used any longer)\n",
    "ignore_namespaces = [446, 447, 2300, 2301, 2302, 2303]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_namespaces(tuples):\n",
    "    result = []\n",
    "    for _tuple in tuples:\n",
    "        if _tuple[1] not in result:\n",
    "            result.append(_tuple[1])\n",
    "    return result\n",
    "\n",
    "def expand_links(namespace, tuples):\n",
    "    result = []\n",
    "    for _tuple in tuples:\n",
    "        if _tuple[1] == namespace:\n",
    "            result.append(_tuple[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df['namespaces'] = wiki_df['inlinks'].apply(get_namespaces)\n",
    "\n",
    "# get the list of unique namespaces in the links\n",
    "unique_namespaces = []\n",
    "\n",
    "for lst in wiki_df.namespaces:\n",
    "    for item in lst:\n",
    "        if item not in unique_namespaces:\n",
    "            unique_namespaces.append(item)\n",
    "\n",
    "for item in ignore_namespaces:\n",
    "    if item in unique_namespaces:\n",
    "        unique_namespaces.remove(item)\n",
    "        \n",
    "for item in unique_namespaces:\n",
    "    if item % 2 == 0:\n",
    "        prefix = 'subject'\n",
    "        namespace = subject_namespaces[item]\n",
    "    else:\n",
    "        prefix = 'talk'\n",
    "        namespace = talk_namespaces[item]\n",
    "        \n",
    "    wiki_df[prefix + '_' + namespace] = wiki_df['inlinks'].apply((lambda x : expand_links(item, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for model\n",
    "columns_to_model = ['qid','pid','mid_level_categories', 'subject_article']\n",
    "\n",
    "wiki_inlinks = wiki_df[columns_to_model]\n",
    "wiki_inlinks = wiki_inlinks.rename(columns={'subject_article':\"pid_inlinks\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_outlinks = pkl.load(open(OUTLINKS_PROCESSED_FILE, \"rb\"))\n",
    "wiki_outlinks = wiki_outlinks.rename(columns={'outlinks':\"pid_outlinks\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_all = pd.merge(wiki_inlinks, wiki_outlinks, left_on='qid', right_on='QID', how='inner').drop(columns=['QID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries left after filtering\n",
      "(99770, 5)\n",
      "Print Sample Entries from Wiki-Missing Label\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>pid_inlinks</th>\n",
       "      <th>pid_outlinks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Q6936272</td>\n",
       "      <td>51735433</td>\n",
       "      <td>[]</td>\n",
       "      <td>[43730243]</td>\n",
       "      <td>[26773749, 39117688]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Q7878232</td>\n",
       "      <td>35730129</td>\n",
       "      <td>[]</td>\n",
       "      <td>[33943904]</td>\n",
       "      <td>[448609, 72243, 1098730, 48768, 1098730]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid       pid mid_level_categories pid_inlinks  \\\n",
       "111  Q6936272  51735433                   []  [43730243]   \n",
       "132  Q7878232  35730129                   []  [33943904]   \n",
       "\n",
       "                                 pid_outlinks  \n",
       "111                      [26773749, 39117688]  \n",
       "132  [448609, 72243, 1098730, 48768, 1098730]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing rows with missing labels\n",
    "mask = wiki_all.mid_level_categories.apply(lambda x: len(x) > 0)\n",
    "wiki_missing_labels = wiki_all[~mask]\n",
    "wiki_all = wiki_all[mask]\n",
    "wiki_all = wiki_all.reset_index(drop=True)\n",
    "print(\"Entries left after filtering\")\n",
    "print(wiki_all.shape)\n",
    "\n",
    "print(\"Print Sample Entries from Wiki-Missing Label\")\n",
    "wiki_missing_labels.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries left after filtering\n",
      "(96873, 5)\n",
      "Print Sample Entries from Wiki-Missing Inlinks and Outlinks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>pid_inlinks</th>\n",
       "      <th>pid_outlinks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Q1225863</td>\n",
       "      <td>32466306</td>\n",
       "      <td>[Geography.Europe]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[3463, 15940363]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Q6400557</td>\n",
       "      <td>32924115</td>\n",
       "      <td>[Culture.Philosophy and religion, Geography.Asia]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[14533, 250724, 799384, 28849, 33101939, 79938...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid       pid                               mid_level_categories  \\\n",
       "54  Q1225863  32466306                                 [Geography.Europe]   \n",
       "73  Q6400557  32924115  [Culture.Philosophy and religion, Geography.Asia]   \n",
       "\n",
       "   pid_inlinks                                       pid_outlinks  \n",
       "54          []                                   [3463, 15940363]  \n",
       "73          []  [14533, 250724, 799384, 28849, 33101939, 79938...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing rows with no inlinks or oulinks tokens\n",
    "mask_1 = wiki_all.pid_inlinks.apply(lambda x: len(x) > 0)\n",
    "mask_2 = wiki_all.pid_outlinks.apply(lambda x: len(x) > 0)\n",
    "wiki_missing_inlinks_and_outlinks = wiki_all[~(mask_1 & mask_2)]\n",
    "wiki_all = wiki_all[mask_1 & mask_2]\n",
    "wiki_all = wiki_all.reset_index(drop=True)\n",
    "print(\"Entries left after filtering\")\n",
    "print(wiki_all.shape)\n",
    "print(\"Print Sample Entries from Wiki-Missing Inlinks and Outlinks\")\n",
    "wiki_missing_inlinks_and_outlinks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>pid_inlinks</th>\n",
       "      <th>pid_outlinks</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q18022170</td>\n",
       "      <td>43808614</td>\n",
       "      <td>[Culture.People]</td>\n",
       "      <td>[158976, 187709, 242465, 364061, 858518, 23705...</td>\n",
       "      <td>[31523, 35730609, 158976, 242465, 19101343]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q28452584</td>\n",
       "      <td>52174626</td>\n",
       "      <td>[Culture.Music]</td>\n",
       "      <td>[14619462, 27136451, 35008514, 30489911]</td>\n",
       "      <td>[14619462, 154403, 7572, 14619462, 27136451, 3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid       pid mid_level_categories  \\\n",
       "0  Q18022170  43808614     [Culture.People]   \n",
       "1  Q28452584  52174626      [Culture.Music]   \n",
       "\n",
       "                                         pid_inlinks  \\\n",
       "0  [158976, 187709, 242465, 364061, 858518, 23705...   \n",
       "1           [14619462, 27136451, 35008514, 30489911]   \n",
       "\n",
       "                                        pid_outlinks  \\\n",
       "0        [31523, 35730609, 158976, 242465, 19101343]   \n",
       "1  [14619462, 154403, 7572, 14619462, 27136451, 3...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize the labels\n",
    "# labels list: mlb.classes_\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "wiki_all[\"labels\"] = list(mlb.fit_transform(wiki_all.mid_level_categories))\n",
    "wiki_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pkl.dump(wiki_all, open(FINAL_PROCESSED_FILE, \"wb\"))\n",
    "wiki_all =  pkl.load(open(FINAL_PROCESSED_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.8, validate_percent=.1, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val/test split\n",
    "wiki_train, wiki_valid, wiki_test = train_validate_test_split(wiki_all, seed=1)\n",
    "\n",
    "wiki_train = wiki_train.reset_index(drop=True)\n",
    "wiki_valid = wiki_valid.reset_index(drop=True)\n",
    "wiki_test = wiki_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary\n",
    "vocab = list(set([y for x in list(wiki_train['pid_inlinks']) for y in x]))\n",
    "vocab += list(set([y for x in list(wiki_train['pid_outlinks']) for y in x]))\n",
    "\n",
    "#only get the unique tokens\n",
    "vocab = list(set(vocab))\n",
    "print(\"Vocab size is: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {\"<pad>\":0, \"<unk>\":1}\n",
    "for word in vocab:\n",
    "    if word not in word_to_index:\n",
    "        word_to_index[word] = len(word_to_index)\n",
    "index_to_word = {v:k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, word_to_index):\n",
    "    _current_dictified_1 = []\n",
    "    _current_dictified_2 = []\n",
    "    for l in tqdm(dataset['pid_inlinks']):\n",
    "        encoded_l = [word_to_index[i] if i in word_to_index else word_to_index['<unk>'] for i in l]\n",
    "        _current_dictified_1.append(encoded_l)\n",
    "    \n",
    "    for l in tqdm(dataset['pid_outlinks']):\n",
    "        encoded_l = [word_to_index[i] if i in word_to_index else word_to_index['<unk>'] for i in l]\n",
    "        _current_dictified_2.append(encoded_l)\n",
    "        \n",
    "    return _current_dictified_1, _current_dictified_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "\n",
    "class TensoredDataset(Dataset):\n",
    "    def __init__(self, tokenized_inlinks, tokenized_outlinks ,targets):\n",
    "        self.inlink_tensors = []\n",
    "        self.outlink_tensors = []\n",
    "        self.target_tensors = []\n",
    "        self.inlink_len = []\n",
    "        self.outlink_len = []\n",
    "        \n",
    "        for i in range(len(tokenized_inlinks)):\n",
    "            self.inlink_tensors.append(torch.LongTensor(tokenized_inlinks[i]))\n",
    "            self.outlink_tensors.append(torch.LongTensor(tokenized_outlinks[i]))\n",
    "            self.target_tensors.append(torch.LongTensor(targets[i]))\n",
    "            self.inlink_len.append(torch.FloatTensor([len(tokenized_inlinks[i])]))\n",
    "            self.outlink_len.append(torch.FloatTensor([len(tokenized_outlinks[i])]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inlink_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.inlink_tensors[idx], self.inlink_len[idx], self.outlink_tensors[idx], self.outlink_len[idx],self.target_tensors[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:\n",
    "        #print(t.reshape(1, -1).shape)\n",
    "        #print(torch.tensor([[pad_token]*(max_length - t.size(-1))])[0].shape)\n",
    "        padded_tensor = torch.cat([t.reshape(1, -1), torch.LongTensor([[pad_token]*(max_length - t.size(-1))])], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch, word_to_index):\n",
    "    # batch is a list of sample tuples\n",
    "    inlink_list = [s[0] for s in batch]\n",
    "    inlink_length = [s[1] for s in batch]\n",
    "    outlink_list = [s[2] for s in batch] \n",
    "    outlink_length = [s[3] for s in batch]\n",
    "    target_list = [s[4] for s in batch]\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = word_to_index['<pad>']\n",
    "    \n",
    "    inlink_tensor = pad_list_of_tensors(inlink_list, pad_token)    \n",
    "    inlink_length_tensor = torch.stack(inlink_length)\n",
    "    outlink_tensor = pad_list_of_tensors(outlink_list, pad_token)    \n",
    "    outlink_length_tensor = torch.stack(outlink_length)\n",
    "    \n",
    "    target_tensor = torch.stack(target_list)\n",
    "    \n",
    "    return inlink_tensor, inlink_length_tensor, outlink_tensor, outlink_length_tensor, target_tensor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tokenized_train_inlink, wiki_tokenized_train_outlink = tokenize_dataset(wiki_train, word_to_index)\n",
    "wiki_tokenized_val_inlink, wiki_tokenized_val_outlink = tokenize_dataset(wiki_valid, word_to_index)\n",
    "wiki_tokenized_test_inlink, wiki_tokenized_test_outlink = tokenize_dataset(wiki_test, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tensor_dataset = {}\n",
    "wiki_tensor_dataset['train'] = TensoredDataset(\n",
    "    wiki_tokenized_train_inlink, wiki_tokenized_train_outlink, list(wiki_train.labels)\n",
    ")\n",
    "wiki_tensor_dataset['val'] = TensoredDataset(\n",
    "    wiki_tokenized_val_inlink, wiki_tokenized_val_outlink, list(wiki_valid.labels)\n",
    ")\n",
    "wiki_tensor_dataset['test'] = TensoredDataset(\n",
    "   wiki_tokenized_test_inlink, wiki_tokenized_test_outlink, list(wiki_test.labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(\n",
    "        wiki_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, word_to_index=word_to_index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "# c - num classifiers\n",
    "# b - batch\n",
    "# l - length (padded)\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, dim_e):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param dim_e: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx\n",
    "        self.embed = nn.Embedding(vocab_size, dim_e, padding_idx=0)\n",
    "       \n",
    "    def forward(self, s1, l1, s2, l2):\n",
    "        \"\"\"\n",
    "        Take average of all words in the text.\n",
    "        \n",
    "        @param data_bl: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length_b: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out1 = self.embed(s1)\n",
    "        out2 = self.embed(s2)\n",
    "        out1 = torch.sum(out1, dim=-2) \n",
    "        out1 /= l1.float()\n",
    "        out2 = torch.sum(out2, dim=-2)\n",
    "        out2 /= l2.float()\n",
    "        out = torch.cat([out1, out2], dim=-1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "def FeedForward(in_features, mid_features, out_features=44, num_layers=1, activation=nn.ReLU(), dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Function that creates sequential model (nn.Module) with specified number of layers.\n",
    "    If 1 layer, returns linear model.\n",
    "    \"\"\"\n",
    "    if num_layers == 1:\n",
    "        return nn.Linear(in_features, out_features)\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, mid_features),\n",
    "        *([activation, nn.Dropout(dropout_rate), nn.Linear(mid_features, mid_features)] * max(0, (num_layers - 2))),\n",
    "        *[activation, nn.Linear(mid_features, out_features)]\n",
    "    )\n",
    "\n",
    "\n",
    "class LinkModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Final model that combines embeddings of words in an article (average) and puts it through layer_out.\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super(LinkModel, self).__init__()\n",
    "        \n",
    "        self.layer_bag_of_words = BagOfWords(options[\"VOCAB_SIZE\"], options[\"dim_e\"])\n",
    "        self.layer_out = FeedForward(\n",
    "            in_features=options[\"dim_e\"]*2,\n",
    "            mid_features=options[\"mid_features\"],\n",
    "            out_features=options[\"num_classes\"], \n",
    "            num_layers=options[\"num_layers\"],\n",
    "            dropout_rate=options[\"dropout_rate\"],\n",
    "            activation=options[\"activation\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, s1, l1, s2, l2):\n",
    "        # get embeddings\n",
    "        embed_be = self.layer_bag_of_words(s1, l1, s2, l2)\n",
    "        # use layer_out\n",
    "        out_bc = self.layer_out(embed_be)\n",
    "        return out_bc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"VOCAB_SIZE\": len(index_to_word),\n",
    "    \"dim_e\": 32,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_classes\": len(mlb.classes_),\n",
    "    \"mid_features\": 128,\n",
    "    \"dropout_rate\": 0.15,\n",
    "    \"activation\": nn.ReLU()\n",
    "}\n",
    "model = LinkModel(options)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import test_model\n",
    "\n",
    "best_val_f1_micro = 0\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    runnin_loss = 0.0\n",
    "    for i, (data_1, length_1, data_2, length_2, labels) in enumerate(wiki_loaders[\"train\"]):        \n",
    "        model.train()\n",
    "        inlink_batch, inlink_length_batch, outlink_batch, outlink_length_batch, label_batch = data_1.to(device),length_1.to(device),data_2.to(device),length_2.to(device), labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inlink_batch, inlink_length_batch, outlink_batch, outlink_length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        runnin_loss += loss.item()\n",
    "        #torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n",
    "        if i>0 and i % 300 == 0:\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Train_loss: {}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(wiki_loaders[\"train\"]), runnin_loss / i))\n",
    "        # validate every 300 iterations\n",
    "        if i > 0 and i % 300 == 0:\n",
    "            metrics_dict = test_model(wiki_loaders[\"val\"], model, device=device)\n",
    "            print(\"Precision macro: {}, Recall macro: {}, F1 macro: {} \".format(\n",
    "                metrics_dict[\"precision_macro\"], metrics_dict[\"recall_macro\"], metrics_dict[\"f1_macro\"]\n",
    "            ))\n",
    "            print(\"Precision micro: {}, Recall micro: {}, F1 micro: {} \".format(\n",
    "                metrics_dict[\"precision_micro\"], metrics_dict[\"recall_micro\"], metrics_dict[\"f1_micro\"]\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results just based on in-links\n",
    "# options = {\n",
    "#     \"VOCAB_SIZE\": len(index_to_word),\n",
    "#     \"dim_e\": 128,\n",
    "#     \"num_layers\": 2,\n",
    "#     \"num_classes\": len(mlb.classes_),\n",
    "#     \"mid_features\": 256,\n",
    "#     \"dropout_rate\": 0.15,\n",
    "#     \"activation\": nn.LeakyReLU(negative_slope=0.01)\n",
    "# }\n",
    "\n",
    "# Precision macro: 0.5473753851993785, Recall macro: 0.32252769088069094, F1 macro: 0.3732075444511734 \n",
    "# Precision micro: 0.5513415892672858, Recall micro: 0.5107857783089333, F1 micro: 0.5302894010360123 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
