{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import io\n",
    "import nltk\n",
    "import json\n",
    "import gzip\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if current_device == 'cuda' else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if current_device == 'cuda' else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if current_device == 'cuda' else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convCategories(categories, category_to_index):\n",
    "    return [category_to_index[category] for category in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.8, validate_percent=.1, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, word_to_index):\n",
    "    _current_dictified = []\n",
    "    for l in tqdm(dataset['tokens']):\n",
    "        encoded_l = [word_to_index[i] if i in word_to_index else word_to_index['<UNK>'] for i in l]\n",
    "        _current_dictified.append(encoded_l)\n",
    "    return _current_dictified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEN_LEN = 2500\n",
    "class TensoredDataset(Dataset):\n",
    "    def __init__(self, list_of_lists_of_tokens, labels):\n",
    "        self.input_tensors = []\n",
    "        self.target_tensors = []\n",
    "        \n",
    "        for i in range(0, len(list_of_lists_of_tokens)):\n",
    "            self.input_tensors.append(torch.tensor(list_of_lists_of_tokens[i], dtype=torch.long))\n",
    "            self.target_tensors.append(torch.tensor(labels[i], dtype=torch.long))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.input_tensors[idx][:MAX_SEN_LEN], self.target_tensors[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:\n",
    "        #print(t.reshape(1, -1).shape)\n",
    "        #print(torch.tensor([[pad_token]*(max_length - t.size(-1))])[0].shape)\n",
    "        padded_tensor = torch.cat([t.reshape(1, -1), torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    return padded_tensor\n",
    "\n",
    "def transform_labels(target_list):\n",
    "    padded_list = []\n",
    "    for t in target_list:\n",
    "        labels = t.unsqueeze(0)\n",
    "        target = torch.zeros(labels.size(0), len(category_to_index)).scatter_(1, labels, 1)\n",
    "        padded_list.append(target)\n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = word_to_index['<PAD>']\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    #target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
    "    target_tensor = transform_labels(target_list)\n",
    "    \n",
    "    return input_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx']).from_pretrained(weights_matrix)\n",
    "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], batch_first=True)\n",
    "        self.projection = nn.Linear(options['hidden_size']*options['num_layers'], options['num_labels'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        lstm_outputs, (hn, cn) = self.lstm(embeddings)\n",
    "        logits = self.projection(hn[-1])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = '/scratch/sa5154/Capstone/Models/wikitext_tokenized.p'\n",
    "wiki_df =  pkl.load(open(OUTPUT_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "temp = list(wiki_df['tokens'].str.len())\n",
    "temp = sorted(temp, reverse = True)\n",
    "plt.hist(temp[3000:])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for i in list(wiki_df['mid_level_categories']):\n",
    "    categories.extend(i)\n",
    "categories = list(set(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_index = {categories[i]:i for i in range(0, len(categories))}\n",
    "index_to_category = {v:k for k, v in category_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df['category_tokens'] = wiki_df.apply(lambda row: convCategories(row['mid_level_categories'], category_to_index), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = wiki_df[wiki_df.astype(str)['category_tokens'] != '[]']\n",
    "wiki_df = wiki_df[wiki_df.astype(str)['tokens'] != '[]']\n",
    "wiki_df = wiki_df.reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>category_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q2000864</td>\n",
       "      <td>[Culture.Philosophy and religion]</td>\n",
       "      <td>[affirming, the, consequent, sometimes, called...</td>\n",
       "      <td>[29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1064113</td>\n",
       "      <td>[History_And_Society.Business and economics]</td>\n",
       "      <td>[growth, two, six, two, zero, one, six, zero, ...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q6941060</td>\n",
       "      <td>[Geography.Europe]</td>\n",
       "      <td>[the, museum, of, work, or, arbetets, museum, ...</td>\n",
       "      <td>[25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q843920</td>\n",
       "      <td>[History_And_Society.History and society, STEM...</td>\n",
       "      <td>[like, this, one, in, dorset, england, arable,...</td>\n",
       "      <td>[11, 31, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q178999</td>\n",
       "      <td>[STEM.Biology, STEM.Medicine]</td>\n",
       "      <td>[an, axon, from, greek, axis, or, nerve, fiber...</td>\n",
       "      <td>[31, 27]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        QID                               mid_level_categories  \\\n",
       "0  Q2000864                  [Culture.Philosophy and religion]   \n",
       "1  Q1064113       [History_And_Society.Business and economics]   \n",
       "2  Q6941060                                 [Geography.Europe]   \n",
       "3   Q843920  [History_And_Society.History and society, STEM...   \n",
       "4   Q178999                      [STEM.Biology, STEM.Medicine]   \n",
       "\n",
       "                                              tokens category_tokens  \n",
       "0  [affirming, the, consequent, sometimes, called...            [29]  \n",
       "1  [growth, two, six, two, zero, one, six, zero, ...            [18]  \n",
       "2  [the, museum, of, work, or, arbetets, museum, ...            [25]  \n",
       "3  [like, this, one, in, dorset, england, arable,...     [11, 31, 1]  \n",
       "4  [an, axon, from, greek, axis, or, nerve, fiber...        [31, 27]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train, wiki_valid, wiki_test = train_validate_test_split(wiki_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>mid_level_categories</th>\n",
       "      <th>tokens</th>\n",
       "      <th>category_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17916</th>\n",
       "      <td>Q5346784</td>\n",
       "      <td>[Culture.Language and literature]</td>\n",
       "      <td>[edwin, romanzo, elmer, one, eight, five, zero...</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29737</th>\n",
       "      <td>Q4723109</td>\n",
       "      <td>[Culture.Language and literature]</td>\n",
       "      <td>[alfred, george, fysh, machin, born, one, eigh...</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17976</th>\n",
       "      <td>Q1456016</td>\n",
       "      <td>[Geography.Americas, Culture.Music]</td>\n",
       "      <td>[too, late, no, friends, is, the, first, full,...</td>\n",
       "      <td>[9, 42]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97446</th>\n",
       "      <td>Q59149462</td>\n",
       "      <td>[Geography.Americas, Culture.Sports, Culture.L...</td>\n",
       "      <td>[mat, as, alexis, romero, born, one, february,...</td>\n",
       "      <td>[9, 26, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91205</th>\n",
       "      <td>Q30602920</td>\n",
       "      <td>[Culture.Plastic arts, Geography.Americas, Cul...</td>\n",
       "      <td>[the, confederate, memorial, fountain, was, hi...</td>\n",
       "      <td>[2, 9, 10, 32]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             QID                               mid_level_categories  \\\n",
       "17916   Q5346784                  [Culture.Language and literature]   \n",
       "29737   Q4723109                  [Culture.Language and literature]   \n",
       "17976   Q1456016                [Geography.Americas, Culture.Music]   \n",
       "97446  Q59149462  [Geography.Americas, Culture.Sports, Culture.L...   \n",
       "91205  Q30602920  [Culture.Plastic arts, Geography.Americas, Cul...   \n",
       "\n",
       "                                                  tokens category_tokens  \n",
       "17916  [edwin, romanzo, elmer, one, eight, five, zero...             [6]  \n",
       "29737  [alfred, george, fysh, machin, born, one, eigh...             [6]  \n",
       "17976  [too, late, no, friends, is, the, first, full,...         [9, 42]  \n",
       "97446  [mat, as, alexis, romero, born, one, february,...      [9, 26, 6]  \n",
       "91205  [the, confederate, memorial, fountain, was, hi...  [2, 9, 10, 32]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(wiki_train['category_tokens'])\n",
    "y_val = list(wiki_valid['category_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([y for x in list(wiki_train['tokens']) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595516"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jernigan',\n",
       " 'baseley',\n",
       " 'digor',\n",
       " 'engyum',\n",
       " 'cremated',\n",
       " 'kiesenwetter',\n",
       " 'futurs',\n",
       " 'hrg',\n",
       " 'yoshikane',\n",
       " 'solm']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "for word in vocab:\n",
    "    if(word not in word_to_index):\n",
    "        word_to_index[word]=len(word_to_index)\n",
    "index_to_word = {v:k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313866"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79968/79968 [00:11<00:00, 6751.62it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_tokenized_train = tokenize_dataset(wiki_train, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9996/9996 [00:01<00:00, 6214.32it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_tokenized_val = tokenize_dataset(wiki_valid, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tokenized_datasets = {}\n",
    "wiki_tokenized_datasets['train'] = wiki_tokenized_train\n",
    "wiki_tokenized_datasets['val'] = wiki_tokenized_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tensor_dataset = {}\n",
    "wiki_tensor_dataset['train'] = TensoredDataset(wiki_tokenized_datasets['train'], y_train)\n",
    "wiki_tensor_dataset['val'] = TensoredDataset(wiki_tokenized_datasets['val'], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_loaders = {}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([308304, 124513, 574170, 124513, 574170, 329431, 340330, 508141, 387060,\n",
       "         350387, 508141, 304212, 560307, 393424, 212730,  16159, 504072, 340330,\n",
       "         508141, 387060, 484092, 340330, 592563,  54685, 137456, 366071, 356016,\n",
       "         491039, 398472, 218360,  78935, 332545, 332545, 163618, 451357,  72203,\n",
       "          13879, 522573,  28430,  13879, 522573, 135067, 440158, 504072, 582354,\n",
       "          78935, 289860,  92848, 440158, 504072, 359854, 472292, 222932, 100658,\n",
       "          10256, 340330, 508141, 387060, 428151, 283942, 308986, 338559, 592563,\n",
       "         571636, 352148, 117077, 544272, 375595, 352148,  35905,  78935, 332545,\n",
       "         309683, 410350, 100548,  78935, 332545, 332545, 430674, 314298, 476864,\n",
       "          78935, 332545, 332545, 309683, 311456, 313866, 413527, 504072,  85371,\n",
       "         484784, 276742, 570801, 160988, 561733, 103355, 561733, 103355, 571636,\n",
       "         248296,  83177, 355755, 313866, 124513, 387060, 560430, 162017,  92848,\n",
       "         163618, 163618, 163618,  13879, 522573, 254191, 255765,  56512, 383373,\n",
       "         571636, 383373, 459116, 313866, 574170, 479234, 500724, 256851, 504072,\n",
       "         470989, 561733, 103355, 303516, 313866, 340330, 508141, 387060, 314298,\n",
       "         256851, 504072, 383373, 494382, 314298, 397066, 401296, 117077, 363814,\n",
       "         203487,  67395, 560430, 185796,  78935, 332545,  92848, 163618,  78935,\n",
       "         430674, 473041, 332937,  75685, 340330, 313866, 290920, 569143, 473041,\n",
       "         332937,  75685, 332937,  75685, 428151, 313892, 560430, 313866,  64299,\n",
       "         472292, 504072, 313866, 574170, 222360, 100658,  31656, 586551, 313866,\n",
       "         574170,  28430,  13879, 522573, 406497, 314298, 269423,  92848, 163618,\n",
       "          78935,  18190, 105922, 203004,  45256, 271038, 313866, 340330, 508141,\n",
       "         387060, 497311, 313866, 341291, 222360, 100658,  31656, 574170, 117077,\n",
       "         313866, 433863, 240629, 224151, 230163, 282718, 253677, 313866,  92848,\n",
       "          70040,  45256, 282718, 207596, 561223, 105922, 534417,  45256, 508589,\n",
       "         313866, 443199, 414189, 117077, 498984, 467950, 240629, 314298, 231159,\n",
       "          92848, 163618,  78935, 410350, 203004, 428151, 546529, 313866, 593402,\n",
       "          45256, 545994,  92848, 163618, 163618, 239072, 117077, 282718, 360442,\n",
       "         308986,  92848, 163618,  92848, 163618, 223788, 314298, 316206, 302219,\n",
       "         313866, 433863, 240629, 117077, 207596,  70040,  45256, 520400, 313866,\n",
       "         340330, 508141, 387060, 329431, 472559, 313866,  46261, 224151, 100658,\n",
       "          31656, 117077, 329431, 149896,  28839, 203004,  45256, 166887, 313866,\n",
       "         523459, 508141, 329431,  74692, 100548, 565647, 467950, 428151, 340330,\n",
       "         349522, 508141, 436185, 575315, 523887, 508141, 387060, 175082, 504072,\n",
       "         124513,  70040, 314298, 340330, 221636, 547210, 494106, 169692, 480482,\n",
       "         171850, 340330, 508141, 124513,  70040, 330881, 314298,  78935, 332545,\n",
       "         332545, 163618, 124513,  70040, 314298, 340330,  13879, 522573]),\n",
       " tensor([9, 2]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tensor_dataset[\"train\"].__getitem__(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for i, (inp, target) in enumerate(wiki_loaders['train']):\n",
    "    print(target[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validate(model):\n",
    "    valid_loss_cache = 0\n",
    "    all_targets = []\n",
    "    all_logits = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inp, target) in enumerate(wiki_loaders['val']):\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "            loss = criterion(logits, target)\n",
    "            m = nn.Sigmoid()\n",
    "            logits = m(logits)\n",
    "            logits = logits.cpu().detach().numpy()\n",
    "            target = target.cpu().numpy()\n",
    "            all_targets.append(target)\n",
    "            all_logits.append(logits)\n",
    "            valid_loss_cache += loss.item()\n",
    "\n",
    "        avg_val_loss = valid_loss_cache / (i+1)\n",
    "        all_logits = np.concatenate(all_logits, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "        all_logits[all_logits > 0.5] = 1\n",
    "        all_logits[all_logits <= 0.5] = 0\n",
    "        prec_macro, rec_macro, f_score_macro, _ = precision_recall_fscore_support(all_targets, all_logits, average = 'macro')\n",
    "        print('Validation macro prec: {}, rec:{}, f_score:{}'.format(prec_macro, rec_macro, f_score_macro))\n",
    "        prec_micro, rec_micro, f_score_micro, _ = precision_recall_fscore_support(all_targets, all_logits, average = 'micro')\n",
    "        print('Validation micro prec: {}, rec:{}, f_score:{}'.format(prec_micro, rec_micro, f_score_micro))\n",
    "        print('Validation loss = {:.{prec}f}'.format(avg_val_loss, prec=4))\n",
    "        return f_score_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2519371it [03:11, 13148.12it/s]\n"
     ]
    }
   ],
   "source": [
    "#Loading pre trained fastText word embeddings\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    data = {}\n",
    "    for line in tqdm(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data\n",
    "\n",
    "fasttext_emb = load_vectors(\"wiki.en.align.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the weight matrix for pretrained word embeddings\n",
    "vocab_size = len(index_to_word)\n",
    "embed_dim = len(fasttext_emb[\"apple\"])\n",
    "weights_matrix = np.zeros((vocab_size,embed_dim))\n",
    "\n",
    "words_found = 0\n",
    "for i, word in enumerate(word_to_index):\n",
    "    try: \n",
    "        weights_matrix[i] = fasttext_emb[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(embed_dim))\n",
    "weights_matrix = torch.FloatTensor(weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab: 595516\n",
      "No. of words from vocab found in fastText: 470498\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words in vocab: {}\".format(len(vocab)))\n",
    "print(\"No. of words from vocab found in fastText: {}\".format(words_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'num_embeddings': len(word_to_index),\n",
    "    'embedding_dim': weights_matrix.size(1),\n",
    "    'num_labels':44,\n",
    "    'padding_idx': word_to_index['<PAD>'],\n",
    "    'input_size': weights_matrix.size(1),\n",
    "    'hidden_size': 64,\n",
    "    'num_layers': 1,\n",
    "}\n",
    "\n",
    "model = LSTModel(options).to(current_device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(model_parameters, lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch:1\n",
      "Step 0 avg train loss = 0.7002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa5154/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro prec: 0.027626928699400463, rec:0.3706603290096035, f_score:0.026910420322028925\n",
      "Validation micro prec: 0.029248965418206027, rec:0.2618477181090399, f_score:0.05262014502539412\n",
      "Validation loss = 0.6723\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.1471\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa5154/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 avg train loss = 0.1352\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1242\n",
      "Step 1500 avg train loss = 0.1313\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1236\n",
      "Step 2000 avg train loss = 0.1294\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1236\n",
      "Running Epoch:2\n",
      "Step 0 avg train loss = 0.1153\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1238\n",
      "Step 1000 avg train loss = 0.1234\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1237\n",
      "Step 1500 avg train loss = 0.1236\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1236\n",
      "Step 2000 avg train loss = 0.1237\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1236\n",
      "Running Epoch:3\n",
      "Step 0 avg train loss = 0.1196\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1237\n",
      "Step 500 avg train loss = 0.1239\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1238\n",
      "Step 1000 avg train loss = 0.1236\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1236\n",
      "Step 1500 avg train loss = 0.1234\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1237\n",
      "Step 2000 avg train loss = 0.1235\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1237\n",
      "Running Epoch:4\n",
      "Step 0 avg train loss = 0.1364\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1236\n",
      "Step 500 avg train loss = 0.1240\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1237\n",
      "Step 1000 avg train loss = 0.1238\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1240\n",
      "Step 1500 avg train loss = 0.1237\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1237\n",
      "Step 2000 avg train loss = 0.1237\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1236\n",
      "Running Epoch:5\n",
      "Step 0 avg train loss = 0.1143\n",
      "Validation macro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation micro prec: 0.0, rec:0.0, f_score:0.0\n",
      "Validation loss = 0.1235\n",
      "Step 500 avg train loss = 0.1225\n",
      "Validation macro prec: 0.017469470827679783, rec:0.009670514008162039, f_score:0.012449436753638137\n",
      "Validation micro prec: 0.7686567164179104, rec:0.0902822415707357, f_score:0.16158552528368977\n",
      "Validation loss = 0.1210\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.1195\n",
      "Validation macro prec: 0.0177192137818433, rec:0.021193760797175835, f_score:0.01930136580249447\n",
      "Validation micro prec: 0.7796454064011052, rec:0.1978612750540525, f_score:0.3156226696495153\n",
      "Validation loss = 0.1140\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.1174\n",
      "Validation macro prec: 0.01884754970721172, rec:0.021193760797175835, f_score:0.019951917411082567\n",
      "Validation micro prec: 0.8292921871173157, rec:0.1978612750540525, f_score:0.31949424419701833\n",
      "Validation loss = 0.1114\n",
      "Model saved!\n",
      "Step 2000 avg train loss = 0.1156\n",
      "Validation macro prec: 0.020639513588748076, rec:0.021162464635337122, f_score:0.02089771800133508\n",
      "Validation micro prec: 0.9081385979049154, rec:0.1975690995149886, f_score:0.32453445958917254\n",
      "Validation loss = 0.1093\n",
      "Model saved!\n",
      "Running Epoch:6\n",
      "Step 0 avg train loss = 0.1087\n",
      "Validation macro prec: 0.020776433866562624, rec:0.021331463909266168, f_score:0.02105029092391506\n",
      "Validation micro prec: 0.9141630901287554, rec:0.1991468474259335, f_score:0.32704764646610046\n",
      "Validation loss = 0.1080\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.1074\n",
      "Validation macro prec: 0.02085676561201037, rec:0.02135650083873714, f_score:0.0211036752062742\n",
      "Validation micro prec: 0.9176976869284562, rec:0.1993805878571846, f_score:0.32758868993327256\n",
      "Validation loss = 0.1080\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.1068\n",
      "Validation macro prec: 0.022188894016305184, rec:0.018057885380936882, f_score:0.019911382270932833\n",
      "Validation micro prec: 0.9763113367174281, rec:0.16858528603985273, f_score:0.28752242375921866\n",
      "Validation loss = 0.1093\n",
      "Step 1500 avg train loss = 0.1067\n",
      "Validation macro prec: 0.021845239674244635, rec:0.02061791141934353, f_score:0.021213838584198463\n",
      "Validation micro prec: 0.961190545666764, rec:0.19248524513527726, f_score:0.32074001947419667\n",
      "Validation loss = 0.1055\n",
      "Step 2000 avg train loss = 0.1063\n",
      "Validation macro prec: 0.021706548142022844, rec:0.02103102075561453, f_score:0.021363445618586194\n",
      "Validation micro prec: 0.9550881182490051, rec:0.19634196225092035, f_score:0.32572342591246184\n",
      "Validation loss = 0.1044\n",
      "Running Epoch:7\n",
      "Step 0 avg train loss = 0.0892\n",
      "Validation macro prec: 0.04011440469260735, rec:0.03886665061835208, f_score:0.03937025851361298\n",
      "Validation micro prec: 0.8916721599648428, rec:0.23712966750423656, f_score:0.3746307237813885\n",
      "Validation loss = 0.1009\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.0974\n",
      "Validation macro prec: 0.062346575370254786, rec:0.04048860770495789, f_score:0.04043607868505832\n",
      "Validation micro prec: 0.9078299776286354, rec:0.23712966750423656, f_score:0.37603669554742164\n",
      "Validation loss = 0.0967\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0978\n",
      "Validation macro prec: 0.07941112888597511, rec:0.04563977324593576, f_score:0.049657895807420104\n",
      "Validation micro prec: 0.9145622895622896, rec:0.25395897855431543, f_score:0.39753029956551567\n",
      "Validation loss = 0.0961\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.0966\n",
      "Validation macro prec: 0.10411196725517037, rec:0.04965311135897857, f_score:0.05384768026333577\n",
      "Validation micro prec: 0.9033844942935852, rec:0.26827557996844503, f_score:0.41369677855372833\n",
      "Validation loss = 0.0941\n",
      "Model saved!\n",
      "Step 2000 avg train loss = 0.0956\n",
      "Validation macro prec: 0.11082046757650406, rec:0.05815645745224498, f_score:0.06553809580753456\n",
      "Validation micro prec: 0.8305943787436646, rec:0.3160170630514813, f_score:0.45783948526921775\n",
      "Validation loss = 0.0906\n",
      "Model saved!\n",
      "Running Epoch:8\n",
      "Step 0 avg train loss = 0.0900\n",
      "Validation macro prec: 0.11107256820036576, rec:0.06225725750551608, f_score:0.0695224148921999\n",
      "Validation micro prec: 0.8676561533704391, rec:0.32793782504528723, f_score:0.47597642169543275\n",
      "Validation loss = 0.0869\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.0845\n",
      "Validation macro prec: 0.10339740729153707, rec:0.07274507316689909, f_score:0.08060716326491599\n",
      "Validation micro prec: 0.8291155098972689, rec:0.38672354350493776, f_score:0.5274357441721459\n",
      "Validation loss = 0.0844\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0832\n",
      "Validation macro prec: 0.10160030403946033, rec:0.08302543774175221, f_score:0.08545973423666965\n",
      "Validation micro prec: 0.8349448299397111, rec:0.4289136913457605, f_score:0.5667078443483632\n",
      "Validation loss = 0.0814\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.0820\n",
      "Validation macro prec: 0.1485621874071883, rec:0.08116409960485034, f_score:0.0850962429977768\n",
      "Validation micro prec: 0.856305701379563, rec:0.40986384619879623, f_score:0.5543787543471388\n",
      "Validation loss = 0.0802\n",
      "Step 2000 avg train loss = 0.0812\n",
      "Validation macro prec: 0.1313148503476371, rec:0.09862285332036493, f_score:0.10660347591995759\n",
      "Validation micro prec: 0.8463462374945628, rec:0.45480044410681936, f_score:0.5916606484472994\n",
      "Validation loss = 0.0778\n",
      "Model saved!\n",
      "Running Epoch:9\n",
      "Step 0 avg train loss = 0.0812\n",
      "Validation macro prec: 0.1659687839759643, rec:0.10393030368634663, f_score:0.11066401798779785\n",
      "Validation micro prec: 0.8100299545849841, rec:0.4898615087944837, f_score:0.6105163498652683\n",
      "Validation loss = 0.0766\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.0756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro prec: 0.17144045975396827, rec:0.10567596841662362, f_score:0.11496184389732246\n",
      "Validation micro prec: 0.8153742543775255, rec:0.49523753871325893, f_score:0.6162067837277784\n",
      "Validation loss = 0.0750\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0740\n",
      "Validation macro prec: 0.1953721343214102, rec:0.13103932852512268, f_score:0.14841895783451964\n",
      "Validation micro prec: 0.87011997714721, rec:0.533980015193128, f_score:0.6618142313959804\n",
      "Validation loss = 0.0713\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.0725\n",
      "Validation macro prec: 0.19294479365622363, rec:0.13987970516577866, f_score:0.15152354587978092\n",
      "Validation micro prec: 0.8541413439833304, rec:0.5748845906620698, f_score:0.6872271314309664\n",
      "Validation loss = 0.0676\n",
      "Model saved!\n",
      "Step 2000 avg train loss = 0.0706\n",
      "Validation macro prec: 0.2172415249019404, rec:0.14925543004052355, f_score:0.16458656326998702\n",
      "Validation micro prec: 0.862605868765506, rec:0.5892011920761994, f_score:0.7001597111311714\n",
      "Validation loss = 0.0654\n",
      "Model saved!\n",
      "Running Epoch:10\n",
      "Step 0 avg train loss = 0.0673\n",
      "Validation macro prec: 0.20761717394836943, rec:0.14821862082504927, f_score:0.1588852581079907\n",
      "Validation micro prec: 0.8622554067971163, rec:0.5870975281949395, f_score:0.6985572744654962\n",
      "Validation loss = 0.0642\n",
      "Step 500 avg train loss = 0.0617\n",
      "Validation macro prec: 0.23341347231086165, rec:0.1651073490014893, f_score:0.18066531786479997\n",
      "Validation micro prec: 0.8586813548174942, rec:0.6103547011044236, f_score:0.7135293916726441\n",
      "Validation loss = 0.0622\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0616\n",
      "Validation macro prec: 0.24932828462154857, rec:0.1692524449307127, f_score:0.19020653939621646\n",
      "Validation micro prec: 0.8746969172151022, rec:0.5902530240168293, f_score:0.7048602630752591\n",
      "Validation loss = 0.0620\n",
      "Step 1500 avg train loss = 0.0609\n",
      "Validation macro prec: 0.24973420447595912, rec:0.18598934119199945, f_score:0.2051286575332537\n",
      "Validation micro prec: 0.8629377277768975, rec:0.6364751942967335, f_score:0.7326046746258619\n",
      "Validation loss = 0.0597\n",
      "Model saved!\n",
      "Step 2000 avg train loss = 0.0604\n",
      "Validation macro prec: 0.26587551364585166, rec:0.1948628693882857, f_score:0.2090103806694673\n",
      "Validation micro prec: 0.8551596483109671, rec:0.6479284754280371, f_score:0.7372585524784733\n",
      "Validation loss = 0.0586\n",
      "Model saved!\n",
      "Running Epoch:11\n",
      "Step 0 avg train loss = 0.0539\n",
      "Validation macro prec: 0.2523031503175058, rec:0.18699669132844354, f_score:0.20156050804070758\n",
      "Validation micro prec: 0.8597451332968493, rec:0.6426108806170747, f_score:0.7354868913857677\n",
      "Validation loss = 0.0580\n",
      "Step 500 avg train loss = 0.0569\n",
      "Validation macro prec: 0.2928659201376433, rec:0.2012227950755344, f_score:0.22215654457309164\n",
      "Validation micro prec: 0.8717540226222718, rec:0.6395138199029977, f_score:0.737789463039741\n",
      "Validation loss = 0.0575\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0559\n",
      "Validation macro prec: 0.294852497952414, rec:0.2014175234071419, f_score:0.21918485241268978\n",
      "Validation micro prec: 0.8682207048112393, rec:0.646409162624905, f_score:0.7410732230186909\n",
      "Validation loss = 0.0569\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.0557\n",
      "Validation macro prec: 0.29519242871527673, rec:0.20481896092415008, f_score:0.22349169759327117\n",
      "Validation micro prec: 0.8502476162317983, rec:0.6721790451703383, f_score:0.7507995561647411\n",
      "Validation loss = 0.0558\n",
      "Model saved!\n",
      "Step 2000 avg train loss = 0.0555\n",
      "Validation macro prec: 0.3237536185809144, rec:0.21086381577062926, f_score:0.22681288053732643\n",
      "Validation micro prec: 0.8518847006651885, rec:0.6735230526500321, f_score:0.7522762131645073\n",
      "Validation loss = 0.0556\n",
      "Model saved!\n",
      "Running Epoch:12\n",
      "Step 0 avg train loss = 0.0574\n",
      "Validation macro prec: 0.2925108791591691, rec:0.21051775495454425, f_score:0.2281778355837167\n",
      "Validation micro prec: 0.8479527903249308, rec:0.6801262198328756, f_score:0.7548234378546645\n",
      "Validation loss = 0.0552\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.0528\n",
      "Validation macro prec: 0.3145657178108255, rec:0.21703247127330128, f_score:0.2361638095864176\n",
      "Validation micro prec: 0.8662482988053833, rec:0.6694910302109507, f_score:0.7552654998516759\n",
      "Validation loss = 0.0544\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0529\n",
      "Validation macro prec: 0.334881046375239, rec:0.21925056333681037, f_score:0.2390198603765576\n",
      "Validation micro prec: 0.8616211374832065, rec:0.6745748845906621, f_score:0.7567106945036216\n",
      "Validation loss = 0.0544\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.0528\n",
      "Validation macro prec: 0.31102942121572624, rec:0.21545444815871162, f_score:0.2383567794865207\n",
      "Validation micro prec: 0.8754948381588139, rec:0.659089581020277, f_score:0.7520336044805974\n",
      "Validation loss = 0.0540\n",
      "Step 2000 avg train loss = 0.0527\n",
      "Validation macro prec: 0.32813365608121353, rec:0.22440128796164238, f_score:0.24268469075173507\n",
      "Validation micro prec: 0.8575416942179119, rec:0.6820545783906972, f_score:0.7597969014451245\n",
      "Validation loss = 0.0539\n",
      "Model saved!\n",
      "Running Epoch:13\n",
      "Step 0 avg train loss = 0.0537\n",
      "Validation macro prec: 0.3918550530510055, rec:0.2428750023403226, f_score:0.265719221121592\n",
      "Validation micro prec: 0.839938167509837, rec:0.6985332787938994, f_score:0.7627372786728345\n",
      "Validation loss = 0.0542\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.0506\n",
      "Validation macro prec: 0.39149667789248677, rec:0.24795358362042255, f_score:0.2755804024667442\n",
      "Validation micro prec: 0.8620339474829537, rec:0.6944428212470052, f_score:0.7692158322275803\n",
      "Validation loss = 0.0526\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0504\n",
      "Validation macro prec: 0.4047858460564109, rec:0.24974039276457666, f_score:0.2806876872076199\n",
      "Validation micro prec: 0.8692956904921951, rec:0.6801262198328756, f_score:0.763163071274015\n",
      "Validation loss = 0.0524\n",
      "Step 1500 avg train loss = 0.0504\n",
      "Validation macro prec: 0.38107602565117094, rec:0.2502595521003273, f_score:0.2784530811442163\n",
      "Validation micro prec: 0.8623193721117876, rec:0.6869631274469702, f_score:0.7647173616080141\n",
      "Validation loss = 0.0527\n",
      "Step 2000 avg train loss = 0.0505\n",
      "Validation macro prec: 0.3778402530846842, rec:0.23476070585809602, f_score:0.25986646922886064\n",
      "Validation micro prec: 0.86133648030683, rec:0.6824051890375737, f_score:0.7615010922369665\n",
      "Validation loss = 0.0526\n",
      "Running Epoch:14\n",
      "Step 0 avg train loss = 0.0373\n",
      "Validation macro prec: 0.3743998934410664, rec:0.23460084807821377, f_score:0.25884577012365484\n",
      "Validation micro prec: 0.8630815888749168, rec:0.681820837959446, f_score:0.7618177069730999\n",
      "Validation loss = 0.0525\n",
      "Step 500 avg train loss = 0.0487\n",
      "Validation macro prec: 0.4038824676979624, rec:0.2688906999597658, f_score:0.29745739175911856\n",
      "Validation micro prec: 0.8498193942761878, rec:0.7148951089814761, f_score:0.776540036180139\n",
      "Validation loss = 0.0519\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0491\n",
      "Validation macro prec: 0.4186145773538639, rec:0.2769520798373986, f_score:0.31028171231080887\n",
      "Validation micro prec: 0.8620058565153733, rec:0.6880733944954128, f_score:0.7652812530465017\n",
      "Validation loss = 0.0519\n",
      "Step 1500 avg train loss = 0.0487\n",
      "Validation macro prec: 0.42362123545434954, rec:0.261494188490323, f_score:0.2979986709125236\n",
      "Validation micro prec: 0.869429821309409, rec:0.685210074212587, f_score:0.7664052287581699\n",
      "Validation loss = 0.0517\n",
      "Step 2000 avg train loss = 0.0487\n",
      "Validation macro prec: 0.4618915634171978, rec:0.264362888013531, f_score:0.2988116370473603\n",
      "Validation micro prec: 0.8636461395082085, rec:0.7039677438204873, f_score:0.7756744575365397\n",
      "Validation loss = 0.0510\n",
      "Running Epoch:15\n",
      "Step 0 avg train loss = 0.0368\n",
      "Validation macro prec: 0.4554393550323523, rec:0.2703747606925676, f_score:0.30733421889324397\n",
      "Validation micro prec: 0.8598568086765436, rec:0.7088178577689476, f_score:0.7770659833440102\n",
      "Validation loss = 0.0509\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.0478\n",
      "Validation macro prec: 0.4391693274779919, rec:0.2944851829673299, f_score:0.3313796456809859\n",
      "Validation micro prec: 0.8526015581524764, rec:0.7162391164611699, f_score:0.7784940772968338\n",
      "Validation loss = 0.0507\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0474\n",
      "Validation macro prec: 0.4575470936762925, rec:0.2877501727535606, f_score:0.3235849197409427\n",
      "Validation micro prec: 0.8529349736379613, rec:0.7089931630923859, f_score:0.7743314825451528\n",
      "Validation loss = 0.0510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500 avg train loss = 0.0475\n",
      "Validation macro prec: 0.4426069036545713, rec:0.29425522013146815, f_score:0.3232495387741116\n",
      "Validation micro prec: 0.8444814891991387, rec:0.7104540407877052, f_score:0.7716915264995239\n",
      "Validation loss = 0.0516\n",
      "Step 2000 avg train loss = 0.0473\n",
      "Validation macro prec: 0.44115713046496996, rec:0.2952634172307953, f_score:0.33653262355808805\n",
      "Validation micro prec: 0.8601255554771814, rec:0.7125577046689651, f_score:0.7794183445190156\n",
      "Validation loss = 0.0495\n",
      "Model saved!\n",
      "Running Epoch:16\n",
      "Step 0 avg train loss = 0.0435\n",
      "Validation macro prec: 0.44899657091081635, rec:0.3044363922389818, f_score:0.3427105482311174\n",
      "Validation micro prec: 0.8599986069513129, rec:0.7214982761643195, f_score:0.784683825865904\n",
      "Validation loss = 0.0491\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.0457\n",
      "Validation macro prec: 0.4506434071014079, rec:0.2982720277512669, f_score:0.34128001527843127\n",
      "Validation micro prec: 0.8675870949281065, rec:0.708700987553322, f_score:0.7801363694841117\n",
      "Validation loss = 0.0494\n",
      "Step 1000 avg train loss = 0.0458\n",
      "Validation macro prec: 0.45620415949395104, rec:0.3003005089596301, f_score:0.33176887528075355\n",
      "Validation micro prec: 0.841545052885259, rec:0.7345877403143809, f_score:0.7844373030482668\n",
      "Validation loss = 0.0498\n",
      "Step 1500 avg train loss = 0.0458\n",
      "Validation macro prec: 0.4723191934987352, rec:0.3039980149377538, f_score:0.3471147174030614\n",
      "Validation micro prec: 0.862090813093981, rec:0.7155963302752294, f_score:0.7820422760074078\n",
      "Validation loss = 0.0492\n",
      "Step 2000 avg train loss = 0.0458\n",
      "Validation macro prec: 0.4522982608000776, rec:0.3197075490456942, f_score:0.3550798621447143\n",
      "Validation micro prec: 0.8646743727093318, rec:0.7168819026471104, f_score:0.7838727197214147\n",
      "Validation loss = 0.0485\n",
      "Running Epoch:17\n",
      "Step 0 avg train loss = 0.0380\n",
      "Validation macro prec: 0.45150719382283777, rec:0.312004158496613, f_score:0.3534455689728034\n",
      "Validation micro prec: 0.858083749306711, rec:0.7232513293987027, f_score:0.7849193011383454\n",
      "Validation loss = 0.0484\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.0446\n",
      "Validation macro prec: 0.4457635203937436, rec:0.31718459091122286, f_score:0.3589666180109894\n",
      "Validation micro prec: 0.8636778863045464, rec:0.7182259101268041, f_score:0.7842649310872893\n",
      "Validation loss = 0.0484\n",
      "Step 1000 avg train loss = 0.0441\n",
      "Validation macro prec: 0.457996691963622, rec:0.3115697693458421, f_score:0.3544528280151622\n",
      "Validation micro prec: 0.866280302494876, rec:0.7162391164611699, f_score:0.7841468875951635\n",
      "Validation loss = 0.0485\n",
      "Step 1500 avg train loss = 0.0443\n",
      "Validation macro prec: 0.46458283039224857, rec:0.33217178802310654, f_score:0.36992142936320765\n",
      "Validation micro prec: 0.8654645347608649, rec:0.7179921696955531, f_score:0.7848610667518365\n",
      "Validation loss = 0.0481\n",
      "Step 2000 avg train loss = 0.0443\n",
      "Validation macro prec: 0.46614392245217834, rec:0.3232209774470889, f_score:0.36268046717658414\n",
      "Validation micro prec: 0.8529005524861878, rec:0.7216735814877578, f_score:0.7818187573196593\n",
      "Validation loss = 0.0489\n",
      "Running Epoch:18\n",
      "Step 0 avg train loss = 0.0449\n",
      "Validation macro prec: 0.47385453508527237, rec:0.3148839697815452, f_score:0.3544383834223876\n",
      "Validation micro prec: 0.8565220390425605, rec:0.7255887337112137, f_score:0.7856374565011073\n",
      "Validation loss = 0.0476\n",
      "Model saved!\n",
      "Step 500 avg train loss = 0.0433\n",
      "Validation macro prec: 0.4692485270543944, rec:0.3259409251953688, f_score:0.36377995527201396\n",
      "Validation micro prec: 0.8624645794457115, rec:0.7292117103956057, f_score:0.7902602748400989\n",
      "Validation loss = 0.0471\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0429\n",
      "Validation macro prec: 0.47225756227865934, rec:0.34207839758441366, f_score:0.3795806300035123\n",
      "Validation micro prec: 0.8625102655351765, rec:0.7364576637643896, f_score:0.79451536643026\n",
      "Validation loss = 0.0461\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.0426\n",
      "Validation macro prec: 0.4746471255577628, rec:0.3395067091205569, f_score:0.3737922304396139\n",
      "Validation micro prec: 0.8526683694044899, rec:0.7413077777128498, f_score:0.7930980588290457\n",
      "Validation loss = 0.0469\n",
      "Step 2000 avg train loss = 0.0426\n",
      "Validation macro prec: 0.4814060995600054, rec:0.3295078277903675, f_score:0.37145610216097413\n",
      "Validation micro prec: 0.8509007797795106, rec:0.7396715946940922, f_score:0.7913970427334396\n",
      "Validation loss = 0.0470\n",
      "Running Epoch:19\n",
      "Step 0 avg train loss = 0.0440\n",
      "Validation macro prec: 0.49083005613699204, rec:0.3474842103212438, f_score:0.38675958904399566\n",
      "Validation micro prec: 0.8612689069878858, rec:0.735347396715947, f_score:0.7933425797503468\n",
      "Validation loss = 0.0456\n",
      "Step 500 avg train loss = 0.0413\n",
      "Validation macro prec: 0.5053605451440631, rec:0.3459723726974546, f_score:0.39043878398630144\n",
      "Validation micro prec: 0.8624195891263747, rec:0.7285689242096651, f_score:0.7898637947418435\n",
      "Validation loss = 0.0457\n",
      "Step 1000 avg train loss = 0.0415\n",
      "Validation macro prec: 0.498443122461563, rec:0.36119887136888756, f_score:0.39830925445731663\n",
      "Validation micro prec: 0.8567792068595927, rec:0.7473850289253784, f_score:0.7983521113573235\n",
      "Validation loss = 0.0452\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.0415\n",
      "Validation macro prec: 0.5225685588627814, rec:0.3492666619065727, f_score:0.39165143159014537\n",
      "Validation micro prec: 0.8616147212602703, rec:0.7414830830362882, f_score:0.7970477386934673\n",
      "Validation loss = 0.0456\n",
      "Step 2000 avg train loss = 0.0414\n",
      "Validation macro prec: 0.5272867141981987, rec:0.354286954181605, f_score:0.3986678346840653\n",
      "Validation micro prec: 0.8589682593624686, rec:0.7385028925378367, f_score:0.7941934267579966\n",
      "Validation loss = 0.0457\n",
      "Running Epoch:20\n",
      "Step 0 avg train loss = 0.0423\n",
      "Validation macro prec: 0.5426284568178855, rec:0.36209602440746186, f_score:0.4028971392856169\n",
      "Validation micro prec: 0.8530370567493846, rec:0.7492549523753871, f_score:0.7977849676455948\n",
      "Validation loss = 0.0450\n",
      "Step 500 avg train loss = 0.0395\n",
      "Validation macro prec: 0.5130917387666756, rec:0.374489149852365, f_score:0.4116024132119875\n",
      "Validation micro prec: 0.8493762311227839, rec:0.7559165546660433, f_score:0.7999257953807625\n",
      "Validation loss = 0.0452\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0395\n",
      "Validation macro prec: 0.5495481333137923, rec:0.37869946255898496, f_score:0.4241820429189431\n",
      "Validation micro prec: 0.8522296325356349, rec:0.7616431952316952, f_score:0.8043941123831271\n",
      "Validation loss = 0.0441\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.0405\n",
      "Validation macro prec: 0.5441704100651328, rec:0.3596055071953688, f_score:0.3968734055742004\n",
      "Validation micro prec: 0.8435691741188848, rec:0.7496639981300766, f_score:0.7938492002103895\n",
      "Validation loss = 0.0459\n",
      "Step 2000 avg train loss = 0.0406\n",
      "Validation macro prec: 0.5511469406254885, rec:0.37368585341005944, f_score:0.42124415360584033\n",
      "Validation micro prec: 0.8600311041990669, rec:0.7432361362706714, f_score:0.7973794746410884\n",
      "Validation loss = 0.0446\n",
      "Running Epoch:21\n",
      "Step 0 avg train loss = 0.0294\n",
      "Validation macro prec: 0.5410677738802122, rec:0.37458649676901934, f_score:0.4181457519039615\n",
      "Validation micro prec: 0.8580705630313985, rec:0.7489627768363233, f_score:0.7998127925117005\n",
      "Validation loss = 0.0441\n",
      "Step 500 avg train loss = 0.0388\n",
      "Validation macro prec: 0.5377826751982805, rec:0.3963783102544092, f_score:0.4395164989221865\n",
      "Validation micro prec: 0.8496108949416342, rec:0.7655583474551511, f_score:0.8053975962868473\n",
      "Validation loss = 0.0434\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0385\n",
      "Validation macro prec: 0.5570449609084346, rec:0.392239035421278, f_score:0.4384178481672087\n",
      "Validation micro prec: 0.8434631923496566, rec:0.7679541868754748, f_score:0.8039395607756774\n",
      "Validation loss = 0.0436\n",
      "Step 1500 avg train loss = 0.0385\n",
      "Validation macro prec: 0.5358361295988334, rec:0.3933741075216529, f_score:0.43828377547363107\n",
      "Validation micro prec: 0.8567565785134867, rec:0.7591304855957459, f_score:0.804994423100756\n",
      "Validation loss = 0.0428\n",
      "Step 2000 avg train loss = 0.0388\n",
      "Validation macro prec: 0.5401203052159765, rec:0.3919929249182545, f_score:0.4324866192799923\n",
      "Validation micro prec: 0.8578807947019867, rec:0.7569683866066733, f_score:0.8042715673796295\n",
      "Validation loss = 0.0431\n",
      "Running Epoch:22\n",
      "Step 0 avg train loss = 0.0373\n",
      "Validation macro prec: 0.5619935472527506, rec:0.40521434224816083, f_score:0.4503214666648097\n",
      "Validation micro prec: 0.8534583821805393, rec:0.7657336527785894, f_score:0.8072196384020698\n",
      "Validation loss = 0.0426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Step 500 avg train loss = 0.0377\n",
      "Validation macro prec: 0.5644680594315163, rec:0.39400646337794315, f_score:0.4447994833460628\n",
      "Validation micro prec: 0.867056294504904, rec:0.7542219365394729, f_score:0.8067127097721805\n",
      "Validation loss = 0.0427\n",
      "Step 1000 avg train loss = 0.0375\n",
      "Validation macro prec: 0.5646446943600728, rec:0.4090663556431569, f_score:0.45214168833439977\n",
      "Validation micro prec: 0.8617056305564702, rec:0.7646233857301467, f_score:0.8102668895906868\n",
      "Validation loss = 0.0419\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.0376\n",
      "Validation macro prec: 0.5538356959226743, rec:0.4156811103028932, f_score:0.4573838159283184\n",
      "Validation micro prec: 0.8531748595051999, rec:0.7718109039911178, f_score:0.8104559121310669\n",
      "Validation loss = 0.0421\n",
      "Model saved!\n",
      "Step 2000 avg train loss = 0.0377\n",
      "Validation macro prec: 0.5431646131225718, rec:0.4360601446565911, f_score:0.4703593918988252\n",
      "Validation micro prec: 0.846510152284264, rec:0.7795827733302167, f_score:0.8116691509749643\n",
      "Validation loss = 0.0420\n",
      "Model saved!\n",
      "Running Epoch:23\n",
      "Step 0 avg train loss = 0.0319\n",
      "Validation macro prec: 0.5552553231747429, rec:0.4165309250646389, f_score:0.46134355067297766\n",
      "Validation micro prec: 0.8472757645849396, rec:0.7705837667270496, f_score:0.8071120359886158\n",
      "Validation loss = 0.0426\n",
      "Step 500 avg train loss = 0.0365\n",
      "Validation macro prec: 0.5722829167516907, rec:0.4048236943432627, f_score:0.4538861761918234\n",
      "Validation micro prec: 0.8703076297601301, rec:0.7505405247472682, f_score:0.805999184211352\n",
      "Validation loss = 0.0418\n",
      "Step 1000 avg train loss = 0.0368\n",
      "Validation macro prec: 0.5650221058926799, rec:0.40881737514227035, f_score:0.4577954514064648\n",
      "Validation micro prec: 0.8536601264585099, rec:0.7652661719160871, f_score:0.8070499784310099\n",
      "Validation loss = 0.0423\n",
      "Step 1500 avg train loss = 0.0366\n",
      "Validation macro prec: 0.5889840083020417, rec:0.4076252569004808, f_score:0.45411865697216547\n",
      "Validation micro prec: 0.8601490863513425, rec:0.7619353707707591, f_score:0.8080689142290531\n",
      "Validation loss = 0.0426\n",
      "Step 2000 avg train loss = 0.0367\n",
      "Validation macro prec: 0.5840220472665796, rec:0.3944719288038467, f_score:0.44556879688169015\n",
      "Validation micro prec: 0.8630771266040481, rec:0.762461286741074, f_score:0.8096553007973689\n",
      "Validation loss = 0.0415\n",
      "Running Epoch:24\n",
      "Step 0 avg train loss = 0.0306\n",
      "Validation macro prec: 0.552541155026488, rec:0.41030425698696854, f_score:0.45262840404642746\n",
      "Validation micro prec: 0.8542528438469493, rec:0.7723368199614328, f_score:0.8112321620377475\n",
      "Validation loss = 0.0419\n",
      "Step 500 avg train loss = 0.0361\n",
      "Validation macro prec: 0.5652831391720805, rec:0.40323099836801624, f_score:0.45586979421312623\n",
      "Validation micro prec: 0.8585493928454218, rec:0.7643312101910829, f_score:0.8087053295412391\n",
      "Validation loss = 0.0419\n",
      "Step 1000 avg train loss = 0.0360\n",
      "Validation macro prec: 0.558000613100874, rec:0.43519710946271656, f_score:0.47627910382267413\n",
      "Validation micro prec: 0.8410243566464216, rec:0.7849003681411792, f_score:0.8119937129730382\n",
      "Validation loss = 0.0421\n",
      "Model saved!\n",
      "Step 1500 avg train loss = 0.0360\n",
      "Validation macro prec: 0.5590783803422459, rec:0.4146332037257354, f_score:0.46015181588475856\n",
      "Validation micro prec: 0.8495530012771392, rec:0.7774206743411441, f_score:0.8118878344979099\n",
      "Validation loss = 0.0417\n",
      "Step 2000 avg train loss = 0.0358\n",
      "Validation macro prec: 0.5638934506254709, rec:0.399206366645115, f_score:0.4463055412732027\n",
      "Validation micro prec: 0.8717087218869994, rec:0.7428855256237947, f_score:0.8021579329274064\n",
      "Validation loss = 0.0423\n",
      "Running Epoch:25\n",
      "Step 0 avg train loss = 0.0360\n",
      "Validation macro prec: 0.5583844308557597, rec:0.4252030722623051, f_score:0.4690366428059271\n",
      "Validation micro prec: 0.8491268618387262, rec:0.7728627359317478, f_score:0.8092018721894214\n",
      "Validation loss = 0.0421\n",
      "Step 500 avg train loss = 0.0352\n",
      "Validation macro prec: 0.5701836348164634, rec:0.4362536683223423, f_score:0.48048612578768973\n",
      "Validation micro prec: 0.846881390593047, rec:0.7743820487348799, f_score:0.8090107139586704\n",
      "Validation loss = 0.0423\n",
      "Step 1000 avg train loss = 0.0354\n",
      "Validation macro prec: 0.5660778479900013, rec:0.42091873580345757, f_score:0.4695838116896455\n",
      "Validation micro prec: 0.8585349901896665, rec:0.7670776602582832, f_score:0.8102336203437953\n",
      "Validation loss = 0.0421\n",
      "Step 1500 avg train loss = 0.0353\n",
      "Validation macro prec: 0.5682521578088557, rec:0.42273539974027674, f_score:0.467663259175209\n",
      "Validation micro prec: 0.8548773383390511, rec:0.7717524688833051, f_score:0.8111909587863153\n",
      "Validation loss = 0.0415\n",
      "Step 2000 avg train loss = 0.0351\n",
      "Validation macro prec: 0.578123837698069, rec:0.42452692102988954, f_score:0.47511531269444407\n",
      "Validation micro prec: 0.8677199089570223, rec:0.7574358674691755, f_score:0.8088359177560761\n",
      "Validation loss = 0.0412\n",
      "Running Epoch:26\n",
      "Step 0 avg train loss = 0.0404\n",
      "Validation macro prec: 0.567409126156006, rec:0.4208338740565995, f_score:0.46722890137512857\n",
      "Validation micro prec: 0.849507735583685, rec:0.7764857126161397, f_score:0.811357044725996\n",
      "Validation loss = 0.0416\n",
      "Step 500 avg train loss = 0.0346\n",
      "Validation macro prec: 0.5781917765092937, rec:0.43594288092595873, f_score:0.4828221451197066\n",
      "Validation micro prec: 0.8597493669242257, rec:0.7737392625489394, f_score:0.814479916343729\n",
      "Validation loss = 0.0410\n",
      "Model saved!\n",
      "Step 1000 avg train loss = 0.0349\n",
      "Validation macro prec: 0.5707155907553104, rec:0.4307862932146225, f_score:0.47493424004037677\n",
      "Validation micro prec: 0.8522417153996101, rec:0.7664348740723427, f_score:0.8070639633264621\n",
      "Validation loss = 0.0423\n",
      "Step 1500 avg train loss = 0.0347\n",
      "Validation macro prec: 0.5776413600411684, rec:0.42874288743931016, f_score:0.47546298271592424\n",
      "Validation micro prec: 0.8514260020554985, rec:0.7745573540583183, f_score:0.8111746886570178\n",
      "Validation loss = 0.0415\n",
      "Step 2000 avg train loss = 0.0346\n",
      "Validation macro prec: 0.5802556699629803, rec:0.4327588008415459, f_score:0.4753520595427192\n",
      "Validation micro prec: 0.8502396683508227, rec:0.7670192251504704, f_score:0.8064882799299561\n",
      "Validation loss = 0.0423\n",
      "Running Epoch:27\n",
      "Step 0 avg train loss = 0.0266\n",
      "Validation macro prec: 0.5716586835637576, rec:0.42758444714832416, f_score:0.4706271915990557\n",
      "Validation micro prec: 0.860374160745714, rec:0.7712849880208029, f_score:0.8133974240463425\n",
      "Validation loss = 0.0411\n",
      "Step 500 avg train loss = 0.0335\n",
      "Validation macro prec: 0.5735695206289706, rec:0.4389053776220797, f_score:0.4832016558044882\n",
      "Validation micro prec: 0.8536457307869003, rec:0.7600654473207503, f_score:0.8041421947449767\n",
      "Validation loss = 0.0430\n",
      "Step 1000 avg train loss = 0.0333\n",
      "Validation macro prec: 0.5840621536339506, rec:0.4557290254674418, f_score:0.4942687594880018\n",
      "Validation micro prec: 0.8420786765170171, rec:0.779290597791153, f_score:0.8094688922610015\n",
      "Validation loss = 0.0420\n",
      "Step 1500 avg train loss = 0.0337\n",
      "Validation macro prec: 0.6106722451360641, rec:0.4345059381848784, f_score:0.4816332926759203\n",
      "Validation micro prec: 0.855228237791932, rec:0.7532285397066557, f_score:0.8009942519807364\n",
      "Validation loss = 0.0428\n",
      "Step 2000 avg train loss = 0.0339\n",
      "Validation macro prec: 0.575667951403587, rec:0.4466316452738722, f_score:0.48350895332404525\n",
      "Validation micro prec: 0.8410243244977857, rec:0.7657336527785894, f_score:0.8016149752248118\n",
      "Validation loss = 0.0436\n",
      "Running Epoch:28\n",
      "Step 0 avg train loss = 0.0391\n",
      "Validation macro prec: 0.5759135583172657, rec:0.448185957286282, f_score:0.4919052446264259\n",
      "Validation micro prec: 0.8564112795956371, rec:0.7524688833050897, f_score:0.8010824597965722\n",
      "Validation loss = 0.0430\n",
      "Step 500 avg train loss = 0.0333\n",
      "Validation macro prec: 0.578622645199551, rec:0.44323405149474, f_score:0.4753711055809159\n",
      "Validation micro prec: 0.8315330680310783, rec:0.7692397592473558, f_score:0.7991743564837299\n",
      "Validation loss = 0.0447\n",
      "Step 1000 avg train loss = 0.0329\n",
      "Validation macro prec: 0.5700590794695273, rec:0.42602454274779283, f_score:0.4717802778083647\n",
      "Validation micro prec: 0.8419442998438313, rec:0.7560918599894817, f_score:0.7967119238939688\n",
      "Validation loss = 0.0450\n",
      "Step 1500 avg train loss = 0.0331\n",
      "Validation macro prec: 0.5909980066800726, rec:0.4246531689287903, f_score:0.4671473998405347\n",
      "Validation micro prec: 0.8348805056290737, rec:0.741015602173786, f_score:0.7851526221286608\n",
      "Validation loss = 0.0473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000 avg train loss = 0.0331\n",
      "Validation macro prec: 0.5769259390964637, rec:0.4236885969504728, f_score:0.47128534202793904\n",
      "Validation micro prec: 0.8440660272721341, rec:0.7559749897738561, f_score:0.7975955610357582\n",
      "Validation loss = 0.0447\n",
      "Running Epoch:29\n",
      "Step 0 avg train loss = 0.0274\n",
      "Validation macro prec: 0.6030449698728951, rec:0.4635004218776681, f_score:0.5009627684580469\n",
      "Validation micro prec: 0.8273084847349691, rec:0.7743236136270671, f_score:0.7999396317536976\n",
      "Validation loss = 0.0451\n",
      "Step 500 avg train loss = 0.0318\n",
      "Validation macro prec: 0.582720797582668, rec:0.4550480776210985, f_score:0.49502341439956987\n",
      "Validation micro prec: 0.8426451612903226, rec:0.7632209431426401, f_score:0.8009689387667495\n",
      "Validation loss = 0.0438\n",
      "Step 1000 avg train loss = 0.0322\n",
      "Validation macro prec: 0.6084819455146997, rec:0.428399319156077, f_score:0.4762574063133124\n",
      "Validation micro prec: 0.8424412167003191, rec:0.7557996844504178, f_score:0.7967720076387605\n",
      "Validation loss = 0.0447\n",
      "Step 1500 avg train loss = 0.0324\n",
      "Validation macro prec: 0.6073857066789891, rec:0.4226100778314013, f_score:0.47345039975020403\n",
      "Validation micro prec: 0.8487389365808212, rec:0.7452813650441185, f_score:0.7936527691350341\n",
      "Validation loss = 0.0449\n",
      "Step 2000 avg train loss = 0.0324\n",
      "Validation macro prec: 0.6074273118108843, rec:0.4509651498805906, f_score:0.4971068777547208\n",
      "Validation micro prec: 0.8506382418739308, rec:0.7554490738035412, f_score:0.8002228343288663\n",
      "Validation loss = 0.0433\n",
      "Running Epoch:30\n",
      "Step 0 avg train loss = 0.0357\n",
      "Validation macro prec: 0.6104127502800699, rec:0.46532116828585246, f_score:0.5066469742919462\n",
      "Validation micro prec: 0.844138198757764, rec:0.7624028516332613, f_score:0.8011913168964354\n",
      "Validation loss = 0.0437\n",
      "Step 500 avg train loss = 0.0321\n",
      "Validation macro prec: 0.6352987317411184, rec:0.4494346628819212, f_score:0.4940063909187767\n",
      "Validation micro prec: 0.8376168988293767, rec:0.7484368608660082, f_score:0.790519688927293\n",
      "Validation loss = 0.0458\n",
      "Step 1000 avg train loss = 0.0319\n",
      "Validation macro prec: 0.6106110794461442, rec:0.4482966561997379, f_score:0.4900295314634499\n",
      "Validation micro prec: 0.8364982298036692, rec:0.759364226026997, f_score:0.7960671404067631\n",
      "Validation loss = 0.0455\n",
      "Step 1500 avg train loss = 0.0320\n",
      "Validation macro prec: 0.6263449615797344, rec:0.43505605115981205, f_score:0.4878834783055108\n",
      "Validation micro prec: 0.8548181879059801, rec:0.7459241512300591, f_score:0.7966672907695188\n",
      "Validation loss = 0.0443\n",
      "Step 2000 avg train loss = 0.0320\n",
      "Validation macro prec: 0.6161089139965131, rec:0.45884876730271723, f_score:0.5034013363028389\n",
      "Validation micro prec: 0.8500586472044832, rec:0.7622859814176357, f_score:0.8037832342339566\n",
      "Validation loss = 0.0433\n"
     ]
    }
   ],
   "source": [
    "best_f_score_micro = -1\n",
    "count = 0\n",
    "PATH = '/scratch/sa5154/Capstone/Models/LSTM_Pretrained_max_len_2500_epoch_30.pth'\n",
    "for epoch_number in range(30):\n",
    "    print(\"Running Epoch:{}\".format(epoch_number + 1))\n",
    "    avg_loss = -1\n",
    "    # do train\n",
    "    model.train()\n",
    "\n",
    "    train_loss_cache = 0\n",
    "\n",
    "    for i, (inp, target) in enumerate(wiki_loaders['train']):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        inp = inp.to(current_device)\n",
    "        target = target.to(current_device)\n",
    "        logits = model(inp)\n",
    "        loss = criterion(logits, target)\n",
    "        train_loss_cache += loss.item()\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            avg_loss = train_loss_cache/(i+1)\n",
    "            print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
    "\n",
    "            #do valid\n",
    "            f_score_micro = Validate(model)\n",
    "            if( f_score_micro > best_f_score_micro):\n",
    "                best_f_score_micro = f_score_micro\n",
    "                torch.save({\n",
    "                        'state_dict': model.state_dict()\n",
    "                        }, PATH)\n",
    "                print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTModel(\n",
       "  (lookup): Embedding(606453, 128, padding_idx=0)\n",
       "  (lstm): LSTM(128, 64, batch_first=True)\n",
       "  (projection): Linear(in_features=64, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '/scratch/sa5154/NLP/Models/LSTM_Baseline.pth'\n",
    "model = LSTModel(options).to(current_device)\n",
    "model.load_state_dict(torch.load(PATH)['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = []\n",
    "all_logits = []\n",
    "for i, (inp, target) in enumerate(wiki_loaders['train']):\n",
    "    inp = inp.to(current_device)\n",
    "    logits = model(inp)\n",
    "    m = nn.Sigmoid()\n",
    "    logits = m(logits)\n",
    "    logits = logits.cpu().detach().numpy()\n",
    "    target = target.detach().numpy()\n",
    "    all_targets.append(target)\n",
    "    all_logits.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits[all_logits > 0.5] = 1\n",
    "all_logits[all_logits <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "prec, rec, f_score, _ = precision_recall_fscore_support(all_targets, all_logits, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
